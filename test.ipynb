{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eungyeop/anaconda3/envs/featllm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GATv2Conv,DynamicEdgeConv, EdgeConv,RGCNConv, TransformerConv, GINConv, global_mean_pool\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset.data import get_dataset\n",
    "from utils.table_to_graph import Table2GraphTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "import argparse,os,random\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "# 기존 ArgumentParser로부터 같거나 유사한 파라미터들을 Namespace로 직접 생성\n",
    "args = argparse.Namespace(\n",
    "    random_seed=42,\n",
    "    train_epochs=200,\n",
    "    batch_size=64,\n",
    "    input_dim=768,\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    dropout_rate=0.3,\n",
    "    threshold=0.5,\n",
    "    heads=8,\n",
    "    model='NORM_GNN',\n",
    "    source_dataset_name='cleveland',\n",
    "    target_dataset_name='hungarian',\n",
    "    few_shot=4,\n",
    "    dataset_seed=4,\n",
    "    source_lr=0.0001,\n",
    "    llm_model='gpt2',\n",
    "    use_gpu=True,\n",
    "    des=None,  # 실험 메모를 남기는 용도\n",
    "    baseline=[],  # 'Logistic_Regression' or 'XGBoost' 등을 리스트로 넣을 수 있음\n",
    "    graph_path=\"/storage/personal/eungyeop/dataset/graph\",\n",
    "    table_path=\"/storage/personal/eungyeop/dataset/table\",\n",
    "    model_type='NORM_GNN',\n",
    "    graph_type='star',\n",
    "    FD='N',\n",
    "    label=False  # --label 플래그 사용 시 True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = str(args.random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "torch.manual_seed(args.random_seed)\n",
    "np.random.seed(args.random_seed)\n",
    "random.seed(args.random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.random_seed)\n",
    "    torch.cuda.manual_seed_all(args.random_seed)\n",
    "\n",
    "if args.label:\n",
    "    file_path = os.path.join(args.graph_path, f\"{args.graph_type}_{args.FD}_label_{args.target_dataset_name}.pkl\")\n",
    "else:\n",
    "    file_path = os.path.join(args.graph_path, f\"{args.graph_type}_{args.FD}_{args.target_dataset_name}.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_and_class = {\n",
    "    \"adult\" : ['income', ['no','yes']],\n",
    "    \"bank\" : ['Class', ['no','yes']],\n",
    "    \"blood\" : ['Class',['no','yes']],\n",
    "    \"car\" : ['class',['unacceptable','acceptable','good','very good']],\n",
    "    \"communities\" : ['ViolentCrimesPerPop',['high','medium','low']],\n",
    "    \"credit-g\" : ['class', ['no','yes']],\n",
    "    \"diabetes\": ['Outcome',['no','yes']],\n",
    "    \"myocardial\" : ['ZSN',['no','yes']],\n",
    "    \"cleveland\": ['target_binary', ['no','yes']],\n",
    "    \"hungarian\": ['target_binary', ['no','yes']],\n",
    "    \"switzerland\": ['target_binary', ['no','yes']],\n",
    "    \"heart_statlog\": ['target_binary', ['no','yes']],\n",
    "    \"heart\": ['target_binary', ['no','yes']]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing( DATASETS: pd.DataFrame, data_name: str) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"데이터셋 전처리\"\"\"\n",
    "    assert data_name in dataset_and_class, f\"{data_name} is not a valid dataset name\"\n",
    "\n",
    "    class_name = dataset_and_class[data_name][0]\n",
    "    class_values = dataset_and_class[data_name][1]\n",
    "    \n",
    "    class_mapping = {label: idx for idx, label in enumerate(class_values)}\n",
    "    \n",
    "    X = DATASETS.drop(class_name, axis=1)\n",
    "    X = X.reset_index(drop=True)\n",
    "\n",
    "    y = DATASETS[class_name]\n",
    "    y = y.map(class_mapping).astype(int)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak\n",
      "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3\n",
      "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5\n",
      "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6\n",
      "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5\n",
      "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: target_binary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/storage/personal/eungyeop/dataset/table/\"\n",
    "data_source = \"label_table\" if args.label else \"origin_table\"\n",
    "file_path = os.path.join(base_path, data_source, f\"{args.source_dataset_name}.csv\")\n",
    "cleveland = pd.read_csv(file_path)\n",
    "X, y = preprocessing(cleveland, args.source_dataset_name)\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for_num = 5\n",
    "\n",
    "for i in range(for_num -3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'graph_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer \u001b[38;5;241m=\u001b[39m \u001b[43mTable2GraphTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_edge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_attention_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_hypergraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorr_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_dataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'graph_type'"
     ]
    }
   ],
   "source": [
    "transformer = Table2GraphTransformer(\n",
    "    include_edge_attr=True, \n",
    "    graph_type=args.graph_type,\n",
    "    lm_model=\"gpt2\", \n",
    "    n_components=768, \n",
    "    n_jobs=1,\n",
    "    use_attention_init=None,\n",
    "    use_hypergraph=None,\n",
    "    corr_threshold=0.5,\n",
    "    FD=args.FD,\n",
    "    dataset_name=args.source_dataset_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = transformer.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n"
     ]
    }
   ],
   "source": [
    "print(len(graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [data.y.item() for data in graphs]\n",
    "num_classes = len(set(class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "61\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "train_val_data, test_data = train_test_split(graphs, test_size=0.2, stratify=class_labels, random_state=args.random_seed)\n",
    "train_labels = [data.y.item() for data in train_val_data]\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.25, stratify=train_labels, random_state=args.random_seed)\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot = args.few_shot \n",
    "base_samples_per_class = shot // num_classes \n",
    "remainder = shot % num_classes\n",
    "extra_samples = random.sample(range(num_classes), remainder)\n",
    "support_data = []\n",
    "for cls in range(num_classes):\n",
    "    cls_data = [data for data in train_data if data.y.item() == cls]\n",
    "    sample_num = base_samples_per_class + (1 if cls in extra_samples else 0)\n",
    "    \n",
    "    if len(cls_data) < sample_num:\n",
    "        warnings.warn(f\"Class {cls} has fewer samples ({len(cls_data)}) than required ({sample_num}). Using replacement sampling.\")\n",
    "        selected_data = random.choices(cls_data, k=sample_num)\n",
    "    else:\n",
    "        selected_data = random.sample(cls_data, k=sample_num)\n",
    "    \n",
    "    support_data.extend(selected_data)\n",
    "\n",
    "train_data = support_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dist = Counter([data.y.item() for data in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eungyeop/anaconda3/envs/featllm/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=args.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tabular_dataloaders(args, dataset_name, few_shot = False):\n",
    "    os.environ['PYTHONHASHSEED'] = str(args.random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    torch.manual_seed(args.random_seed)\n",
    "    np.random.seed(args.random_seed)\n",
    "    random.seed(args.random_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.random_seed)\n",
    "        torch.cuda.manual_seed_all(args.random_seed)\n",
    "    dataset_and_class = {\n",
    "    \"adult\" : ['income', ['no','yes']],\n",
    "    \"bank\" : ['Class', ['no','yes']],\n",
    "    \"blood\" : ['Class',['no','yes']],\n",
    "    \"car\" : ['class',['unacceptable','acceptable','good','very good']],\n",
    "    \"communities\" : ['ViolentCrimesPerPop',['high','medium','low']],\n",
    "    \"credit-g\" : ['class', ['no','yes']],\n",
    "    \"diabetes\": ['Outcome',['no','yes']],\n",
    "    \"myocardial\" : ['ZSN',['no','yes']],\n",
    "    \"cleveland\": ['target_binary', ['no','yes']],\n",
    "    \"hungarian\": ['target_binary', ['no','yes']],\n",
    "    \"switzerland\": ['target_binary', ['no','yes']],\n",
    "    \"heart_statlog\": ['target_binary', ['no','yes']],\n",
    "    \"heart\": ['target_binary', ['no','yes']]\n",
    "    }\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "featllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
