"""
Silhouette Analysis for Optimal Cluster Number

K-means í´ëŸ¬ìŠ¤í„°ë§ì˜ ìµœì  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ë¥¼ í†µí•´ ê²°ì •í•©ë‹ˆë‹¤.

Usage:
    python silhouette_analysis.py --checkpoint_dir /path/to/checkpoint.pt --mode Full
"""

import os
# CUDA deterministic ì„¤ì •ì„ ê°€ì¥ ë¨¼ì € ì„¤ì •
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'

import torch
import argparse
import numpy as np
import logging
from pathlib import Path
import json
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.manifold import TSNE
import seaborn as sns

# ê¸°ì¡´ ëª¨ë“ˆë“¤ import
from models.TabularFLM import Model
from dataset.data_dataloaders import prepare_embedding_dataloaders, get_few_shot_embedding_samples
from utils.util import setup_logger, fix_seed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SilhouetteAnalyzer:
    def __init__(self, checkpoint_dir, device='cuda'):
        """
        Args:
            checkpoint_dir (str): ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            device (str): 'cuda' ë˜ëŠ” 'cpu'
        """
        self.checkpoint_dir = checkpoint_dir
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        self.checkpoint = torch.load(checkpoint_dir, map_location=self.device)
        self.args = self.checkpoint['args']
        
        logger.info(f"Loaded checkpoint from {checkpoint_dir}")
        logger.info(f"Checkpoint epoch: {self.checkpoint['epoch']}, Val AUC: {self.checkpoint['val_auc']:.4f}")
        
        # ëª¨ë¸ ì´ˆê¸°í™” ë° ë¡œë“œ
        self._load_model()
        
        # ë°ì´í„°ë¡œë” ì¤€ë¹„
        self._prepare_dataloaders()
        
    def _load_model(self):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        experiment_id = "silhouette_analysis"
        mode = "analysis"
        
        self.model = Model(
            self.args, 
            self.args.input_dim, 
            self.args.hidden_dim, 
            self.args.output_dim, 
            self.args.num_layers, 
            self.args.dropout_rate, 
            self.args.llm_model,
            experiment_id,
            mode
        ).to(self.device)
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model.load_state_dict(self.checkpoint['model_state_dict'])
        self.model.eval()
        
        logger.info("Model loaded and set to evaluation mode")
    
    def _prepare_dataloaders(self):
        """ë°ì´í„°ë¡œë” ì¤€ë¹„"""
        fix_seed(self.args.random_seed)
        
        # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë” ì¤€ë¹„
        results = prepare_embedding_dataloaders(self.args, self.args.source_dataset_name)
        self.train_loader, self.val_loader, self.test_loader = results['loaders']
        self.num_classes = results['num_classes']
        
        # Few-shot ë¡œë” ì¤€ë¹„ (í•„ìš”í•œ ê²½ìš°)
        if hasattr(self.args, 'few_shot') and self.args.few_shot > 0:
            self.train_loader_few = get_few_shot_embedding_samples(self.train_loader, self.args)
        
        logger.info(f"Dataloaders prepared for dataset: {self.args.source_dataset_name}")
    
    def extract_attention_maps(self, data_loader):
        """
        ë°ì´í„°ë¡œë”ì—ì„œ attention maps ì¶”ì¶œ
        
        Args:
            data_loader: ë°ì´í„°ë¡œë”
            
        Returns:
            dict: ë ˆì´ì–´ë³„ attention mapsì™€ ë©”íƒ€ë°ì´í„°
        """
        attention_data = {
            'layer_0': [],
            'layer_1': [], 
            'layer_2': [],
            'labels': [],
            'sample_ids': [],
            'feature_names': None
        }
        
        sample_count = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(data_loader):
                # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                batch_on_device = {
                    k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                    for k, v in batch.items()
                }
                
                # ëª¨ë¸ forward (attention weights ì¶”ì¶œ)
                pred, attention_weights = self._extract_attention_from_model(batch_on_device)
                
                # Feature names ì¶”ì¶œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ)
                if attention_data['feature_names'] is None:
                    feature_names = self.model.extract_feature_names(batch_on_device)
                    attention_data['feature_names'] = ["CLS"] + feature_names
                
                # ë°°ì¹˜ í¬ê¸° í™•ì¸
                batch_size = attention_weights[0].shape[0]
                
                for sample_idx in range(batch_size):
                    # ê° ë ˆì´ì–´ë³„ attention map ì €ì¥
                    for layer_idx, layer_attention in enumerate(attention_weights):
                        # Multi-head attentionì„ í‰ê· ë‚´ì–´ ë‹¨ì¼ attention mapìœ¼ë¡œ ë³€í™˜
                        attention_map = layer_attention[sample_idx].mean(dim=0)  # [seq_len, seq_len]
                        attention_numpy = attention_map.detach().cpu().numpy()
                        attention_data[f'layer_{layer_idx}'].append(attention_numpy)
                    
                    # ë¼ë²¨ê³¼ ìƒ˜í”Œ ID ì €ì¥
                    if 'y' in batch:
                        label = batch['y'][sample_idx].item()
                    else:
                        label = -1  # ë¼ë²¨ì´ ì—†ëŠ” ê²½ìš°
                    attention_data['labels'].append(label)
                    
                    # ìƒ˜í”Œ ID (ì‹¤ì œ ë°ì´í„°ì…‹ì˜ ì¸ë±ìŠ¤ ë˜ëŠ” ì¹´ìš´í„°)
                    if 'sample_ids' in batch:
                        sample_id = batch['sample_ids'][sample_idx]
                    else:
                        sample_id = sample_count
                    attention_data['sample_ids'].append(sample_id)
                    
                    sample_count += 1
                
                if batch_idx % 10 == 0:
                    logger.info(f"Processed {sample_count} samples...")
        
        logger.info(f"Extracted attention maps for {sample_count} samples")
        return attention_data
    
    def _extract_attention_from_model(self, batch):
        """
        ëª¨ë¸ì—ì„œ attention weightsì™€ ì˜ˆì¸¡ê°’ì„ ì¶”ì¶œ
        """
        # ëª¨ë¸ì˜ predict ë¡œì§ì„ ë³µì‚¬í•˜ë˜ attention_weightsë„ ë°˜í™˜
        label_description_embeddings = batch['label_description_embeddings'].to(self.device)
        desc_embeddings = [] 
        name_value_embeddings = [] 
        
        if all(k in batch for k in ['cat_name_value_embeddings', 'cat_desc_embeddings']):
            cat_name_value_embeddings = batch['cat_name_value_embeddings'].to(self.device).squeeze(-2)
            cat_desc_embeddings = batch['cat_desc_embeddings'].to(self.device).squeeze(-2)
            
            name_value_embeddings.append(cat_name_value_embeddings)
            desc_embeddings.append(cat_desc_embeddings)
            
        if all(k in batch for k in ['num_prompt_embeddings', 'num_desc_embeddings']):
            num_prompt_embeddings = batch['num_prompt_embeddings'].to(self.device).squeeze(-2)
            num_desc_embeddings = batch['num_desc_embeddings'].to(self.device).squeeze(-2)
            name_value_embeddings.append(num_prompt_embeddings)
            desc_embeddings.append(num_desc_embeddings)
            
        if not desc_embeddings or not name_value_embeddings:
            raise ValueError("No categorical or numerical features found in batch")

        desc_embeddings = torch.cat(desc_embeddings, dim = 1)
        name_value_embeddings = torch.cat(name_value_embeddings, dim = 1)
        
        # [CLS] Token ì¶”ê°€
        attention_weights = [] 
        cls_token = self.model.cls.expand(name_value_embeddings.size(0), -1, -1)
        name_value_embeddings = torch.cat([cls_token, name_value_embeddings], dim=1)
        x = name_value_embeddings
        
        # Graph Attention Layers
        for i, layer in enumerate(self.model.layers):
            norm_x = self.model.layer_norms[i](x)
            attn_output, attn_weights = layer(desc_embeddings, norm_x)
            attention_weights.append(attn_weights)
            x = x + attn_output
        
        # ì˜ˆì¸¡ê°’ ê³„ì‚°
        pred = x[:, 0, :]
        pred = self.model.predictor(pred)

        return pred, attention_weights

    def perform_silhouette_analysis(self, attention_data, layer_idx, k_range=(2, 15), output_dir=None):
        """
        íŠ¹ì • ë ˆì´ì–´ì— ëŒ€í•´ ì‹¤ë£¨ì—£ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ ìµœì  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        
        Args:
            attention_data (dict): attention maps ë°ì´í„°
            layer_idx (int): ë¶„ì„í•  ë ˆì´ì–´ ì¸ë±ìŠ¤
            k_range (tuple): í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ë²”ìœ„ (min_k, max_k)
            output_dir (str, optional): ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        
        Returns:
            dict: ì‹¤ë£¨ì—£ ë¶„ì„ ê²°ê³¼
        """
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # íŠ¹ì • ë ˆì´ì–´ì˜ attention maps ì¶”ì¶œ
        attention_maps = np.stack(attention_data[f'layer_{layer_idx}'])
        labels = np.array(attention_data['labels'])
        feature_names = attention_data['feature_names']
        
        logger.info(f"Performing silhouette analysis on layer {layer_idx} with {len(attention_maps)} samples")
        logger.info(f"Testing cluster range: {k_range[0]} to {k_range[1]}")
        
        # í‰íƒ„í™” (ë²¡í„°í™”)
        flattened_maps = attention_maps.reshape(len(attention_maps), -1)
        
        min_k, max_k = k_range
        k_values = list(range(min_k, max_k + 1))
        silhouette_scores = []
        inertias = []
        
        # ê° kê°’ì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„°ë§ ë° ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°
        for k in k_values:
            logger.info(f"Testing k={k}...")
            
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(flattened_maps)
            
            # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°
            sil_score = silhouette_score(flattened_maps, cluster_labels)
            silhouette_scores.append(sil_score)
            inertias.append(kmeans.inertia_)
            
            logger.info(f"k={k}: Silhouette Score = {sil_score:.4f}, Inertia = {kmeans.inertia_:.2f}")
        
        # ìµœì  k ì°¾ê¸°
        best_k_idx = np.argmax(silhouette_scores)
        best_k = k_values[best_k_idx]
        best_score = silhouette_scores[best_k_idx]
        
        logger.info(f"ğŸ¯ Best k for layer {layer_idx}: {best_k} (Silhouette Score: {best_score:.4f})")
        
        # ê²°ê³¼ ì €ì¥
        results = {
            'layer_idx': layer_idx,
            'k_values': k_values,
            'silhouette_scores': [float(score) for score in silhouette_scores],  # float ë³€í™˜
            'inertias': [float(inertia) for inertia in inertias],  # float ë³€í™˜
            'best_k': int(best_k),  # int ë³€í™˜
            'best_score': float(best_score),  # float ë³€í™˜
            'feature_names': feature_names
        }
        
        if output_dir:
            # 1. ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ í”Œë¡¯
            self._plot_silhouette_scores(results, output_dir)
            
            # 2. ì—˜ë³´ìš° í”Œë¡¯
            self._plot_elbow_curve(results, output_dir)
            
            # 3. ìµœì  kì— ëŒ€í•œ ìƒì„¸ ì‹¤ë£¨ì—£ ë¶„ì„
            self._detailed_silhouette_analysis(flattened_maps, best_k, layer_idx, output_dir)
            
            # 4. t-SNE ì‹œê°í™” (ìµœì  k)
            self._visualize_optimal_clustering(flattened_maps, labels, best_k, layer_idx, output_dir)
            
            # 5. ê²°ê³¼ JSON ì €ì¥
            results_json = results.copy()
            results_json['feature_names'] = list(feature_names)  # numpy arrayë¥¼ listë¡œ ë³€í™˜
            
            with open(output_dir / f'layer_{layer_idx}_silhouette_results.json', 'w') as f:
                json.dump(results_json, f, indent=2)
            
            logger.info(f"âœ… Silhouette analysis results saved to {output_dir}")
        
        return results

    def _plot_silhouette_scores(self, results, output_dir):
        """ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ í”Œë¡¯ ìƒì„±"""
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        k_values = results['k_values']
        scores = results['silhouette_scores']
        best_k = results['best_k']
        
        # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ í”Œë¡¯
        ax.plot(k_values, scores, 'bo-', linewidth=2, markersize=8)
        ax.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, linewidth=2)
        ax.axhline(y=results['best_score'], color='red', linestyle='--', alpha=0.7, linewidth=2)
        
        # ìµœì  k í¬ì¸íŠ¸ ê°•ì¡°
        best_idx = k_values.index(best_k)
        ax.plot(best_k, scores[best_idx], 'ro', markersize=12, markerfacecolor='red', 
                markeredgecolor='darkred', markeredgewidth=2)
        
        ax.set_xlabel('Number of Clusters (k)', fontsize=12)
        ax.set_ylabel('Silhouette Score', fontsize=12)
        ax.set_title(f'Layer {results["layer_idx"]}: Silhouette Analysis\nOptimal k = {best_k} (Score: {results["best_score"]:.4f})', 
                    fontsize=14, pad=20)
        ax.grid(True, alpha=0.3)
        ax.set_xticks(k_values)
        
        # ì ìˆ˜ ê°’ í‘œì‹œ
        for i, (k, score) in enumerate(zip(k_values, scores)):
            ax.annotate(f'{score:.3f}', (k, score), textcoords="offset points", 
                       xytext=(0,10), ha='center', fontsize=9)
        
        # ìµœì  k í…ìŠ¤íŠ¸ ë°•ìŠ¤
        ax.text(0.02, 0.98, f'Best k: {best_k}\nBest Score: {results["best_score"]:.4f}', 
                transform=ax.transAxes, fontsize=12, verticalalignment='top',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(output_dir / f'layer_{results["layer_idx"]}_silhouette_scores.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_elbow_curve(self, results, output_dir):
        """ì—˜ë³´ìš° ì»¤ë¸Œ í”Œë¡¯ ìƒì„±"""
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        k_values = results['k_values']
        inertias = results['inertias']
        best_k = results['best_k']
        
        # ì—˜ë³´ìš° ì»¤ë¸Œ
        ax.plot(k_values, inertias, 'go-', linewidth=2, markersize=8)
        ax.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, linewidth=2)
        
        # ìµœì  k í¬ì¸íŠ¸ ê°•ì¡°
        best_idx = k_values.index(best_k)
        ax.plot(best_k, inertias[best_idx], 'ro', markersize=12, markerfacecolor='red', 
                markeredgecolor='darkred', markeredgewidth=2)
        
        ax.set_xlabel('Number of Clusters (k)', fontsize=12)
        ax.set_ylabel('Inertia (Within-cluster Sum of Squares)', fontsize=12)
        ax.set_title(f'Layer {results["layer_idx"]}: Elbow Method\nSilhouette-based Optimal k = {best_k}', 
                    fontsize=14, pad=20)
        ax.grid(True, alpha=0.3)
        ax.set_xticks(k_values)
        
        plt.tight_layout()
        plt.savefig(output_dir / f'layer_{results["layer_idx"]}_elbow_curve.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()

    def _detailed_silhouette_analysis(self, flattened_maps, optimal_k, layer_idx, output_dir):
        """ìµœì  kì— ëŒ€í•œ ìƒì„¸ ì‹¤ë£¨ì—£ ë¶„ì„"""
        # ìµœì  kë¡œ í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(flattened_maps)
        
        # ìƒ˜í”Œë³„ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°
        sample_silhouette_values = silhouette_samples(flattened_maps, cluster_labels)
        
        # ì‹¤ë£¨ì—£ í”Œë¡¯ ìƒì„±
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # 1. ì‹¤ë£¨ì—£ í”Œë¡¯
        y_lower = 10
        colors = plt.cm.nipy_spectral(np.linspace(0, 1, optimal_k))
        
        for i in range(optimal_k):
            # ië²ˆì§¸ í´ëŸ¬ìŠ¤í„°ì˜ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ë“¤
            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
            ith_cluster_silhouette_values.sort()
            
            size_cluster_i = ith_cluster_silhouette_values.shape[0]
            y_upper = y_lower + size_cluster_i
            
            ax1.fill_betweenx(np.arange(y_lower, y_upper),
                             0, ith_cluster_silhouette_values,
                             facecolor=colors[i], edgecolor=colors[i], alpha=0.7)
            
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì•™ì— ë¼ë²¨ í‘œì‹œ
            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
            y_lower = y_upper + 10
        
        ax1.set_xlabel('Silhouette Coefficient Values')
        ax1.set_ylabel('Cluster Label')
        ax1.set_title(f'Silhouette Plot (k={optimal_k})')
        
        # í‰ê·  ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ë¼ì¸
        silhouette_avg = silhouette_score(flattened_maps, cluster_labels)
        ax1.axvline(x=silhouette_avg, color="red", linestyle="--", 
                   label=f'Average Score: {silhouette_avg:.3f}')
        ax1.legend()
        
        # 2. í´ëŸ¬ìŠ¤í„° í¬ê¸°ì™€ í‰ê·  ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
        cluster_sizes = []
        cluster_avg_scores = []
        
        for i in range(optimal_k):
            cluster_mask = cluster_labels == i
            cluster_sizes.append(np.sum(cluster_mask))
            cluster_avg_scores.append(np.mean(sample_silhouette_values[cluster_mask]))
        
        x_pos = np.arange(optimal_k)
        bars = ax2.bar(x_pos, cluster_avg_scores, color=colors, alpha=0.7)
        ax2.axhline(y=silhouette_avg, color="red", linestyle="--", 
                   label=f'Overall Average: {silhouette_avg:.3f}')
        
        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ì •ë³´ ì¶”ê°€
        for i, (bar, size) in enumerate(zip(bars, cluster_sizes)):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'n={size}', ha='center', va='bottom', fontsize=9)
        
        ax2.set_xlabel('Cluster')
        ax2.set_ylabel('Average Silhouette Score')
        ax2.set_title('Average Silhouette Score per Cluster')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels([f'C{i}' for i in range(optimal_k)])
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.suptitle(f'Layer {layer_idx}: Detailed Silhouette Analysis (k={optimal_k})', 
                    fontsize=16, y=1.02)
        plt.tight_layout()
        plt.savefig(output_dir / f'layer_{layer_idx}_detailed_silhouette.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()

    def _visualize_optimal_clustering(self, flattened_maps, true_labels, optimal_k, layer_idx, output_dir):
        """ìµœì  kë¡œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ t-SNEë¡œ ì‹œê°í™”"""
        # ìµœì  kë¡œ í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(flattened_maps)
        
        # t-SNE ì°¨ì› ì¶•ì†Œ
        perplexity = min(30, len(flattened_maps)-1, max(1, len(flattened_maps)//3))
        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
        tsne_embeddings = tsne.fit_transform(flattened_maps)
        
        # ì‹œê°í™”
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # 1. í´ëŸ¬ìŠ¤í„° ê²°ê³¼
        colors = plt.cm.tab10(np.linspace(0, 1, optimal_k))
        for i in range(optimal_k):
            mask = cluster_labels == i
            ax1.scatter(tsne_embeddings[mask, 0], tsne_embeddings[mask, 1], 
                       c=[colors[i]], label=f'Cluster {i}', alpha=0.7, s=50)
        
        ax1.set_title(f'K-means Clustering (k={optimal_k})')
        ax1.set_xlabel('t-SNE Dimension 1')
        ax1.set_ylabel('t-SNE Dimension 2')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. ì‹¤ì œ ë¼ë²¨ ê²°ê³¼
        unique_labels = np.unique(true_labels)
        label_colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))
        
        for i, label in enumerate(unique_labels):
            mask = true_labels == label
            marker = 'o' if label == 0 else 's'
            ax2.scatter(tsne_embeddings[mask, 0], tsne_embeddings[mask, 1], 
                       c=[label_colors[i]], label=f'Label {int(label)}', 
                       alpha=0.7, s=50, marker=marker)
        
        ax2.set_title('True Labels')
        ax2.set_xlabel('t-SNE Dimension 1')
        ax2.set_ylabel('t-SNE Dimension 2')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.suptitle(f'Layer {layer_idx}: Optimal Clustering Visualization', 
                    fontsize=16, y=1.02)
        plt.tight_layout()
        plt.savefig(output_dir / f'layer_{layer_idx}_optimal_clustering_viz.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()

    def analyze_all_layers(self, data_loader, k_range=(2, 15), output_dir=None):
        """
        ëª¨ë“  ë ˆì´ì–´ì— ëŒ€í•´ ì‹¤ë£¨ì—£ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            data_loader: ë°ì´í„°ë¡œë”
            k_range (tuple): í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ë²”ìœ„
            output_dir (str, optional): ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        
        Returns:
            dict: ëª¨ë“  ë ˆì´ì–´ì˜ ë¶„ì„ ê²°ê³¼
        """
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # Attention maps ì¶”ì¶œ
        logger.info("Extracting attention maps...")
        attention_data = self.extract_attention_maps(data_loader)
        
        # ëª¨ë“  ë ˆì´ì–´ ë¶„ì„
        all_results = {}
        for layer_idx in range(len(self.model.layers)):
            logger.info(f"\n{'='*50}")
            logger.info(f"Analyzing Layer {layer_idx}")
            logger.info(f"{'='*50}")
            
            layer_output_dir = output_dir / f'layer_{layer_idx}' if output_dir else None
            results = self.perform_silhouette_analysis(
                attention_data, 
                layer_idx, 
                k_range=k_range,
                output_dir=layer_output_dir
            )
            all_results[f'layer_{layer_idx}'] = results
        
        # ì „ì²´ ìš”ì•½ ìƒì„±
        if output_dir:
            self._generate_summary_comparison(all_results, output_dir)
        
        return all_results

    def _generate_summary_comparison(self, all_results, output_dir):
        """ì „ì²´ ë ˆì´ì–´ ê²°ê³¼ ë¹„êµ ìš”ì•½"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        layers = sorted([int(k.split('_')[1]) for k in all_results.keys()])
        
        # 1. ë ˆì´ì–´ë³„ ìµœì  k ë¹„êµ
        ax = axes[0, 0]
        best_ks = [all_results[f'layer_{layer}']['best_k'] for layer in layers]
        best_scores = [all_results[f'layer_{layer}']['best_score'] for layer in layers]
        
        bars = ax.bar(layers, best_ks, color='skyblue', alpha=0.7)
        for i, (layer, k, score) in enumerate(zip(layers, best_ks, best_scores)):
            ax.text(layer, k + 0.1, f'k={k}\n({score:.3f})', 
                   ha='center', va='bottom', fontsize=10)
        
        ax.set_xlabel('Layer')
        ax.set_ylabel('Optimal k')
        ax.set_title('Optimal Cluster Number by Layer')
        ax.set_xticks(layers)
        ax.grid(True, alpha=0.3)
        
        # 2. ë ˆì´ì–´ë³„ ìµœê³  ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
        ax = axes[0, 1]
        ax.plot(layers, best_scores, 'ro-', linewidth=2, markersize=8)
        for layer, score in zip(layers, best_scores):
            ax.annotate(f'{score:.3f}', (layer, score), textcoords="offset points", 
                       xytext=(0,10), ha='center', fontsize=9)
        
        ax.set_xlabel('Layer')
        ax.set_ylabel('Best Silhouette Score')
        ax.set_title('Best Silhouette Score by Layer')
        ax.set_xticks(layers)
        ax.grid(True, alpha=0.3)
        
        # 3. ì „ì²´ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ íˆíŠ¸ë§µ
        ax = axes[1, 0]
        k_range = all_results[f'layer_{layers[0]}']['k_values']
        silhouette_matrix = []
        
        for layer in layers:
            silhouette_matrix.append(all_results[f'layer_{layer}']['silhouette_scores'])
        
        silhouette_matrix = np.array(silhouette_matrix)
        im = ax.imshow(silhouette_matrix, cmap='viridis', aspect='auto')
        
        ax.set_xlabel('k value')
        ax.set_ylabel('Layer')
        ax.set_title('Silhouette Scores Heatmap')
        ax.set_xticks(range(len(k_range)))
        ax.set_xticklabels(k_range)
        ax.set_yticks(range(len(layers)))
        ax.set_yticklabels([f'Layer {layer}' for layer in layers])
        
        # ì»¬ëŸ¬ë°”
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label('Silhouette Score')
        
        # ìµœì  k ìœ„ì¹˜ í‘œì‹œ
        for i, layer in enumerate(layers):
            best_k = all_results[f'layer_{layer}']['best_k']
            best_k_idx = k_range.index(best_k)
            ax.plot(best_k_idx, i, 'r*', markersize=15, markeredgecolor='white', markeredgewidth=1)
        
        # 4. ìš”ì•½ í†µê³„
        ax = axes[1, 1]
        ax.axis('off')
        
        summary_text = "Silhouette Analysis Summary\n\n"
        summary_text += f"Tested k range: {min(k_range)} - {max(k_range)}\n"
        summary_text += f"Total layers: {len(layers)}\n\n"
        
        summary_text += "Optimal k per layer:\n"
        for layer in layers:
            result = all_results[f'layer_{layer}']
            summary_text += f"Layer {layer}: k={result['best_k']} (score: {result['best_score']:.4f})\n"
        
        # ì „ì²´ í‰ê· 
        avg_best_k = np.mean(best_ks)
        avg_best_score = np.mean(best_scores)
        summary_text += f"\nAverage optimal k: {avg_best_k:.1f}\n"
        summary_text += f"Average best score: {avg_best_score:.4f}\n"
        
        # ê°€ì¥ ì¢‹ì€ ë ˆì´ì–´
        best_layer_idx = layers[np.argmax(best_scores)]
        best_overall_score = max(best_scores)
        summary_text += f"\nBest performing layer: {best_layer_idx}\n"
        summary_text += f"Best overall score: {best_overall_score:.4f}"
        
        ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
        
        plt.suptitle('Comprehensive Silhouette Analysis Summary', fontsize=16, y=0.98)
        plt.tight_layout()
        plt.savefig(output_dir / 'silhouette_analysis_summary.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # ìš”ì•½ JSON ì €ì¥
        summary_data = {
            'analysis_summary': {
                'k_range': [int(k) for k in k_range],  # int ë³€í™˜
                'layers_analyzed': [int(layer) for layer in layers],  # int ë³€í™˜
                'optimal_k_per_layer': {f'layer_{layer}': int(all_results[f'layer_{layer}']['best_k']) for layer in layers},
                'best_scores_per_layer': {f'layer_{layer}': float(all_results[f'layer_{layer}']['best_score']) for layer in layers},
                'average_optimal_k': float(avg_best_k),
                'average_best_score': float(avg_best_score),
                'best_performing_layer': int(best_layer_idx),
                'best_overall_score': float(best_overall_score)
            },
            'detailed_results': {}
        }
        
        # ê° ë ˆì´ì–´ì˜ ìƒì„¸ ê²°ê³¼ë„ í¬í•¨
        for layer_key, result in all_results.items():
            summary_data['detailed_results'][layer_key] = {
                'best_k': int(result['best_k']),
                'best_score': float(result['best_score']),
                'k_values': [int(k) for k in result['k_values']],
                'silhouette_scores': [float(score) for score in result['silhouette_scores']]
            }
        
        with open(output_dir / 'comprehensive_silhouette_analysis.json', 'w') as f:
            json.dump(summary_data, f, indent=2)
        
        logger.info("âœ… Comprehensive silhouette analysis summary generated!")
        
        # ì¶”ì²œì‚¬í•­ ë¡œê·¸ ì¶œë ¥
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ RECOMMENDATIONS")
        logger.info("="*60)
        logger.info(f"ğŸ“Š Best performing layer: Layer {best_layer_idx} (score: {best_overall_score:.4f})")
        logger.info(f"ğŸ”¢ Most common optimal k: {max(set(best_ks), key=best_ks.count)}")
        logger.info(f"ğŸ“ˆ Layer with highest variability: Layer {layers[np.argmax([max(all_results[f'layer_{layer}']['silhouette_scores']) - min(all_results[f'layer_{layer}']['silhouette_scores']) for layer in layers])]}")
        
        # ì•ˆì •ì ì¸ k ì¶”ì²œ
        k_consensus = []
        for k in k_range:
            scores_for_k = [all_results[f'layer_{layer}']['silhouette_scores'][all_results[f'layer_{layer}']['k_values'].index(k)] for layer in layers]
            if min(scores_for_k) > 0.3:  # ëª¨ë“  ë ˆì´ì–´ì—ì„œ 0.3 ì´ìƒ
                k_consensus.append((k, np.mean(scores_for_k)))
        
        if k_consensus:
            best_consensus_k = max(k_consensus, key=lambda x: x[1])
            logger.info(f"ğŸŒŸ Consensus recommendation: k={best_consensus_k[0]} (avg score: {best_consensus_k[1]:.4f})")
        
        logger.info("="*60)


def main():
    parser = argparse.ArgumentParser(description='Silhouette Analysis for Optimal Cluster Number')
    parser.add_argument('--checkpoint_dir', type=str, required=True,
                       help='Path to model checkpoint')
    parser.add_argument('--mode', type=str, choices=['Full', 'Few'], default='Full',
                       help='Which model to use (Full or Few)')
    parser.add_argument('--min_k', type=int, default=2,
                       help='Minimum number of clusters to test (default: 2)')
    parser.add_argument('--max_k', type=int, default=15,
                       help='Maximum number of clusters to test (default: 15)')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='Output directory for results')
    parser.add_argument('--layer_idx', type=int, default=None,
                       help='Specific layer to analyze (if not specified, analyze all layers)')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir is None:
        checkpoint_dir = Path(args.checkpoint_dir)
        path_parts = checkpoint_dir.parts
        
        # visualization í´ë” êµ¬ì¡°ì— ë§ê²Œ ì„¤ì •
        for i, part in enumerate(path_parts):
            if part == 'checkpoints':
                viz_parts = list(path_parts)
                viz_parts[i] = 'visualization'
                viz_path = Path(*viz_parts[:-1])  # best_model_epoch_XX.pt ì œì™¸
                args.output_dir = viz_path / 'silhouette_analysis'
                break
        
        if args.output_dir is None:
            args.output_dir = checkpoint_dir / 'silhouette_analysis'
    
    # Silhouette Analyzer ì´ˆê¸°í™”
    analyzer = SilhouetteAnalyzer(args.checkpoint_dir)
    
    # ë°ì´í„°ë¡œë” ì„ íƒ
    if args.mode == 'Full':
        data_loader = analyzer.train_loader
        logger.info("Using Full dataset loader")
    else:
        data_loader = analyzer.train_loader_few if hasattr(analyzer, 'train_loader_few') else analyzer.test_loader
        logger.info("Using Few-shot dataset loader")
    
    k_range = (args.min_k, args.max_k)
    
    if args.layer_idx is not None:
        # íŠ¹ì • ë ˆì´ì–´ë§Œ ë¶„ì„
        logger.info(f"Analyzing only Layer {args.layer_idx}")
        
        # Attention maps ì¶”ì¶œ
        attention_data = analyzer.extract_attention_maps(data_loader)
        
        # íŠ¹ì • ë ˆì´ì–´ ë¶„ì„
        layer_output_dir = Path(args.output_dir) / f'layer_{args.layer_idx}'
        results = analyzer.perform_silhouette_analysis(
            attention_data, 
            args.layer_idx, 
            k_range=k_range,
            output_dir=layer_output_dir
        )
        
        logger.info(f"\nğŸ¯ Results for Layer {args.layer_idx}:")
        logger.info(f"   Optimal k: {results['best_k']}")
        logger.info(f"   Best Silhouette Score: {results['best_score']:.4f}")
        
    else:
        # ëª¨ë“  ë ˆì´ì–´ ë¶„ì„
        logger.info("Analyzing all layers")
        all_results = analyzer.analyze_all_layers(
            data_loader, 
            k_range=k_range,
            output_dir=args.output_dir
        )
    
    logger.info(f"\nâœ… Silhouette analysis completed! Results saved to {args.output_dir}")


if __name__ == "__main__":
    main()