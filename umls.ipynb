{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'administeredBy': ['https://data.bioontology.org/users/MFAmith', 'https://data.bioontology.org/users/taocui'], 'acronym': 'OCHV', 'name': 'Ontology of Consumer Health Vocabulary', 'summaryOnly': False, 'flat': None, 'ontologyType': 'https://data.bioontology.org/ontology_types/ONTOLOGY', '@id': 'https://data.bioontology.org/ontologies/OCHV', '@type': 'http://data.bioontology.org/metadata/Ontology', 'links': {'submissions': 'https://data.bioontology.org/ontologies/OCHV/submissions', 'properties': 'https://data.bioontology.org/ontologies/OCHV/properties', 'classes': 'https://data.bioontology.org/ontologies/OCHV/classes', 'single_class': 'https://data.bioontology.org/ontologies/OCHV/classes/{class_id}', 'roots': 'https://data.bioontology.org/ontologies/OCHV/classes/roots', 'instances': 'https://data.bioontology.org/ontologies/OCHV/instances', 'metrics': 'https://data.bioontology.org/ontologies/OCHV/metrics', 'reviews': 'https://data.bioontology.org/ontologies/OCHV/reviews', 'notes': 'https://data.bioontology.org/ontologies/OCHV/notes', 'groups': 'https://data.bioontology.org/ontologies/OCHV/groups', 'categories': 'https://data.bioontology.org/ontologies/OCHV/categories', 'latest_submission': 'https://data.bioontology.org/ontologies/OCHV/latest_submission', 'projects': 'https://data.bioontology.org/ontologies/OCHV/projects', 'download': 'https://data.bioontology.org/ontologies/OCHV/download', 'views': 'https://data.bioontology.org/ontologies/OCHV/views', 'analytics': 'https://data.bioontology.org/ontologies/OCHV/analytics', 'ui': 'http://bioportal.bioontology.org/ontologies/OCHV', '@context': {'submissions': 'http://data.bioontology.org/metadata/OntologySubmission', 'properties': 'http://data.bioontology.org/metadata/Property', 'classes': 'http://www.w3.org/2002/07/owl#Class', 'single_class': 'http://www.w3.org/2002/07/owl#Class', 'roots': 'http://www.w3.org/2002/07/owl#Class', 'instances': 'http://data.bioontology.org/metadata/Instance', 'metrics': 'http://data.bioontology.org/metadata/Metrics', 'reviews': 'http://data.bioontology.org/metadata/Review', 'notes': 'http://data.bioontology.org/metadata/Note', 'groups': 'http://data.bioontology.org/metadata/Group', 'categories': 'http://data.bioontology.org/metadata/Category', 'latest_submission': 'http://data.bioontology.org/metadata/OntologySubmission', 'projects': 'http://data.bioontology.org/metadata/Project', 'download': 'http://data.bioontology.org/metadata/Ontology', 'views': 'http://data.bioontology.org/metadata/Ontology', 'analytics': 'http://data.bioontology.org/metadata/Analytics', 'ui': 'http://data.bioontology.org/metadata/Ontology'}}, '@context': {'@vocab': 'http://data.bioontology.org/metadata/', 'acronym': 'http://omv.ontoware.org/2005/05/ontology#acronym', 'name': 'http://omv.ontoware.org/2005/05/ontology#name', 'administeredBy': {'@id': 'http://data.bioontology.org/metadata/User', '@type': '@id'}, 'flat': 'http://data.bioontology.org/metadata/flat', 'summaryOnly': 'http://data.bioontology.org/metadata/summaryOnly', 'ontologyType': {'@id': 'http://data.bioontology.org/metadata/OntologyType', '@type': '@id'}, '@language': 'en'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_KEY = \"53d9893d-8565-4cbe-967b-97161b79b4f4\"  # ì—¬ê¸°ì— ì‹¤ì œ í‚¤ ë„£ì–´ì¤˜\n",
    "BASE_URL = \"https://data.bioontology.org\"\n",
    "\n",
    "headers = {\"Authorization\": f\"apikey token={API_KEY}\"}\n",
    "\n",
    "r = requests.get(\"https://data.bioontology.org/ontologies/OCHV\", headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'administeredBy': ['https://data.bioontology.org/users/MFAmith',\n",
       "  'https://data.bioontology.org/users/taocui'],\n",
       " 'acronym': 'OCHV',\n",
       " 'name': 'Ontology of Consumer Health Vocabulary',\n",
       " 'summaryOnly': False,\n",
       " 'flat': None,\n",
       " 'ontologyType': 'https://data.bioontology.org/ontology_types/ONTOLOGY',\n",
       " '@id': 'https://data.bioontology.org/ontologies/OCHV',\n",
       " '@type': 'http://data.bioontology.org/metadata/Ontology',\n",
       " 'links': {'submissions': 'https://data.bioontology.org/ontologies/OCHV/submissions',\n",
       "  'properties': 'https://data.bioontology.org/ontologies/OCHV/properties',\n",
       "  'classes': 'https://data.bioontology.org/ontologies/OCHV/classes',\n",
       "  'single_class': 'https://data.bioontology.org/ontologies/OCHV/classes/{class_id}',\n",
       "  'roots': 'https://data.bioontology.org/ontologies/OCHV/classes/roots',\n",
       "  'instances': 'https://data.bioontology.org/ontologies/OCHV/instances',\n",
       "  'metrics': 'https://data.bioontology.org/ontologies/OCHV/metrics',\n",
       "  'reviews': 'https://data.bioontology.org/ontologies/OCHV/reviews',\n",
       "  'notes': 'https://data.bioontology.org/ontologies/OCHV/notes',\n",
       "  'groups': 'https://data.bioontology.org/ontologies/OCHV/groups',\n",
       "  'categories': 'https://data.bioontology.org/ontologies/OCHV/categories',\n",
       "  'latest_submission': 'https://data.bioontology.org/ontologies/OCHV/latest_submission',\n",
       "  'projects': 'https://data.bioontology.org/ontologies/OCHV/projects',\n",
       "  'download': 'https://data.bioontology.org/ontologies/OCHV/download',\n",
       "  'views': 'https://data.bioontology.org/ontologies/OCHV/views',\n",
       "  'analytics': 'https://data.bioontology.org/ontologies/OCHV/analytics',\n",
       "  'ui': 'http://bioportal.bioontology.org/ontologies/OCHV',\n",
       "  '@context': {'submissions': 'http://data.bioontology.org/metadata/OntologySubmission',\n",
       "   'properties': 'http://data.bioontology.org/metadata/Property',\n",
       "   'classes': 'http://www.w3.org/2002/07/owl#Class',\n",
       "   'single_class': 'http://www.w3.org/2002/07/owl#Class',\n",
       "   'roots': 'http://www.w3.org/2002/07/owl#Class',\n",
       "   'instances': 'http://data.bioontology.org/metadata/Instance',\n",
       "   'metrics': 'http://data.bioontology.org/metadata/Metrics',\n",
       "   'reviews': 'http://data.bioontology.org/metadata/Review',\n",
       "   'notes': 'http://data.bioontology.org/metadata/Note',\n",
       "   'groups': 'http://data.bioontology.org/metadata/Group',\n",
       "   'categories': 'http://data.bioontology.org/metadata/Category',\n",
       "   'latest_submission': 'http://data.bioontology.org/metadata/OntologySubmission',\n",
       "   'projects': 'http://data.bioontology.org/metadata/Project',\n",
       "   'download': 'http://data.bioontology.org/metadata/Ontology',\n",
       "   'views': 'http://data.bioontology.org/metadata/Ontology',\n",
       "   'analytics': 'http://data.bioontology.org/metadata/Analytics',\n",
       "   'ui': 'http://data.bioontology.org/metadata/Ontology'}},\n",
       " '@context': {'@vocab': 'http://data.bioontology.org/metadata/',\n",
       "  'acronym': 'http://omv.ontoware.org/2005/05/ontology#acronym',\n",
       "  'name': 'http://omv.ontoware.org/2005/05/ontology#name',\n",
       "  'administeredBy': {'@id': 'http://data.bioontology.org/metadata/User',\n",
       "   '@type': '@id'},\n",
       "  'flat': 'http://data.bioontology.org/metadata/flat',\n",
       "  'summaryOnly': 'http://data.bioontology.org/metadata/summaryOnly',\n",
       "  'ontologyType': {'@id': 'http://data.bioontology.org/metadata/OntologyType',\n",
       "   '@type': '@id'},\n",
       "  '@language': 'en'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pdb\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "logger = logging.getLogger(__name__)\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def visualize_cluster_centroids(clustering_info, clustering_dir, epoch, feature_names):\n",
    "    \"\"\"\n",
    "    ê° í´ëŸ¬ìŠ¤í„°ì˜ centroid attention mapì„ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”\n",
    "    í´ëŸ¬ìŠ¤í„°ë³„ í´ë” êµ¬ì¡°ë¡œ ì •ë¦¬\n",
    "    \"\"\"\n",
    "    if clustering_info['cluster_centroids'] is None:\n",
    "        return\n",
    "    \n",
    "    # visualizations í´ë” ìƒì„±\n",
    "    visualizations_dir = os.path.join(clustering_dir, 'visualizations')\n",
    "    os.makedirs(visualizations_dir, exist_ok=True)\n",
    "        \n",
    "    for cluster_id, centroid in enumerate(clustering_info['cluster_centroids']):\n",
    "        # í´ëŸ¬ìŠ¤í„°ë³„ í´ë” ìƒì„± (visualizations í•˜ìœ„ì—)\n",
    "        cluster_folder = os.path.join(visualizations_dir, f'cluster_{cluster_id}')\n",
    "        os.makedirs(cluster_folder, exist_ok=True)\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        \n",
    "        centroid_np = centroid.detach().cpu().numpy()\n",
    "        all_node_names = [\"CLS\"] + feature_names\n",
    "        \n",
    "        im = ax.imshow(centroid_np, cmap='viridis', interpolation='nearest')\n",
    "        ax.set_title(f'Cluster {cluster_id} Centroid - Epoch {epoch}', fontsize=14)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        \n",
    "        # ì¶• ë¼ë²¨ ì„¤ì •\n",
    "        ax.set_xticks(np.arange(len(all_node_names)))\n",
    "        ax.set_yticks(np.arange(len(all_node_names)))\n",
    "        ax.set_xticklabels(all_node_names, rotation=90, fontsize=8)\n",
    "        ax.set_yticklabels(all_node_names, fontsize=8)\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i in range(len(all_node_names)):\n",
    "            for j in range(len(all_node_names)):\n",
    "                ax.text(j, i, f\"{centroid_np[i,j]:.2f}\", \n",
    "                       ha=\"center\", va=\"center\", \n",
    "                       color=\"white\" if centroid_np[i,j] > 0.5 else \"black\", \n",
    "                       fontsize=6)\n",
    "        \n",
    "        # visualizations í´ë”ì˜ í´ëŸ¬ìŠ¤í„°ë³„ í´ë”ì— ì €ì¥\n",
    "        centroid_viz_path = os.path.join(cluster_folder, f'epoch_{epoch}.png')\n",
    "        fig.savefig(centroid_viz_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logger.info(f\"Saved centroid visualization for cluster {cluster_id}: {centroid_viz_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_model_structure(model, data_loader, device, args, mode, experiment_id, epoch, max_samples=10):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì˜ ë‚´ë¶€ êµ¬ì¡°(ì–´í…ì…˜, ê·¸ë˜í”„ êµ¬ì¡° ë“±)ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        model: ì‹œê°í™”í•  ëª¨ë¸\n",
    "        data_loader: ì‹œê°í™”ì— ì‚¬ìš©í•  ë°ì´í„° ë¡œë”\n",
    "        device: ê³„ì‚°ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤\n",
    "        args: ì‹¤í—˜ ì„¤ì • ì¸ì (args.viz_heatmap, args.viz_graph í”Œë˜ê·¸ ì‚¬ìš©)\n",
    "        mode: 'train' ë˜ëŠ” 'val' ëª¨ë“œ\n",
    "        experiment_id: í˜„ì¬ ì‹¤í—˜ ID\n",
    "        epoch: í˜„ì¬ ì—í¬í¬\n",
    "        max_samples: ì‹œê°í™”í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    base_viz_dir = os.path.join(f\"/storage/personal/eungyeop/experiments/visualization/{args.llm_model}/{args.source_dataset_name}/{mode}/{experiment_id}\")\n",
    "    os.makedirs(base_viz_dir, exist_ok=True)\n",
    "\n",
    "    # ìƒ˜í”Œë³„ ë””ë ‰í† ë¦¬ ë¯¸ë¦¬ ìƒì„±\n",
    "    sample_dirs = []\n",
    "    for i in range(max_samples):\n",
    "        # ê° ìƒ˜í”Œ ë””ë ‰í† ë¦¬\n",
    "        sample_dir = os.path.join(base_viz_dir, f'sample_{i}')\n",
    "        os.makedirs(sample_dir, exist_ok=True)\n",
    "        sample_dirs.append(sample_dir)\n",
    "        \n",
    "        # ê° ìƒ˜í”Œ ë‚´ì— heatmapê³¼ graph í´ë” ìƒì„±\n",
    "        heatmap_dir = os.path.join(sample_dir, 'heatmap')\n",
    "        graph_dir = os.path.join(sample_dir, 'graph')\n",
    "        os.makedirs(heatmap_dir, exist_ok=True)\n",
    "        os.makedirs(graph_dir, exist_ok=True)\n",
    "        \n",
    "        # ê° í´ë” ë‚´ì— ë ˆì´ì–´ë³„ ì„œë¸Œí´ë” ìƒì„±\n",
    "        for layer_idx in range(len(model.layers)):\n",
    "            heatmap_layer_dir = os.path.join(heatmap_dir, f'layer_{layer_idx}')\n",
    "            graph_layer_dir = os.path.join(graph_dir, f'layer_{layer_idx}')\n",
    "            os.makedirs(heatmap_layer_dir, exist_ok=True)\n",
    "            os.makedirs(graph_layer_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        sample_count = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_on_device = {\n",
    "                k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in batch.items()\n",
    "            }\n",
    "            prediction = model.predict(batch_on_device)\n",
    "            \n",
    "            # ë°°ì¹˜ í¬ê¸° í™•ì¸\n",
    "            batch_size = model.layers[0].attn_weights.shape[0]\n",
    "            \n",
    "            for sample_idx in range(batch_size):\n",
    "                # íŠ¹ì„± ì´ë¦„ ì •ë¦¬ (ëª¨ë“  ë ˆì´ì–´ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©)\n",
    "                feature_names = []\n",
    "                if 'cat_desc_texts' in batch_on_device:\n",
    "                    for feature in batch_on_device['cat_desc_texts']:\n",
    "                        if isinstance(feature, tuple):\n",
    "                            clean_name = str(feature[0])\n",
    "                        else:\n",
    "                            try:\n",
    "                                clean_name = feature.split(\"'\")[1] if \"'\" in feature else feature\n",
    "                                clean_name = clean_name.split(',')[0]\n",
    "                            except:\n",
    "                                clean_name = str(feature)\n",
    "                        feature_names.append(clean_name)\n",
    "\n",
    "                if 'num_desc_texts' in batch_on_device:\n",
    "                    for feature in batch_on_device['num_desc_texts']:\n",
    "                        if isinstance(feature, tuple):\n",
    "                            clean_name = str(feature[0])\n",
    "                        else:\n",
    "                            try:\n",
    "                                clean_name = feature.split(\"'\")[1] if \"'\" in feature else feature\n",
    "                                clean_name = clean_name.split(',')[0]\n",
    "                            except:\n",
    "                                clean_name = str(feature)\n",
    "                        feature_names.append(clean_name)\n",
    "                        \n",
    "                # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)\n",
    "                seen = set()\n",
    "                unique_features = []\n",
    "                for feat in feature_names:\n",
    "                    if feat not in seen:\n",
    "                        seen.add(feat)\n",
    "                        unique_features.append(feat)\n",
    "                feature_names = unique_features\n",
    "                \n",
    "                # 1. íˆíŠ¸ë§µ ì‹œê°í™”\n",
    "                if args.viz_heatmap:\n",
    "                    # ì‹œê°í™” ì‹œì ì—ì„œë§Œ í´ëŸ¬ìŠ¤í„°ë§ ë¦¬ì…‹ (ì²« ë²ˆì§¸ ìƒ˜í”Œì—ì„œë§Œ)\n",
    "                    if sample_count == 0:\n",
    "                        model.reset_epoch_clustering()\n",
    "                        \n",
    "                        # í˜„ì¬ ì—í¬í¬ì˜ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•´ data_loader ìˆœíšŒ\n",
    "                        model.train()  # attention ìˆ˜ì§‘ìš©\n",
    "                        with torch.no_grad():\n",
    "                            for batch in data_loader:\n",
    "                                batch_on_device = {\n",
    "                                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                                    for k, v in batch.items()\n",
    "                                }\n",
    "                                _ = model.predict(batch_on_device)  # attention maps ìˆ˜ì§‘\n",
    "                        \n",
    "                        model.stop_attention_collection()\n",
    "                        model.eval()\n",
    "                    \n",
    "                    # ìˆ˜ì§‘ ì™„ë£Œ í›„ í´ëŸ¬ìŠ¤í„°ë§ ì—…ë°ì´íŠ¸\n",
    "                    clustering_updated = model.update_attention_clustering()\n",
    "                    \n",
    "                    # í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "                    clustering_info = model.get_clustering_info()\n",
    "                    \n",
    "                    # 1. ê¸°ì¡´: ê° ë ˆì´ì–´ë³„ ì‹œê°í™” (sample_*/heatmap/layer_*/ì— ì €ì¥)\n",
    "                    for layer_idx in range(len(model.layers)):\n",
    "                        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "                        \n",
    "                        # 1. Attention Map íˆíŠ¸ë§µ \n",
    "                        batch_size = model.layers[layer_idx].attn_weights.shape[0]\n",
    "                        actual_sample_idx = min(sample_idx, batch_size - 1)  # ë°°ì¹˜ í¬ê¸° ì´ˆê³¼ ë°©ì§€\n",
    "                        attn_weights = model.layers[layer_idx].attn_weights[actual_sample_idx]  # [n_heads, seq, seq]\n",
    "                        attn_weights_mean = attn_weights.mean(dim=0).cpu().numpy()  # í—¤ë“œë³„ í‰ê· \n",
    "                        \n",
    "                        # CLS í† í° í¬í•¨í•œ feature names\n",
    "                        all_node_names = [\"CLS\"] + feature_names \n",
    "                        \n",
    "                        im1 = axes[0].imshow(attn_weights_mean, cmap='viridis', interpolation='nearest')\n",
    "                        axes[0].set_title(f'Attention Map - Layer {layer_idx}', fontsize=14)\n",
    "                        fig.colorbar(im1, ax=axes[0])\n",
    "                        \n",
    "                        # ì¶• ë¼ë²¨ ì„¤ì •\n",
    "                        axes[0].set_xticks(np.arange(len(all_node_names)))\n",
    "                        axes[0].set_yticks(np.arange(len(all_node_names)))\n",
    "                        axes[0].set_xticklabels(all_node_names, rotation=90, fontsize=8)\n",
    "                        axes[0].set_yticklabels(all_node_names, fontsize=8)\n",
    "                        \n",
    "                        # ê° ì…€ì— ê°’ í‘œì‹œ\n",
    "                        for i in range(len(all_node_names)):\n",
    "                            for j in range(len(all_node_names)):\n",
    "                                axes[0].text(j, i, f\"{attn_weights_mean[i,j]:.2f}\", \n",
    "                                        ha=\"center\", va=\"center\", \n",
    "                                        color=\"white\" if attn_weights_mean[i,j] > 0.5 else \"black\", \n",
    "                                        fontsize=7)\n",
    "                        \n",
    "                        # 2. ì˜¤ë¥¸ìª½: í•´ë‹¹í•˜ëŠ” í´ëŸ¬ìŠ¤í„° centroid í‘œì‹œ\n",
    "                        if (layer_idx == len(model.layers) - 1 and  # ë§ˆì§€ë§‰ ë ˆì´ì–´(Layer 2)ì´ê³ \n",
    "                            clustering_info['cluster_centroids'] is not None and \n",
    "                            len(clustering_info['cluster_assignments']) > 0):\n",
    "                            \n",
    "                            # í˜„ì¬ ìƒ˜í”Œì˜ attention mapì´ ì–´ëŠ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ”ì§€ ì°¾ê¸°\n",
    "                            sample_attention = attn_weights_mean  # í˜„ì¬ ìƒ˜í”Œì˜ attention map\n",
    "                            \n",
    "                            # ëª¨ë“  centroidì™€ì˜ ê±°ë¦¬ ê³„ì‚°\n",
    "                            min_distance = float('inf')\n",
    "                            assigned_cluster = 0\n",
    "                            \n",
    "                            for cluster_id, centroid in enumerate(clustering_info['cluster_centroids']):\n",
    "                                centroid_np = centroid.detach().cpu().numpy()\n",
    "                                # Frobenius norm ê±°ë¦¬ ê³„ì‚°\n",
    "                                distance = np.linalg.norm(sample_attention - centroid_np, 'fro')\n",
    "                                if distance < min_distance:\n",
    "                                    min_distance = distance\n",
    "                                    assigned_cluster = cluster_id\n",
    "                            \n",
    "                            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ centroid í‘œì‹œ\n",
    "                            assigned_centroid = clustering_info['cluster_centroids'][assigned_cluster].detach().cpu().numpy()\n",
    "                            \n",
    "                            im2 = axes[1].imshow(assigned_centroid, cmap='viridis', interpolation='nearest')\n",
    "                            axes[1].set_title(f'Closest Cluster Centroid - Cluster {assigned_cluster}\\n(Distance: {min_distance:.3f})', fontsize=14)\n",
    "                            fig.colorbar(im2, ax=axes[1])\n",
    "                            \n",
    "                            # ì¶• ë¼ë²¨ ì„¤ì •\n",
    "                            axes[1].set_xticks(np.arange(len(all_node_names)))\n",
    "                            axes[1].set_yticks(np.arange(len(all_node_names)))\n",
    "                            axes[1].set_xticklabels(all_node_names, rotation=90, fontsize=8)\n",
    "                            axes[1].set_yticklabels(all_node_names, fontsize=8)\n",
    "                            \n",
    "                            # ê° ì…€ì— ê°’ í‘œì‹œ\n",
    "                            for i in range(len(all_node_names)):\n",
    "                                for j in range(len(all_node_names)):\n",
    "                                    axes[1].text(j, i, f\"{assigned_centroid[i,j]:.2f}\", \n",
    "                                            ha=\"center\", va=\"center\", \n",
    "                                            color=\"white\" if assigned_centroid[i,j] > 0.5 else \"black\", \n",
    "                                            fontsize=7)\n",
    "                        else:\n",
    "                            # ë§ˆì§€ë§‰ ë ˆì´ì–´ê°€ ì•„ë‹ˆê±°ë‚˜ í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹\n",
    "                            axes[1].text(0.5, 0.5, f'Layer {layer_idx} Attention Pattern\\n\\nFull clustering results\\navailable in clustering/ folder\\n\\nLayer 2 = Final clustering layer', \n",
    "                                    ha='center', va='center', transform=axes[1].transAxes, fontsize=14,\n",
    "                                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcyan\", alpha=0.8))\n",
    "                            axes[1].set_title(f'Layer {layer_idx} - See clustering/ for full results', fontsize=14)\n",
    "                            axes[1].axis('off')\n",
    "                        \n",
    "                        # ì „ì²´ íƒ€ì´í‹€\n",
    "                        fig.suptitle(f'Layer {layer_idx} Attention Analysis - Epoch {epoch} - Sample {sample_count}', fontsize=16)\n",
    "                        plt.tight_layout()\n",
    "                        \n",
    "                        # ê¸°ì¡´ ê²½ë¡œì— ì €ì¥\n",
    "                        heatmap_path = os.path.join(sample_dirs[sample_count], 'heatmap', f'layer_{layer_idx}', f'epoch_{epoch}.png')\n",
    "                        fig.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "                        logger.info(f\"Epoch {epoch} - ìƒ˜í”Œ {sample_count} ë ˆì´ì–´ {layer_idx} íˆíŠ¸ë§µ ì €ì¥: {heatmap_path}\")\n",
    "\n",
    "                    # 2. ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (clustering/ í´ë”ì— ì €ì¥) - ì²« ë²ˆì§¸ ìƒ˜í”Œì—ì„œë§Œ ìƒì„±\n",
    "                    if sample_count == 0:  # ì¤‘ë³µ ë°©ì§€: ì²« ë²ˆì§¸ ìƒ˜í”Œì—ì„œë§Œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™” ìƒì„±\n",
    "                        # clustering í´ë” ìƒì„±\n",
    "                        clustering_dir = os.path.join(base_viz_dir, 'clustering')\n",
    "                        os.makedirs(clustering_dir, exist_ok=True)\n",
    "                        model.save_cluster_centroids(clustering_dir, epoch)\n",
    "                        if clustering_info['cluster_centroids'] is not None:\n",
    "                            visualize_cluster_centroids(clustering_info, clustering_dir, epoch, feature_names)\n",
    "    \n",
    "                        # ğŸ†• 1x3 í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”\n",
    "                        fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "                        \n",
    "                        # í”Œë¡¯ 1: ì „ì²´ ë°ì´í„° í´ëŸ¬ìŠ¤í„°ë§ (ê¸°ì¡´)\n",
    "                        cluster_assignments = clustering_info['cluster_assignments']\n",
    "                        attention_maps = clustering_info['attention_maps']\n",
    "                        attention_labels = clustering_info['attention_labels']\n",
    "                        \n",
    "                        # ì „ì²´ ë°ì´í„° í”Œë¡¯\n",
    "                        if (clustering_info['cluster_centroids'] is not None and \n",
    "                            len(cluster_assignments) > 0 and len(attention_maps) > 0):\n",
    "                            \n",
    "                            try:\n",
    "                                from sklearn.manifold import TSNE\n",
    "                                \n",
    "                                cluster_assignments = np.array(cluster_assignments)\n",
    "                                attention_labels = np.array(attention_labels)\n",
    "                                \n",
    "                                # attention mapsë¥¼ numpyë¡œ ë³€í™˜\n",
    "                                attention_np = torch.stack(attention_maps).detach().cpu().numpy()\n",
    "                                n_maps, seq_len, seq_len2 = attention_np.shape\n",
    "                                \n",
    "                                # í‰íƒ„í™”í•´ì„œ t-SNE ì ìš©\n",
    "                                flattened_maps = attention_np.reshape(n_maps, -1)\n",
    "                                \n",
    "                                if n_maps >= 2:\n",
    "                                    perplexity = min(30, n_maps-1, max(1, n_maps//3))\n",
    "                                    \n",
    "                                    # Centroid ì²˜ë¦¬\n",
    "                                    if clustering_info['cluster_centroids'] is not None:\n",
    "                                        cluster_centroids = clustering_info['cluster_centroids']\n",
    "                                        \n",
    "                                        if isinstance(cluster_centroids, torch.Tensor):\n",
    "                                            centroids_np = cluster_centroids.detach().cpu().numpy()\n",
    "                                        else:\n",
    "                                            centroids_np = torch.stack(cluster_centroids).detach().cpu().numpy()\n",
    "                                        \n",
    "                                        centroids_flat = centroids_np.reshape(len(centroids_np), -1)\n",
    "                                        \n",
    "                                        # ì „ì²´ ë°ì´í„°(attention maps + centroids)ë¥¼ í•¨ê»˜ t-SNE ë³€í™˜\n",
    "                                        all_data = np.vstack([flattened_maps, centroids_flat])\n",
    "                                        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "                                        tsne_all_embeddings = tsne.fit_transform(all_data)\n",
    "                                        \n",
    "                                        # ì›ë³¸ ë°ì´í„°ì™€ centroid ë¶„ë¦¬\n",
    "                                        tsne_embeddings = tsne_all_embeddings[:n_maps]\n",
    "                                        centroid_embeddings = tsne_all_embeddings[n_maps:]\n",
    "                                    else:\n",
    "                                        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "                                        tsne_embeddings = tsne.fit_transform(flattened_maps)\n",
    "                                        centroid_embeddings = None\n",
    "                                    \n",
    "                                    # ì‹œê°í™”\n",
    "                                    unique_clusters = np.unique(cluster_assignments)\n",
    "                                    unique_labels = np.unique(attention_labels)\n",
    "                                    base_colors = plt.cm.Set3(np.linspace(0, 1, max(len(unique_clusters), 1)))\n",
    "                                    \n",
    "                                    for i, cluster_id in enumerate(unique_clusters):\n",
    "                                        cluster_mask = cluster_assignments == cluster_id\n",
    "                                        cluster_points = tsne_embeddings[cluster_mask]\n",
    "                                        cluster_labels = attention_labels[cluster_mask]\n",
    "                                        \n",
    "                                        if len(cluster_points) > 0:\n",
    "                                            for label in unique_labels:\n",
    "                                                label_mask = cluster_labels == label\n",
    "                                                if np.any(label_mask):\n",
    "                                                    label_points = cluster_points[label_mask]\n",
    "                                                    \n",
    "                                                    marker = 'o' if label == 0 else 's'\n",
    "                                                    marker_name = 'Label 0' if label == 0 else 'Label 1'\n",
    "                                                    \n",
    "                                                    axes[0].scatter(label_points[:, 0], label_points[:, 1], \n",
    "                                                            color=base_colors[i], \n",
    "                                                            label=f'Cluster {cluster_id} ({marker_name})', \n",
    "                                                            alpha=0.7, s=50, marker=marker)\n",
    "                                    \n",
    "                                    # Centroid í‘œì‹œ\n",
    "                                    if centroid_embeddings is not None:\n",
    "                                        for i, cluster_id in enumerate(unique_clusters):\n",
    "                                            if i < len(centroid_embeddings):\n",
    "                                                axes[0].scatter(centroid_embeddings[i, 0], centroid_embeddings[i, 1], \n",
    "                                                        marker='*', s=300, c='black', \n",
    "                                                        edgecolors=base_colors[i], linewidth=3,\n",
    "                                                        label='Centroids' if i == 0 else \"\", zorder=5)\n",
    "                                    \n",
    "                                    axes[0].set_title(f'All Data (Epoch {epoch})', fontsize=14)\n",
    "                                    axes[0].set_xlabel('t-SNE Dimension 1', fontsize=10)\n",
    "                                    axes[0].set_ylabel('t-SNE Dimension 2', fontsize=10)\n",
    "                                    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "                                    axes[0].grid(True, alpha=0.3)\n",
    "                                else:\n",
    "                                    axes[0].text(0.5, 0.5, f'All Data\\nNeed more data\\nCurrent: {n_maps}', \n",
    "                                        ha='center', va='center', transform=axes[0].transAxes, fontsize=12,\n",
    "                                        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "                                    axes[0].set_title('All Data', fontsize=14)\n",
    "                                    axes[0].axis('off')\n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"All data clustering visualization error: {e}\")\n",
    "                                axes[0].text(0.5, 0.5, f'All Data\\nError: {str(e)}', \n",
    "                                    ha='center', va='center', transform=axes[0].transAxes, fontsize=12,\n",
    "                                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                                axes[0].set_title('All Data', fontsize=14)\n",
    "                                axes[0].axis('off')\n",
    "                        else:\n",
    "                            axes[0].text(0.5, 0.5, 'All Data\\nNo clustering data', \n",
    "                                ha='center', va='center', transform=axes[0].transAxes, fontsize=12,\n",
    "                                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                            axes[0].set_title('All Data', fontsize=14)\n",
    "                            axes[0].axis('off')\n",
    "                        \n",
    "                        # í”Œë¡¯ 2: Label 0ë§Œ í´ëŸ¬ìŠ¤í„°ë§\n",
    "                        attention_maps_0 = clustering_info['attention_maps_label_0']\n",
    "                        cluster_assignments_0 = clustering_info['cluster_assignments_label_0']\n",
    "                        cluster_centroids_0 = clustering_info['cluster_centroids_label_0']\n",
    "                        \n",
    "                        # if (cluster_centroids_0 is not None and \n",
    "                        #     len(cluster_assignments_0) > 0 and len(attention_maps_0) > 0):\n",
    "                            \n",
    "                        #     try:\n",
    "                        #         cluster_assignments_0 = np.array(cluster_assignments_0)\n",
    "                                \n",
    "                        #         # attention mapsë¥¼ numpyë¡œ ë³€í™˜\n",
    "                        #         attention_np_0 = torch.stack(attention_maps_0).detach().cpu().numpy()\n",
    "                        #         n_maps_0, seq_len, seq_len2 = attention_np_0.shape\n",
    "                                \n",
    "                        #         flattened_maps_0 = attention_np_0.reshape(n_maps_0, -1)\n",
    "                                \n",
    "                        #         if n_maps_0 >= 2:\n",
    "                        #             perplexity_0 = min(30, n_maps_0-1, max(1, n_maps_0//3))\n",
    "                                    \n",
    "                        #             if isinstance(cluster_centroids_0, torch.Tensor):\n",
    "                        #                 centroids_np_0 = cluster_centroids_0.detach().cpu().numpy()\n",
    "                        #             else:\n",
    "                        #                 centroids_np_0 = torch.stack(cluster_centroids_0).detach().cpu().numpy()\n",
    "                                    \n",
    "                        #             centroids_flat_0 = centroids_np_0.reshape(len(centroids_np_0), -1)\n",
    "                                    \n",
    "                        #             all_data_0 = np.vstack([flattened_maps_0, centroids_flat_0])\n",
    "                        #             tsne_0 = TSNE(n_components=2, random_state=42, perplexity=perplexity_0)\n",
    "                        #             tsne_all_embeddings_0 = tsne_0.fit_transform(all_data_0)\n",
    "                                    \n",
    "                        #             tsne_embeddings_0 = tsne_all_embeddings_0[:n_maps_0]\n",
    "                        #             centroid_embeddings_0 = tsne_all_embeddings_0[n_maps_0:]\n",
    "                                    \n",
    "                        #             unique_clusters_0 = np.unique(cluster_assignments_0)\n",
    "                        #             base_colors_0 = plt.cm.Set3(np.linspace(0, 1, max(len(unique_clusters_0), 1)))\n",
    "                                    \n",
    "                        #             for i, cluster_id in enumerate(unique_clusters_0):\n",
    "                        #                 cluster_mask_0 = cluster_assignments_0 == cluster_id\n",
    "                        #                 cluster_points_0 = tsne_embeddings_0[cluster_mask_0]\n",
    "                                        \n",
    "                        #                 if len(cluster_points_0) > 0:\n",
    "                        #                     axes[1].scatter(cluster_points_0[:, 0], cluster_points_0[:, 1], \n",
    "                        #                             color=base_colors_0[i], \n",
    "                        #                             label=f'Cluster {cluster_id}', \n",
    "                        #                             alpha=0.7, s=50, marker='o')\n",
    "                                    \n",
    "                        #             # Centroid í‘œì‹œ\n",
    "                        #             for i, cluster_id in enumerate(unique_clusters_0):\n",
    "                        #                 if i < len(centroid_embeddings_0):\n",
    "                        #                     axes[1].scatter(centroid_embeddings_0[i, 0], centroid_embeddings_0[i, 1], \n",
    "                        #                             marker='*', s=300, c='black', \n",
    "                        #                             edgecolors=base_colors_0[i], linewidth=3,\n",
    "                        #                             label='Centroids' if i == 0 else \"\", zorder=5)\n",
    "                                    \n",
    "                        #             axes[1].set_title(f'Label 0 Only (Epoch {epoch})', fontsize=14)\n",
    "                        #             axes[1].set_xlabel('t-SNE Dimension 1', fontsize=10)\n",
    "                        #             axes[1].set_ylabel('t-SNE Dimension 2', fontsize=10)\n",
    "                        #             axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "                        #             axes[1].grid(True, alpha=0.3)\n",
    "                        #         else:\n",
    "                        #             axes[1].text(0.5, 0.5, f'Label 0 Only\\nNeed more data\\nCurrent: {n_maps_0}', \n",
    "                        #                 ha='center', va='center', transform=axes[1].transAxes, fontsize=12,\n",
    "                        #                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "                        #             axes[1].set_title('Label 0 Only', fontsize=14)\n",
    "                        #             axes[1].axis('off')\n",
    "                        #     except Exception as e:\n",
    "                        #         logger.error(f\"Label 0 clustering visualization error: {e}\")\n",
    "                        #         axes[1].text(0.5, 0.5, f'Label 0 Only\\nError: {str(e)}', \n",
    "                        #             ha='center', va='center', transform=axes[1].transAxes, fontsize=12,\n",
    "                        #             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                        #         axes[1].set_title('Label 0 Only', fontsize=14)\n",
    "                        #         axes[1].axis('off')\n",
    "                        # else:\n",
    "                        #     axes[1].text(0.5, 0.5, 'Label 0 Only\\nNo clustering data', \n",
    "                        #         ha='center', va='center', transform=axes[1].transAxes, fontsize=12,\n",
    "                        #         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                        #     axes[1].set_title('Label 0 Only', fontsize=14)\n",
    "                        #     axes[1].axis('off')\n",
    "                        \n",
    "                        # # í”Œë¡¯ 3: Label 1ë§Œ í´ëŸ¬ìŠ¤í„°ë§\n",
    "                        # attention_maps_1 = clustering_info['attention_maps_label_1']\n",
    "                        # cluster_assignments_1 = clustering_info['cluster_assignments_label_1']\n",
    "                        # cluster_centroids_1 = clustering_info['cluster_centroids_label_1']\n",
    "                        \n",
    "                        # if (cluster_centroids_1 is not None and \n",
    "                        #     len(cluster_assignments_1) > 0 and len(attention_maps_1) > 0):\n",
    "                            \n",
    "                        #     try:\n",
    "                        #         cluster_assignments_1 = np.array(cluster_assignments_1)\n",
    "                                \n",
    "                        #         # attention mapsë¥¼ numpyë¡œ ë³€í™˜\n",
    "                        #         attention_np_1 = torch.stack(attention_maps_1).detach().cpu().numpy()\n",
    "                        #         n_maps_1, seq_len, seq_len2 = attention_np_1.shape\n",
    "                                \n",
    "                        #         flattened_maps_1 = attention_np_1.reshape(n_maps_1, -1)\n",
    "                                \n",
    "                        #         if n_maps_1 >= 2:\n",
    "                        #             perplexity_1 = min(30, n_maps_1-1, max(1, n_maps_1//3))\n",
    "                                    \n",
    "                        #             if isinstance(cluster_centroids_1, torch.Tensor):\n",
    "                        #                 centroids_np_1 = cluster_centroids_1.detach().cpu().numpy()\n",
    "                        #             else:\n",
    "                        #                 centroids_np_1 = torch.stack(cluster_centroids_1).detach().cpu().numpy()\n",
    "                                    \n",
    "                        #             centroids_flat_1 = centroids_np_1.reshape(len(centroids_np_1), -1)\n",
    "                                    \n",
    "                        #             all_data_1 = np.vstack([flattened_maps_1, centroids_flat_1])\n",
    "                        #             tsne_1 = TSNE(n_components=2, random_state=42, perplexity=perplexity_1)\n",
    "                        #             tsne_all_embeddings_1 = tsne_1.fit_transform(all_data_1)\n",
    "                                    \n",
    "                        #             tsne_embeddings_1 = tsne_all_embeddings_1[:n_maps_1]\n",
    "                        #             centroid_embeddings_1 = tsne_all_embeddings_1[n_maps_1:]\n",
    "                                    \n",
    "                        #             unique_clusters_1 = np.unique(cluster_assignments_1)\n",
    "                        #             base_colors_1 = plt.cm.Set3(np.linspace(0, 1, max(len(unique_clusters_1), 1)))\n",
    "                                    \n",
    "                        #             for i, cluster_id in enumerate(unique_clusters_1):\n",
    "                        #                 cluster_mask_1 = cluster_assignments_1 == cluster_id\n",
    "                        #                 cluster_points_1 = tsne_embeddings_1[cluster_mask_1]\n",
    "                                        \n",
    "                        #                 if len(cluster_points_1) > 0:\n",
    "                        #                     axes[2].scatter(cluster_points_1[:, 0], cluster_points_1[:, 1], \n",
    "                        #                             color=base_colors_1[i], \n",
    "                        #                             label=f'Cluster {cluster_id}', \n",
    "                        #                             alpha=0.7, s=50, marker='s')\n",
    "                                    \n",
    "                        #             # Centroid í‘œì‹œ\n",
    "                        #             for i, cluster_id in enumerate(unique_clusters_1):\n",
    "                        #                 if i < len(centroid_embeddings_1):\n",
    "                        #                     axes[2].scatter(centroid_embeddings_1[i, 0], centroid_embeddings_1[i, 1], \n",
    "                        #                             marker='*', s=300, c='black', \n",
    "                        #                             edgecolors=base_colors_1[i], linewidth=3,\n",
    "                        #                             label='Centroids' if i == 0 else \"\", zorder=5)\n",
    "                                    \n",
    "                        #             axes[2].set_title(f'Label 1 Only (Epoch {epoch})', fontsize=14)\n",
    "                        #             axes[2].set_xlabel('t-SNE Dimension 1', fontsize=10)\n",
    "                        #             axes[2].set_ylabel('t-SNE Dimension 2', fontsize=10)\n",
    "                        #             axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "                        #             axes[2].grid(True, alpha=0.3)\n",
    "                        #         else:\n",
    "                        #             axes[2].text(0.5, 0.5, f'Label 1 Only\\nNeed more data\\nCurrent: {n_maps_1}', \n",
    "                        #                 ha='center', va='center', transform=axes[2].transAxes, fontsize=12,\n",
    "                        #                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "                        #             axes[2].set_title('Label 1 Only', fontsize=14)\n",
    "                        #             axes[2].axis('off')\n",
    "                        #     except Exception as e:\n",
    "                        #         logger.error(f\"Label 1 clustering visualization error: {e}\")\n",
    "                        #         axes[2].text(0.5, 0.5, f'Label 1 Only\\nError: {str(e)}', \n",
    "                        #             ha='center', va='center', transform=axes[2].transAxes, fontsize=12,\n",
    "                        #             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                        #         axes[2].set_title('Label 1 Only', fontsize=14)\n",
    "                        #         axes[2].axis('off')\n",
    "                        # else:\n",
    "                        #     axes[2].text(0.5, 0.5, 'Label 1 Only\\nNo clustering data', \n",
    "                        #         ha='center', va='center', transform=axes[2].transAxes, fontsize=12,\n",
    "                        #         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                        #     axes[2].set_title('Label 1 Only', fontsize=14)\n",
    "                        #     axes[2].axis('off')\n",
    "                        \n",
    "                        # plt.tight_layout()\n",
    "                        \n",
    "                        # clustering í´ë”ì— ì €ì¥\n",
    "                        clustering_path = os.path.join(clustering_dir, f'epoch_{epoch}.png')\n",
    "                        fig.savefig(clustering_path, dpi=300, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "                        logger.info(f\"Epoch {epoch} - 1x3 í´ëŸ¬ìŠ¤í„°ë§ ì €ì¥: {clustering_path}\")\n",
    "\n",
    "                sample_count += 1\n",
    "                if sample_count >= max_samples:\n",
    "                    break\n",
    "            \n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "                # 2. ê·¸ë˜í”„ êµ¬ì¡° ì‹œê°í™”\n",
    "                if args.viz_graph:\n",
    "                    # ê° ë ˆì´ì–´ë³„ë¡œ ì‹œê°í™” ìˆ˜í–‰\n",
    "                    for layer_idx in range(len(model.layers)):\n",
    "                        # 1) Attention ê°€ì¤‘ì¹˜(í—¤ë“œ í‰ê· )\n",
    "                        attn_weights = model.layers[layer_idx].attn_weights[sample_idx]  # [n_heads, seq, seq]\n",
    "                        attn_weights_mean = attn_weights.mean(dim=0).cpu()\n",
    "\n",
    "                        # ì›ë³¸ adjacency ì‚¬ìš© (íˆíŠ¸ë§µê³¼ ì¼ì¹˜í•˜ëŠ” ê°’)\n",
    "                        adjacency = model.layers[layer_idx].adjacency[sample_idx].cpu()\n",
    "                        # adj_row_sums = adjacency.sum(axis=1, keepdims=True) + 1e-9\n",
    "                        # adjacency = adjacency / adj_row_sums\n",
    "                        new_seq = attn_weights_mean.shape[0]\n",
    "                        graph_matrix = torch.zeros((new_seq, new_seq), device=attn_weights_mean.device, dtype = torch.float)\n",
    "\n",
    "                        graph_matrix[1:, 1:] = adjacency  # ë³€ìˆ˜ ê°„ ì—°ê²°ì€ ì›ë³¸ adjacency ì‚¬ìš©\n",
    "                        graph_matrix[0, 1:] = 1.0  # CLS->ë³€ìˆ˜ ì—°ê²°\n",
    "                        graph_matrix[1:, 0] = 0.0  # ë³€ìˆ˜->CLS ì—°ê²°\n",
    "                        \n",
    "                        mask = (graph_matrix == 0)\n",
    "                        final_graph_matrix = (attn_weights_mean * graph_matrix).numpy()\n",
    "                        final_graph_matrix[mask.numpy()] = 0.0\n",
    "                        # row_sums = final_graph_matrix.sum(axis=1, keepdims=True)\n",
    "                        # final_graph_matrix = final_graph_matrix / (row_sums + 1e-9)  # stability ìœ„í•´ 1e-9 ë”í•¨ \n",
    "                        n_nodes = final_graph_matrix.shape[0]\n",
    "                        \n",
    "                        # 2) Edge ë¦¬ìŠ¤íŠ¸(ëª¨ë“  i->j) ìˆ˜ì§‘ (Barplotìš©)\n",
    "                        cls_edges_info = []  # CLSì—ì„œ ë‚˜ê°€ëŠ” ì—£ì§€\n",
    "                        var_edges_info = []  # ë‚˜ë¨¸ì§€ ì—£ì§€\n",
    "                        \n",
    "                        for i in range(n_nodes):\n",
    "                            for j in range(n_nodes):\n",
    "                                if i != j:\n",
    "                                    w = final_graph_matrix[i, j]\n",
    "                                    if i == 0:\n",
    "                                        cls_edges_info.append((f\"{i}->{j}\", w))\n",
    "                                    else:\n",
    "                                        var_edges_info.append((f\"{i}->{j}\", w))\n",
    "                        \n",
    "                        # topK ì ìš©\n",
    "                        top_k = min(10, len(var_edges_info))\n",
    "                        var_edges_info.sort(key=lambda x: x[1], reverse=True)\n",
    "                        var_edges_info = var_edges_info[:top_k]\n",
    "                        \n",
    "                        # ì „ì²´ í•©ì¹˜ê¸°\n",
    "                        edges_info = cls_edges_info + var_edges_info\n",
    "                        edges_info.sort(key=lambda x: x[1], reverse=True)\n",
    "                        edge_labels = [x[0] for x in edges_info]\n",
    "                        edge_weights = [x[1] for x in edges_info]\n",
    "                        \n",
    "                        # CLS ì—£ì§€ì™€ ì¼ë°˜ ì—£ì§€ êµ¬ë¶„ì„ ìœ„í•œ ìƒ‰ìƒ ë¦¬ìŠ¤íŠ¸\n",
    "                        bar_colors = []\n",
    "                        for label in edge_labels:\n",
    "                            if label.startswith(\"0->\"):\n",
    "                                bar_colors.append(\"crimson\")  # CLS ì—£ì§€ëŠ” ë¹¨ê°„ìƒ‰\n",
    "                            else:\n",
    "                                bar_colors.append(\"cornflowerblue\")  # ì¼ë°˜ ì—£ì§€ëŠ” íŒŒë€ìƒ‰\n",
    "                        \n",
    "                        # ë…¸ë“œ ì´ë¦„ ë§¤í•‘\n",
    "                        node_name_map = {0: \"CLS\"}\n",
    "                        for i in range(1, n_nodes):\n",
    "                            idx_feat = i - 1\n",
    "                            if idx_feat < len(feature_names):\n",
    "                                node_name_map[i] = feature_names[idx_feat]\n",
    "                            else:\n",
    "                                node_name_map[i] = f\"feature_{i}\"\n",
    "                                \n",
    "                        # xì¶• ë¼ë²¨ì— ì‚¬ìš©í•  ì´ë¦„ ë³€í™˜\n",
    "                        display_edge_labels = []\n",
    "                        for label in edge_labels:\n",
    "                            i, j = map(int, label.split('->'))\n",
    "                            display_edge_labels.append(f\"{node_name_map[i]}->{node_name_map[j]}\")\n",
    "                        \n",
    "                        # Figure & 2 Subplots ìƒì„±\n",
    "                        fig, axes = plt.subplots(2, 2, figsize=(24, 20))\n",
    "                        ax_bar = axes[0, 0]\n",
    "                        \n",
    "                        # -----(A) Left Subplot: Barplot)-----\n",
    "                        bars = ax_bar.bar(range(len(edge_weights)), edge_weights, color=bar_colors)\n",
    "                        \n",
    "                        # ê° ë°” ìœ„ì— attention score ê°’ í‘œì‹œ\n",
    "                        for i, (weight, label) in enumerate(zip(edge_weights, edge_labels)):\n",
    "                            ax_bar.text(i, weight + 0.01, f\"{weight:.3f}\", \n",
    "                                      ha='center', va='bottom', rotation=45, \n",
    "                                      fontsize=7, color='black')\n",
    "                        \n",
    "                        ax_bar.set_title(f'Top Edge Weights - Layer {layer_idx}', fontsize=12)\n",
    "                        ax_bar.set_xlabel('Edge (i->j)')\n",
    "                        ax_bar.set_ylabel('Attention Weight')\n",
    "                        # xì¶• ë¼ë²¨ (ë„ˆë¬´ ë§ìœ¼ë©´ íšŒì „)\n",
    "                        ax_bar.set_xticks(range(len(edge_labels)))\n",
    "                        ax_bar.set_xticklabels(display_edge_labels, rotation=90, fontsize=8)\n",
    "                        \n",
    "                        # -----(B) Right Subplot: Network Graph)-----\n",
    "                        ax_graph = axes[0, 1]\n",
    "                        G = nx.DiGraph()\n",
    "                        node_labels = {}\n",
    "\n",
    "                        for i in range(n_nodes):\n",
    "                            if i == 0:\n",
    "                                node_name = \"CLS\"\n",
    "                                node_color = \"red\"\n",
    "                            else:\n",
    "                                idx_feat = i - 1\n",
    "                                if idx_feat < len(feature_names):\n",
    "                                    node_name = feature_names[idx_feat]\n",
    "                                    node_color = \"blue\"\n",
    "                                else:\n",
    "                                    node_name = f\"feature_{i}\"\n",
    "                                    node_color = \"blue\"\n",
    "\n",
    "                            G.add_node(i, name=node_name, color=node_color)\n",
    "                            node_labels[i] = node_name\n",
    "\n",
    "                        # CLS->Var / Var->Var êµ¬ë¶„í•´ì„œ ê·¸ë¦¬ê¸°\n",
    "                        cls_min_edge_weight = 0.001\n",
    "                        min_edge_weight = 0.001\n",
    "                        for i in range(n_nodes):\n",
    "                            for j in range(n_nodes):\n",
    "                                if i == j:\n",
    "                                    continue\n",
    "\n",
    "                                w = final_graph_matrix[i, j]\n",
    "                                if i == 0 and j != 0:\n",
    "                                    # CLS->Var\n",
    "                                    if w > cls_min_edge_weight:\n",
    "                                        G.add_edge(i, j, weight=w, cls_to_var=True)\n",
    "                                elif j == 0:\n",
    "                                    # Var->CLSëŠ” í‘œì‹œ ì•ˆ í•¨\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    if w > min_edge_weight:\n",
    "                                        # Var->Var\n",
    "                                        G.add_edge(i, j, weight=w, cls_to_var=False)\n",
    "\n",
    "                        pos = {}\n",
    "                        pos[0] = np.array([0, 0])\n",
    "                        non_center_nodes = n_nodes - 1\n",
    "                        radius = 1.0\n",
    "                        for i_ in range(1, n_nodes):\n",
    "                            angle_ = 2 * np.pi * (i_ - 1) / non_center_nodes\n",
    "                            pos[i_] = np.array([radius * np.cos(angle_), radius * np.sin(angle_)])\n",
    "\n",
    "                        # ë°°ê²½ ê·¸ë¦¬ë“œ\n",
    "                        for r_ in [0.25, 0.5, 0.75, 1.0]:\n",
    "                            circle = plt.Circle((0, 0), r_, fill=False, color='lightgray', linestyle='--', alpha=0.5)\n",
    "                            ax_graph.add_patch(circle)\n",
    "                        for i_ in range(1, n_nodes):\n",
    "                            angle__ = 2 * np.pi * (i_ - 1) / non_center_nodes\n",
    "                            x_ = 1.1 * np.cos(angle__)\n",
    "                            y_ = 1.1 * np.sin(angle__)\n",
    "                            ax_graph.plot([0, x_], [0, y_], color='lightgray', linestyle='--', alpha=0.5)\n",
    "\n",
    "                        node_colors = [d[\"color\"] for _, d in G.nodes(data=True)]\n",
    "                        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=800, ax=ax_graph, edgecolors='gray')\n",
    "\n",
    "                        cls_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('cls_to_var')]\n",
    "                        var_edges = [(u, v) for u, v, d in G.edges(data=True) if not d.get('cls_to_var')]\n",
    "\n",
    "                        cls_weights = [G[u][v]['weight'] for (u, v) in cls_edges]\n",
    "                        var_weights = [G[u][v]['weight'] for (u, v) in var_edges]\n",
    "\n",
    "                        # CLS->Var: ë¹¨ê°• êµµì€ì„ \n",
    "                        if cls_edges:\n",
    "                            nx.draw_networkx_edges(\n",
    "                                G, pos,\n",
    "                                edgelist=cls_edges,\n",
    "                                width=[2 + w * 5 for w in cls_weights],\n",
    "                                alpha=0.7,\n",
    "                                edge_color='crimson',\n",
    "                                connectionstyle='arc3,rad=0.1',  \n",
    "                                arrowstyle='-|>',  # í™”ì‚´í‘œ ìŠ¤íƒ€ì¼ ë³€ê²½\n",
    "                                arrowsize=15,      # í™”ì‚´í‘œ í¬ê¸° í‚¤ìš°ê¸° (ê¸°ë³¸ê°’ë³´ë‹¤ í¬ê²Œ)\n",
    "                                node_size=800,\n",
    "                                ax=ax_graph\n",
    "                            )\n",
    "\n",
    "                        # Var->Var: íŒŒë‘ ì ì„ \n",
    "                        if var_edges:\n",
    "                            nx.draw_networkx_edges(\n",
    "                                G, pos,\n",
    "                                edgelist=var_edges,\n",
    "                                width=[1 + w * 2 for w in var_weights],\n",
    "                                edge_color='blue',\n",
    "                                style='dashed',\n",
    "                                arrowstyle='-|>',\n",
    "                                arrowsize=30,\n",
    "                                alpha=0.5,\n",
    "                                ax=ax_graph,\n",
    "                                arrows=True\n",
    "                            )\n",
    "\n",
    "                        label_options = {\n",
    "                            \"font_size\": 9,\n",
    "                            \"font_color\": \"black\",\n",
    "                            \"bbox\": dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n",
    "                        }\n",
    "                        nx.draw_networkx_labels(G, pos, labels=node_labels, ax=ax_graph, **label_options)\n",
    "\n",
    "                        ax_graph.set_title(f'Graph Structure - Layer {layer_idx} - Epoch {epoch} - Sample {sample_count}', fontsize=12)\n",
    "                        ax_graph.axis('off')\n",
    "                        ax_graph.set_aspect('equal')\n",
    "                        ax_graph.set_xlim([-1.2, 1.2])\n",
    "                        ax_graph.set_ylim([-1.2, 1.2])\n",
    "\n",
    "                        # 3. í™•ì¥ëœ graph matrix heatmap\n",
    "                        ax_graph_matrix = axes[1, 0]\n",
    "                        graph_matrix_np = graph_matrix.cpu().numpy() \n",
    "                        im_graph = ax_graph_matrix.imshow(graph_matrix_np, cmap=\"Blues\", interpolation='nearest')\n",
    "                        ax_graph_matrix.set_title(\"Graph Matrix (with CLS)\", fontsize=14)\n",
    "                        fig.colorbar(im_graph, ax=ax_graph_matrix)\n",
    "\n",
    "                        all_node_names = [\"CLS\"] + feature_names \n",
    "                        ax_graph_matrix.set_xticks(np.arange(len(all_node_names)))\n",
    "                        ax_graph_matrix.set_yticks(np.arange(len(all_node_names)))\n",
    "                        ax_graph_matrix.set_xticklabels(all_node_names, rotation=90, fontsize=8)\n",
    "                        ax_graph_matrix.set_yticklabels(all_node_names, fontsize=8)\n",
    "\n",
    "                        for i in range(len(all_node_names)):\n",
    "                            for j in range(len(all_node_names)):\n",
    "                                ax_graph_matrix.text(j, i, f\"{graph_matrix_np[i,j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\" if graph_matrix_np[i,j] < 0.5 else \"white\", fontsize=8)\n",
    "\n",
    "                        ax_final = axes[1, 1]\n",
    "                        vmax = final_graph_matrix.max()\n",
    "                        vmin = 0.0  # 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ì„¤ì •\n",
    "\n",
    "                        # ë‹¤ë¥¸ ì»¬ëŸ¬ë§µ ì‚¬ìš© ë° ë²”ìœ„ ì¡°ì •\n",
    "                        im_final = ax_final.imshow(final_graph_matrix, \n",
    "                                                cmap='YlOrRd',  # 'YlOrRd', 'hot', 'OrRd' ë“± ì‹œë„í•´ë³¼ ìˆ˜ ìˆìŒ\n",
    "                                                interpolation='nearest',\n",
    "                                                vmin=vmin, \n",
    "                                                vmax=vmax)\n",
    "                        ax_final.set_title(\"Final Graph Matrix (Attention * Graph_matrix)\", fontsize=14)\n",
    "                        fig.colorbar(im_final, ax=ax_final)\n",
    "                        \n",
    "                        ax_final.set_xticks(np.arange(len(all_node_names)))\n",
    "                        ax_final.set_yticks(np.arange(len(all_node_names)))\n",
    "                        ax_final.set_xticklabels(all_node_names, rotation=90, fontsize=8)\n",
    "                        ax_final.set_yticklabels(all_node_names, fontsize=8)\n",
    "                        \n",
    "                        # ê° ì…€ì— ê°’ í‘œì‹œ\n",
    "                        for i in range(len(all_node_names)):\n",
    "                            for j in range(len(all_node_names)):\n",
    "                                # ìƒëŒ€ì ì¸ ê°’ì— ë”°ë¼ í…ìŠ¤íŠ¸ ìƒ‰ìƒ ê²°ì • (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê²€ì •, ìµœëŒ€ê°’ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í°ìƒ‰)\n",
    "                                relative_value = final_graph_matrix[i,j] / vmax if vmax > 0 else 0\n",
    "                                text_color = \"black\" if relative_value < 0.7 else \"white\"\n",
    "                                \n",
    "                                # ê°’ì´ 0ì¼ ê²½ìš° ë¹ˆ ë¬¸ìì—´ í‘œì‹œí•  ìˆ˜ë„ ìˆìŒ\n",
    "                                value_text = f\"{final_graph_matrix[i,j]:.3f}\"\n",
    "                                \n",
    "                                ax_final.text(j, i, value_text, \n",
    "                                            ha=\"center\", va=\"center\", \n",
    "                                            color=text_color, \n",
    "                                            fontsize=7)\n",
    "                        \n",
    "                        # ì „ì²´ ì œëª© ì„¤ì •\n",
    "                        fig.suptitle(f'Layer {layer_idx} - Epoch {epoch} - Sample {sample_count}', fontsize=18)\n",
    "                        fig.tight_layout(rect=[0, 0.03, 1, 0.97])  # suptitleì„ ìœ„í•œ ì—¬ë°± í™•ë³´\n",
    "                        \n",
    "                        # # ë ˆì´ì–´ë³„ í´ë”ì— ì €ì¥\n",
    "                        # layer_dir = os.path.join(sample_dirs[sample_count], f'layer_{layer_idx}')\n",
    "                        # graph_path = os.path.join(layer_dir, f'epoch_{epoch}_complete.png')\n",
    "                        # fig.savefig(graph_path, dpi=300, bbox_inches='tight')\n",
    "                        # plt.close(fig)\n",
    "                        \n",
    "                        # logger.info(f\"ìƒ˜í”Œ {sample_count} - ë ˆì´ì–´ {layer_idx} - ì—í¬í¬ {epoch} ì¢…í•© ì‹œê°í™” ì €ì¥: {graph_path}\")\n",
    "                        graph_path = os.path.join(sample_dirs[sample_count], 'graph', f'layer_{layer_idx}', f'epoch_{epoch}_complete.png')\n",
    "                        fig.savefig(graph_path, dpi=300, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "                        logger.info(f\"ìƒ˜í”Œ {sample_count} - ë ˆì´ì–´ {layer_idx} - ì—í¬í¬ {epoch} ê·¸ë˜í”„ ì‹œê°í™” ì €ì¥: {graph_path}\")\n",
    "                sample_count += 1\n",
    "                if sample_count >= max_samples:\n",
    "                    break\n",
    "\n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_gmm_clusters(gmm, embeddings, output_dir=\"visualizations/gmm_clusters\", step=None, filename=None):\n",
    "    \"\"\"\n",
    "    t-SNE ì‹œê°í™”ë§Œ ì œê³µí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        gmm: GMM ë˜ëŠ” GMM2 ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤\n",
    "        embeddings: ì…ë ¥ ì„ë² ë”© [batch_size, input_dim]\n",
    "        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "        step: ìŠ¤í… ë²ˆí˜¸ (íŒŒì¼ëª…ìš©)\n",
    "        filename: ì‚¬ìš©ì ì§€ì • íŒŒì¼ëª… (ì„ íƒì‚¬í•­)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.manifold import TSNE\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "        device = embeddings.device\n",
    "        \n",
    "        # GMMìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° í• ë‹¹ í™•ë¥  ê³„ì‚°\n",
    "        if hasattr(gmm, 'forward'):\n",
    "            with torch.no_grad():\n",
    "                if 'GMM2' in gmm.__class__.__name__:\n",
    "                    r, _, _ = gmm(embeddings, is_train=False)\n",
    "                else:\n",
    "                    r, _ = gmm(embeddings, is_train=False)\n",
    "                \n",
    "                # ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í´ëŸ¬ìŠ¤í„° í• ë‹¹\n",
    "                cluster_assignments = torch.argmax(r, dim=1).cpu().numpy()\n",
    "                # í• ë‹¹ í™•ë¥  (ì‹ ë¢°ë„)\n",
    "                confidences = torch.max(r, dim=1)[0].cpu().numpy()\n",
    "                # í”„ë¡œí† íƒ€ì… ê°€ì ¸ì˜¤ê¸°\n",
    "                prototypes = gmm.running_prototypes.detach().cpu().numpy()\n",
    "        else:\n",
    "            # GMM ê°ì²´ê°€ ì•„ë‹Œ ê²½ìš° (ë””ë²„ê¹…ìš©)\n",
    "            prototypes = np.random.randn(32, embeddings.shape[1])\n",
    "            cluster_assignments = np.random.randint(0, 32, size=embeddings.shape[0])\n",
    "            confidences = np.random.rand(embeddings.shape[0])\n",
    "        \n",
    "        # ì„ë² ë”©ì„ CPUë¡œ ì´ë™\n",
    "        embeddings_np = embeddings.detach().cpu().numpy()\n",
    "        \n",
    "        # prototypesê°€ 3ì°¨ì›ì¸ ê²½ìš° 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
    "        if prototypes.ndim == 3:  # [1, num_prototypes, input_dim] í˜•íƒœì¸ ê²½ìš°\n",
    "            prototypes = prototypes.squeeze(0)  # [num_prototypes, input_dim] í˜•íƒœë¡œ ë³€í™˜\n",
    "        \n",
    "        # t-SNEë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œí† íƒ€ì…ê³¼ ì„ë² ë”©ì„ 2Dë¡œ íˆ¬ì˜\n",
    "        combined_data = np.vstack([prototypes, embeddings_np])\n",
    "        \n",
    "        # t-SNE ì°¨ì› ì¶•ì†Œ (perplexity ì¡°ì •)\n",
    "        tsne = TSNE(n_components=2, perplexity=min(30, len(combined_data)-1), \n",
    "                   random_state=42, learning_rate=200)\n",
    "        combined_reduced = tsne.fit_transform(combined_data)\n",
    "        \n",
    "        # í”„ë¡œí† íƒ€ì…ê³¼ ì„ë² ë”©ìœ¼ë¡œ ë‹¤ì‹œ ë¶„ë¦¬\n",
    "        reduced_prototypes = combined_reduced[:len(prototypes)]\n",
    "        reduced_embeddings = combined_reduced[len(prototypes):]\n",
    "        \n",
    "        # ===== t-SNE ì‹œê°í™”ë§Œ ìƒì„± =====\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # ì£¼ìš” í”„ë¡œí† íƒ€ì… ì¶”ì  (ì‹œê°í™”ìš©)\n",
    "        key_prototypes = set(cluster_assignments)\n",
    "        \n",
    "        # í´ëŸ¬ìŠ¤í„° í†µê³„ ê³„ì‚°\n",
    "        cluster_counts = {}\n",
    "        for c in cluster_assignments:\n",
    "            cluster_counts[c] = cluster_counts.get(c, 0) + 1\n",
    "        \n",
    "        largest_cluster = max(cluster_counts.items(), key=lambda x: x[1]) if cluster_counts else (-1, 0)\n",
    "        empty_clusters = len(prototypes) - len(cluster_counts)\n",
    "        \n",
    "        # í”„ë¡œí† íƒ€ì… ì‹œê°í™”\n",
    "        for i, (x, y) in enumerate(reduced_prototypes):\n",
    "            # ì£¼ìš” í”„ë¡œí† íƒ€ì… ê°•ì¡° (ìƒ˜í”Œì´ í• ë‹¹ëœ ê²ƒë“¤)\n",
    "            if i in key_prototypes:\n",
    "                marker_size = 250\n",
    "                edge_width = 2.5\n",
    "                zorder = 10\n",
    "                marker_style = '*'\n",
    "                plt.scatter(x, y, s=marker_size, c=f'C{i % 10}', marker=marker_style, \n",
    "                           edgecolors='black', linewidths=edge_width, alpha=0.9,\n",
    "                           zorder=zorder, label=f'P{i+1} (Main)')\n",
    "                \n",
    "                # í”„ë¡œí† íƒ€ì… ë ˆì´ë¸” (ëˆˆì— ë„ê²Œ)\n",
    "                plt.annotate(f'P{i+1}', (x, y), fontsize=14, fontweight='bold',\n",
    "                            ha='center', va='center', color='white',\n",
    "                            bbox=dict(boxstyle='round,pad=0.4', fc=f'C{i % 10}', alpha=0.8),\n",
    "                            zorder=zorder+1)\n",
    "            else:\n",
    "                # ë¹„í™œì„± í”„ë¡œí† íƒ€ì… (ìƒ˜í”Œì´ í• ë‹¹ë˜ì§€ ì•Šì€ ê²ƒë“¤)\n",
    "                marker_size = 100\n",
    "                plt.scatter(x, y, s=marker_size, c=f'C{i % 10}', marker='o', \n",
    "                           alpha=0.4, zorder=5)\n",
    "                plt.annotate(f'P{i+1}', (x, y), fontsize=9, ha='center', va='center', \n",
    "                            zorder=6)\n",
    "        \n",
    "        # ìƒ˜í”Œ ì‹œê°í™” (ë‹¤ì´ì•„ëª¬ë“œ ëª¨ì–‘)\n",
    "        for j, (x, y) in enumerate(reduced_embeddings):\n",
    "            cluster_id = cluster_assignments[j]\n",
    "            confidence = confidences[j]\n",
    "            \n",
    "            # ìƒ˜í”Œ ë§ˆì»¤\n",
    "            plt.scatter(x, y, s=250, c='white', marker='D', edgecolors='black', \n",
    "                       linewidths=2, alpha=0.9, zorder=15)\n",
    "            \n",
    "            # ìƒ˜í”Œ ID\n",
    "            plt.annotate(f'S{j+1}', (x, y), fontsize=12, fontweight='bold', \n",
    "                        ha='center', va='center', zorder=16)\n",
    "            \n",
    "            for i, (proto_x, proto_y) in enumerate(reduced_prototypes):\n",
    "                if i == cluster_id:\n",
    "                    # í• ë‹¹ í™•ë¥ ì— ë¹„ë¡€í•˜ëŠ” ì„  ë‘ê»˜\n",
    "                    line_width = 1.5 + 5 * confidence\n",
    "                    plt.plot([x, proto_x], [y, proto_y], '--', \n",
    "                            linewidth=line_width, alpha=0.7, \n",
    "                            c=f'C{i % 10}', zorder=1)\n",
    "                    \n",
    "                    # í• ë‹¹ í™•ë¥  í‘œì‹œ\n",
    "                    mid_x = (x + proto_x) * 0.6 + (proto_x + x) * 0.4\n",
    "                    mid_y = (y + proto_y) * 0.6 + (proto_y + y) * 0.4\n",
    "                    plt.annotate(f'{confidence:.2f}', (mid_x, mid_y), \n",
    "                                fontsize=11, fontweight='bold',\n",
    "                                bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.9),\n",
    "                                zorder=7)\n",
    "        \n",
    "        # í´ëŸ¬ìŠ¤í„°ë§ í†µê³„ ì •ë³´ ì¶”ê°€\n",
    "        info_text = f\"Largest cluster: {largest_cluster[1]} samples (P{largest_cluster[0]+1})\\n\"\n",
    "        info_text += f\"Empty clusters: {empty_clusters}\\n\"\n",
    "        \n",
    "        for c, count in sorted(cluster_counts.items()):\n",
    "            info_text += f\"Cluster {c+1}: {count} samples\\n\"\n",
    "        \n",
    "        plt.figtext(0.02, 0.02, info_text, fontsize=10,\n",
    "                  bbox=dict(boxstyle='round', fc='whitesmoke', alpha=0.9))\n",
    "        \n",
    "        # ê·¸ë˜í”„ ì œëª© ë° ë ˆì´ë¸”\n",
    "        plt.title('t-SNE Visualization of Sample-Prototype Assignments', fontsize=14)\n",
    "        plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "        plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        if key_prototypes:\n",
    "            plt.legend(loc='upper right', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        if filename is None:\n",
    "            if step is not None:\n",
    "                filename = f\"tsne_only_{step}.png\"\n",
    "            else:\n",
    "                filename = \"tsne_only.png\"\n",
    "                \n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"t-SNE visualization saved to {os.path.join(output_dir, filename)}\")\n",
    "        \n",
    "        return {\n",
    "            'cluster_assignments': cluster_assignments,\n",
    "            'confidences': confidences,\n",
    "            'prototypes': reduced_prototypes,\n",
    "            'embeddings': reduced_embeddings\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_results(args, results, exp_dir):\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # few-shotì´ 8 ì´ìƒì¼ ë•ŒëŠ” few-shot ê²°ê³¼ë§Œ ì‹œê°í™”\n",
    "    if args.few_shot > 4:\n",
    "        fig, (ax3, ax4) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Few-shotì˜ Train vs Valid\n",
    "        ax3.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_train_few_losses\"], label='Train Loss')\n",
    "        ax3.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_val_few_losses\"], label='Valid Loss')\n",
    "        ax3.set_xlabel('Epochs')\n",
    "        ax3.set_ylabel('Loss')\n",
    "        ax3.set_title('Few-shot: Train vs Valid Loss')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "\n",
    "        ax4.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_train_few_auc\"], label='Train AUC')\n",
    "        ax4.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_val_few_auc\"], label='Valid AUC')\n",
    "        ax4.set_xlabel('Epochs')\n",
    "        ax4.set_ylabel('AUC')\n",
    "        ax4.set_title('Few-shot: Train vs Valid AUC')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "\n",
    "    # few-shotì´ 4ì¼ ë•ŒëŠ” fullê³¼ few-shot ëª¨ë‘ ì‹œê°í™”\n",
    "    else:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Full datasetì˜ Train vs Valid\n",
    "        ax1.plot(results[\"Full_results\"][\"Ours\"][\"Ours_train_full_losses\"], label='Train Loss')\n",
    "        ax1.plot(results[\"Full_results\"][\"Ours\"][\"Ours_val_full_losses\"], label='Valid Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Full Dataset: Train vs Valid Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(results[\"Full_results\"][\"Ours\"][\"Ours_train_full_auc\"], label='Train AUC')\n",
    "        ax2.plot(results[\"Full_results\"][\"Ours\"][\"Ours_val_full_auc\"], label='Valid AUC')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('AUC')\n",
    "        ax2.set_title('Full Dataset: Train vs Valid AUC')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # Few-shotì˜ Train vs Valid\n",
    "        ax3.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_train_few_losses\"], label='Train Loss')\n",
    "        ax3.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_val_few_losses\"], label='Valid Loss')\n",
    "        ax3.set_xlabel('Epochs')\n",
    "        ax3.set_ylabel('Loss')\n",
    "        ax3.set_title('Few-shot: Train vs Valid Loss')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "\n",
    "        ax4.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_train_few_auc\"], label='Train AUC')\n",
    "        ax4.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_val_few_auc\"], label='Valid AUC')\n",
    "        ax4.set_xlabel('Epochs')\n",
    "        ax4.set_ylabel('AUC')\n",
    "        ax4.set_title('Few-shot: Train vs Valid AUC')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "\n",
    "    plt.suptitle(f'Training Progress - {args.source_dataset_name} (K={args.few_shot})', y=1.02, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    metrics_plot_path = os.path.join(exp_dir, f\"f{args.few_shot}_b{args.batch_size}_l{args.num_layers}_h{args.n_heads}_{timestamp}.png\")\n",
    "    plt.savefig(metrics_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Metrics plot saved as {metrics_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
