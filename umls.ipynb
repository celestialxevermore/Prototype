{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'administeredBy': ['https://data.bioontology.org/users/MFAmith', 'https://data.bioontology.org/users/taocui'], 'acronym': 'OCHV', 'name': 'Ontology of Consumer Health Vocabulary', 'summaryOnly': False, 'flat': None, 'ontologyType': 'https://data.bioontology.org/ontology_types/ONTOLOGY', '@id': 'https://data.bioontology.org/ontologies/OCHV', '@type': 'http://data.bioontology.org/metadata/Ontology', 'links': {'submissions': 'https://data.bioontology.org/ontologies/OCHV/submissions', 'properties': 'https://data.bioontology.org/ontologies/OCHV/properties', 'classes': 'https://data.bioontology.org/ontologies/OCHV/classes', 'single_class': 'https://data.bioontology.org/ontologies/OCHV/classes/{class_id}', 'roots': 'https://data.bioontology.org/ontologies/OCHV/classes/roots', 'instances': 'https://data.bioontology.org/ontologies/OCHV/instances', 'metrics': 'https://data.bioontology.org/ontologies/OCHV/metrics', 'reviews': 'https://data.bioontology.org/ontologies/OCHV/reviews', 'notes': 'https://data.bioontology.org/ontologies/OCHV/notes', 'groups': 'https://data.bioontology.org/ontologies/OCHV/groups', 'categories': 'https://data.bioontology.org/ontologies/OCHV/categories', 'latest_submission': 'https://data.bioontology.org/ontologies/OCHV/latest_submission', 'projects': 'https://data.bioontology.org/ontologies/OCHV/projects', 'download': 'https://data.bioontology.org/ontologies/OCHV/download', 'views': 'https://data.bioontology.org/ontologies/OCHV/views', 'analytics': 'https://data.bioontology.org/ontologies/OCHV/analytics', 'ui': 'http://bioportal.bioontology.org/ontologies/OCHV', '@context': {'submissions': 'http://data.bioontology.org/metadata/OntologySubmission', 'properties': 'http://data.bioontology.org/metadata/Property', 'classes': 'http://www.w3.org/2002/07/owl#Class', 'single_class': 'http://www.w3.org/2002/07/owl#Class', 'roots': 'http://www.w3.org/2002/07/owl#Class', 'instances': 'http://data.bioontology.org/metadata/Instance', 'metrics': 'http://data.bioontology.org/metadata/Metrics', 'reviews': 'http://data.bioontology.org/metadata/Review', 'notes': 'http://data.bioontology.org/metadata/Note', 'groups': 'http://data.bioontology.org/metadata/Group', 'categories': 'http://data.bioontology.org/metadata/Category', 'latest_submission': 'http://data.bioontology.org/metadata/OntologySubmission', 'projects': 'http://data.bioontology.org/metadata/Project', 'download': 'http://data.bioontology.org/metadata/Ontology', 'views': 'http://data.bioontology.org/metadata/Ontology', 'analytics': 'http://data.bioontology.org/metadata/Analytics', 'ui': 'http://data.bioontology.org/metadata/Ontology'}}, '@context': {'@vocab': 'http://data.bioontology.org/metadata/', 'acronym': 'http://omv.ontoware.org/2005/05/ontology#acronym', 'name': 'http://omv.ontoware.org/2005/05/ontology#name', 'administeredBy': {'@id': 'http://data.bioontology.org/metadata/User', '@type': '@id'}, 'flat': 'http://data.bioontology.org/metadata/flat', 'summaryOnly': 'http://data.bioontology.org/metadata/summaryOnly', 'ontologyType': {'@id': 'http://data.bioontology.org/metadata/OntologyType', '@type': '@id'}, '@language': 'en'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_KEY = \"53d9893d-8565-4cbe-967b-97161b79b4f4\"  # 여기에 실제 키 넣어줘\n",
    "BASE_URL = \"https://data.bioontology.org\"\n",
    "\n",
    "headers = {\"Authorization\": f\"apikey token={API_KEY}\"}\n",
    "\n",
    "r = requests.get(\"https://data.bioontology.org/ontologies/OCHV\", headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'administeredBy': ['https://data.bioontology.org/users/MFAmith',\n",
       "  'https://data.bioontology.org/users/taocui'],\n",
       " 'acronym': 'OCHV',\n",
       " 'name': 'Ontology of Consumer Health Vocabulary',\n",
       " 'summaryOnly': False,\n",
       " 'flat': None,\n",
       " 'ontologyType': 'https://data.bioontology.org/ontology_types/ONTOLOGY',\n",
       " '@id': 'https://data.bioontology.org/ontologies/OCHV',\n",
       " '@type': 'http://data.bioontology.org/metadata/Ontology',\n",
       " 'links': {'submissions': 'https://data.bioontology.org/ontologies/OCHV/submissions',\n",
       "  'properties': 'https://data.bioontology.org/ontologies/OCHV/properties',\n",
       "  'classes': 'https://data.bioontology.org/ontologies/OCHV/classes',\n",
       "  'single_class': 'https://data.bioontology.org/ontologies/OCHV/classes/{class_id}',\n",
       "  'roots': 'https://data.bioontology.org/ontologies/OCHV/classes/roots',\n",
       "  'instances': 'https://data.bioontology.org/ontologies/OCHV/instances',\n",
       "  'metrics': 'https://data.bioontology.org/ontologies/OCHV/metrics',\n",
       "  'reviews': 'https://data.bioontology.org/ontologies/OCHV/reviews',\n",
       "  'notes': 'https://data.bioontology.org/ontologies/OCHV/notes',\n",
       "  'groups': 'https://data.bioontology.org/ontologies/OCHV/groups',\n",
       "  'categories': 'https://data.bioontology.org/ontologies/OCHV/categories',\n",
       "  'latest_submission': 'https://data.bioontology.org/ontologies/OCHV/latest_submission',\n",
       "  'projects': 'https://data.bioontology.org/ontologies/OCHV/projects',\n",
       "  'download': 'https://data.bioontology.org/ontologies/OCHV/download',\n",
       "  'views': 'https://data.bioontology.org/ontologies/OCHV/views',\n",
       "  'analytics': 'https://data.bioontology.org/ontologies/OCHV/analytics',\n",
       "  'ui': 'http://bioportal.bioontology.org/ontologies/OCHV',\n",
       "  '@context': {'submissions': 'http://data.bioontology.org/metadata/OntologySubmission',\n",
       "   'properties': 'http://data.bioontology.org/metadata/Property',\n",
       "   'classes': 'http://www.w3.org/2002/07/owl#Class',\n",
       "   'single_class': 'http://www.w3.org/2002/07/owl#Class',\n",
       "   'roots': 'http://www.w3.org/2002/07/owl#Class',\n",
       "   'instances': 'http://data.bioontology.org/metadata/Instance',\n",
       "   'metrics': 'http://data.bioontology.org/metadata/Metrics',\n",
       "   'reviews': 'http://data.bioontology.org/metadata/Review',\n",
       "   'notes': 'http://data.bioontology.org/metadata/Note',\n",
       "   'groups': 'http://data.bioontology.org/metadata/Group',\n",
       "   'categories': 'http://data.bioontology.org/metadata/Category',\n",
       "   'latest_submission': 'http://data.bioontology.org/metadata/OntologySubmission',\n",
       "   'projects': 'http://data.bioontology.org/metadata/Project',\n",
       "   'download': 'http://data.bioontology.org/metadata/Ontology',\n",
       "   'views': 'http://data.bioontology.org/metadata/Ontology',\n",
       "   'analytics': 'http://data.bioontology.org/metadata/Analytics',\n",
       "   'ui': 'http://data.bioontology.org/metadata/Ontology'}},\n",
       " '@context': {'@vocab': 'http://data.bioontology.org/metadata/',\n",
       "  'acronym': 'http://omv.ontoware.org/2005/05/ontology#acronym',\n",
       "  'name': 'http://omv.ontoware.org/2005/05/ontology#name',\n",
       "  'administeredBy': {'@id': 'http://data.bioontology.org/metadata/User',\n",
       "   '@type': '@id'},\n",
       "  'flat': 'http://data.bioontology.org/metadata/flat',\n",
       "  'summaryOnly': 'http://data.bioontology.org/metadata/summaryOnly',\n",
       "  'ontologyType': {'@id': 'http://data.bioontology.org/metadata/OntologyType',\n",
       "   '@type': '@id'},\n",
       "  '@language': 'en'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pdb\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "logger = logging.getLogger(__name__)\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def visualize_cluster_centroids(clustering_info, clustering_dir, epoch, feature_names):\n",
    "    \"\"\"\n",
    "    각 클러스터의 centroid attention map을 히트맵으로 시각화\n",
    "    클러스터별 폴더 구조로 정리\n",
    "    \"\"\"\n",
    "    if clustering_info['cluster_centroids'] is None:\n",
    "        return\n",
    "    \n",
    "    # visualizations 폴더 생성\n",
    "    visualizations_dir = os.path.join(clustering_dir, 'visualizations')\n",
    "    os.makedirs(visualizations_dir, exist_ok=True)\n",
    "        \n",
    "    for cluster_id, centroid in enumerate(clustering_info['cluster_centroids']):\n",
    "        # 클러스터별 폴더 생성 (visualizations 하위에)\n",
    "        cluster_folder = os.path.join(visualizations_dir, f'cluster_{cluster_id}')\n",
    "        os.makedirs(cluster_folder, exist_ok=True)\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        \n",
    "        centroid_np = centroid.detach().cpu().numpy()\n",
    "        all_node_names = [\"CLS\"] + feature_names\n",
    "        \n",
    "        im = ax.imshow(centroid_np, cmap='viridis', interpolation='nearest')\n",
    "        ax.set_title(f'Cluster {cluster_id} Centroid - Epoch {epoch}', fontsize=14)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        \n",
    "        # 축 라벨 설정\n",
    "        ax.set_xticks(np.arange(len(all_node_names)))\n",
    "        ax.set_yticks(np.arange(len(all_node_names)))\n",
    "        ax.set_xticklabels(all_node_names, rotation=90, fontsize=8)\n",
    "        ax.set_yticklabels(all_node_names, fontsize=8)\n",
    "        \n",
    "        # 값 표시\n",
    "        for i in range(len(all_node_names)):\n",
    "            for j in range(len(all_node_names)):\n",
    "                ax.text(j, i, f\"{centroid_np[i,j]:.2f}\", \n",
    "                       ha=\"center\", va=\"center\", \n",
    "                       color=\"white\" if centroid_np[i,j] > 0.5 else \"black\", \n",
    "                       fontsize=6)\n",
    "        \n",
    "        # visualizations 폴더의 클러스터별 폴더에 저장\n",
    "        centroid_viz_path = os.path.join(cluster_folder, f'epoch_{epoch}.png')\n",
    "        fig.savefig(centroid_viz_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logger.info(f\"Saved centroid visualization for cluster {cluster_id}: {centroid_viz_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_model_structure(model, data_loader, device, args, mode, experiment_id, epoch, max_samples=10):\n",
    "    \"\"\"\n",
    "    모델의 내부 구조(어텐션, 그래프 구조 등)를 시각화하는 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 시각화할 모델\n",
    "        data_loader: 시각화에 사용할 데이터 로더\n",
    "        device: 계산에 사용할 디바이스\n",
    "        args: 실험 설정 인자 (args.viz_heatmap, args.viz_graph 플래그 사용)\n",
    "        mode: 'train' 또는 'val' 모드\n",
    "        experiment_id: 현재 실험 ID\n",
    "        epoch: 현재 에포크\n",
    "        max_samples: 시각화할 최대 샘플 수\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    base_viz_dir = os.path.join(f\"/storage/personal/eungyeop/experiments/visualization/{args.llm_model}/{args.source_dataset_name}/{mode}/{experiment_id}\")\n",
    "    os.makedirs(base_viz_dir, exist_ok=True)\n",
    "\n",
    "    # 샘플별 디렉토리 미리 생성\n",
    "    sample_dirs = []\n",
    "    for i in range(max_samples):\n",
    "        # 각 샘플 디렉토리\n",
    "        sample_dir = os.path.join(base_viz_dir, f'sample_{i}')\n",
    "        os.makedirs(sample_dir, exist_ok=True)\n",
    "        sample_dirs.append(sample_dir)\n",
    "        \n",
    "        # 각 샘플 내에 heatmap과 graph 폴더 생성\n",
    "        heatmap_dir = os.path.join(sample_dir, 'heatmap')\n",
    "        graph_dir = os.path.join(sample_dir, 'graph')\n",
    "        os.makedirs(heatmap_dir, exist_ok=True)\n",
    "        os.makedirs(graph_dir, exist_ok=True)\n",
    "        \n",
    "        # 각 폴더 내에 레이어별 서브폴더 생성\n",
    "        for layer_idx in range(len(model.layers)):\n",
    "            heatmap_layer_dir = os.path.join(heatmap_dir, f'layer_{layer_idx}')\n",
    "            graph_layer_dir = os.path.join(graph_dir, f'layer_{layer_idx}')\n",
    "            os.makedirs(heatmap_layer_dir, exist_ok=True)\n",
    "            os.makedirs(graph_layer_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        sample_count = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_on_device = {\n",
    "                k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in batch.items()\n",
    "            }\n",
    "            prediction = model.predict(batch_on_device)\n",
    "            \n",
    "            # 배치 크기 확인\n",
    "            batch_size = model.layers[0].attn_weights.shape[0]\n",
    "            \n",
    "            for sample_idx in range(batch_size):\n",
    "                # 특성 이름 정리 (모든 레이어에서 공통으로 사용)\n",
    "                feature_names = []\n",
    "                if 'cat_desc_texts' in batch_on_device:\n",
    "                    for feature in batch_on_device['cat_desc_texts']:\n",
    "                        if isinstance(feature, tuple):\n",
    "                            clean_name = str(feature[0])\n",
    "                        else:\n",
    "                            try:\n",
    "                                clean_name = feature.split(\"'\")[1] if \"'\" in feature else feature\n",
    "                                clean_name = clean_name.split(',')[0]\n",
    "                            except:\n",
    "                                clean_name = str(feature)\n",
    "                        feature_names.append(clean_name)\n",
    "\n",
    "                if 'num_desc_texts' in batch_on_device:\n",
    "                    for feature in batch_on_device['num_desc_texts']:\n",
    "                        if isinstance(feature, tuple):\n",
    "                            clean_name = str(feature[0])\n",
    "                        else:\n",
    "                            try:\n",
    "                                clean_name = feature.split(\"'\")[1] if \"'\" in feature else feature\n",
    "                                clean_name = clean_name.split(',')[0]\n",
    "                            except:\n",
    "                                clean_name = str(feature)\n",
    "                        feature_names.append(clean_name)\n",
    "                        \n",
    "                # 중복 제거 (순서 유지)\n",
    "                seen = set()\n",
    "                unique_features = []\n",
    "                for feat in feature_names:\n",
    "                    if feat not in seen:\n",
    "                        seen.add(feat)\n",
    "                        unique_features.append(feat)\n",
    "                feature_names = unique_features\n",
    "                \n",
    "                # 1. 히트맵 시각화\n",
    "                if args.viz_heatmap:\n",
    "                    # 시각화 시점에서만 클러스터링 리셋 (첫 번째 샘플에서만)\n",
    "                    if sample_count == 0:\n",
    "                        model.reset_epoch_clustering()\n",
    "                        \n",
    "                        # 현재 에포크의 데이터 수집을 위해 data_loader 순회\n",
    "                        model.train()  # attention 수집용\n",
    "                        with torch.no_grad():\n",
    "                            for batch in data_loader:\n",
    "                                batch_on_device = {\n",
    "                                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                                    for k, v in batch.items()\n",
    "                                }\n",
    "                                _ = model.predict(batch_on_device)  # attention maps 수집\n",
    "                        \n",
    "                        model.stop_attention_collection()\n",
    "                        model.eval()\n",
    "                    \n",
    "                    # 수집 완료 후 클러스터링 업데이트\n",
    "                    clustering_updated = model.update_attention_clustering()\n",
    "                    \n",
    "                    # 클러스터링 정보 가져오기\n",
    "                    clustering_info = model.get_clustering_info()\n",
    "                    \n",
    "                    # 1. 기존: 각 레이어별 시각화 (sample_*/heatmap/layer_*/에 저장)\n",
    "                    for layer_idx in range(len(model.layers)):\n",
    "                        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "                        \n",
    "                        # 1. Attention Map 히트맵 \n",
    "                        batch_size = model.layers[layer_idx].attn_weights.shape[0]\n",
    "                        actual_sample_idx = min(sample_idx, batch_size - 1)  # 배치 크기 초과 방지\n",
    "                        attn_weights = model.layers[layer_idx].attn_weights[actual_sample_idx]  # [n_heads, seq, seq]\n",
    "                        attn_weights_mean = attn_weights.mean(dim=0).cpu().numpy()  # 헤드별 평균\n",
    "                        \n",
    "                        # CLS 토큰 포함한 feature names\n",
    "                        all_node_names = [\"CLS\"] + feature_names \n",
    "                        \n",
    "                        im1 = axes[0].imshow(attn_weights_mean, cmap='viridis', interpolation='nearest')\n",
    "                        axes[0].set_title(f'Attention Map - Layer {layer_idx}', fontsize=14)\n",
    "                        fig.colorbar(im1, ax=axes[0])\n",
    "                        \n",
    "                        # 축 라벨 설정\n",
    "                        axes[0].set_xticks(np.arange(len(all_node_names)))\n",
    "                        axes[0].set_yticks(np.arange(len(all_node_names)))\n",
    "                        axes[0].set_xticklabels(all_node_names, rotation=90, fontsize=8)\n",
    "                        axes[0].set_yticklabels(all_node_names, fontsize=8)\n",
    "                        \n",
    "                        # 각 셀에 값 표시\n",
    "                        for i in range(len(all_node_names)):\n",
    "                            for j in range(len(all_node_names)):\n",
    "                                axes[0].text(j, i, f\"{attn_weights_mean[i,j]:.2f}\", \n",
    "                                        ha=\"center\", va=\"center\", \n",
    "                                        color=\"white\" if attn_weights_mean[i,j] > 0.5 else \"black\", \n",
    "                                        fontsize=7)\n",
    "                        \n",
    "                        # 2. 오른쪽: 해당하는 클러스터 centroid 표시\n",
    "                        if (layer_idx == len(model.layers) - 1 and  # 마지막 레이어(Layer 2)이고\n",
    "                            clustering_info['cluster_centroids'] is not None and \n",
    "                            len(clustering_info['cluster_assignments']) > 0):\n",
    "                            \n",
    "                            # 현재 샘플의 attention map이 어느 클러스터에 속하는지 찾기\n",
    "                            sample_attention = attn_weights_mean  # 현재 샘플의 attention map\n",
    "                            \n",
    "                            # 모든 centroid와의 거리 계산\n",
    "                            min_distance = float('inf')\n",
    "                            assigned_cluster = 0\n",
    "                            \n",
    "                            for cluster_id, centroid in enumerate(clustering_info['cluster_centroids']):\n",
    "                                centroid_np = centroid.detach().cpu().numpy()\n",
    "                                # Frobenius norm 거리 계산\n",
    "                                distance = np.linalg.norm(sample_attention - centroid_np, 'fro')\n",
    "                                if distance < min_distance:\n",
    "                                    min_distance = distance\n",
    "                                    assigned_cluster = cluster_id\n",
    "                            \n",
    "                            # 해당 클러스터의 centroid 표시\n",
    "                            assigned_centroid = clustering_info['cluster_centroids'][assigned_cluster].detach().cpu().numpy()\n",
    "                            \n",
    "                            im2 = axes[1].imshow(assigned_centroid, cmap='viridis', interpolation='nearest')\n",
    "                            axes[1].set_title(f'Closest Cluster Centroid - Cluster {assigned_cluster}\\n(Distance: {min_distance:.3f})', fontsize=14)\n",
    "                            fig.colorbar(im2, ax=axes[1])\n",
    "                            \n",
    "                            # 축 라벨 설정\n",
    "                            axes[1].set_xticks(np.arange(len(all_node_names)))\n",
    "                            axes[1].set_yticks(np.arange(len(all_node_names)))\n",
    "                            axes[1].set_xticklabels(all_node_names, rotation=90, fontsize=8)\n",
    "                            axes[1].set_yticklabels(all_node_names, fontsize=8)\n",
    "                            \n",
    "                            # 각 셀에 값 표시\n",
    "                            for i in range(len(all_node_names)):\n",
    "                                for j in range(len(all_node_names)):\n",
    "                                    axes[1].text(j, i, f\"{assigned_centroid[i,j]:.2f}\", \n",
    "                                            ha=\"center\", va=\"center\", \n",
    "                                            color=\"white\" if assigned_centroid[i,j] > 0.5 else \"black\", \n",
    "                                            fontsize=7)\n",
    "                        else:\n",
    "                            # 마지막 레이어가 아니거나 클러스터링 데이터가 없는 경우 기존 방식\n",
    "                            axes[1].text(0.5, 0.5, f'Layer {layer_idx} Attention Pattern\\n\\nFull clustering results\\navailable in clustering/ folder\\n\\nLayer 2 = Final clustering layer', \n",
    "                                    ha='center', va='center', transform=axes[1].transAxes, fontsize=14,\n",
    "                                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcyan\", alpha=0.8))\n",
    "                            axes[1].set_title(f'Layer {layer_idx} - See clustering/ for full results', fontsize=14)\n",
    "                            axes[1].axis('off')\n",
    "                        \n",
    "                        # 전체 타이틀\n",
    "                        fig.suptitle(f'Layer {layer_idx} Attention Analysis - Epoch {epoch} - Sample {sample_count}', fontsize=16)\n",
    "                        plt.tight_layout()\n",
    "                        \n",
    "                        # 기존 경로에 저장\n",
    "                        heatmap_path = os.path.join(sample_dirs[sample_count], 'heatmap', f'layer_{layer_idx}', f'epoch_{epoch}.png')\n",
    "                        fig.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "                        logger.info(f\"Epoch {epoch} - 샘플 {sample_count} 레이어 {layer_idx} 히트맵 저장: {heatmap_path}\")\n",
    "\n",
    "                    # 2. 전체 클러스터링 결과 (clustering/ 폴더에 저장) - 첫 번째 샘플에서만 생성\n",
    "                    if sample_count == 0:  # 중복 방지: 첫 번째 샘플에서만 클러스터링 시각화 생성\n",
    "                        # clustering 폴더 생성\n",
    "                        clustering_dir = os.path.join(base_viz_dir, 'clustering')\n",
    "                        os.makedirs(clustering_dir, exist_ok=True)\n",
    "                        model.save_cluster_centroids(clustering_dir, epoch)\n",
    "                        if clustering_info['cluster_centroids'] is not None:\n",
    "                            visualize_cluster_centroids(clustering_info, clustering_dir, epoch, feature_names)\n",
    "    \n",
    "                        # 🆕 1x3 클러스터링 결과 시각화\n",
    "                        fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "                        \n",
    "                        # 플롯 1: 전체 데이터 클러스터링 (기존)\n",
    "                        cluster_assignments = clustering_info['cluster_assignments']\n",
    "                        attention_maps = clustering_info['attention_maps']\n",
    "                        attention_labels = clustering_info['attention_labels']\n",
    "                        \n",
    "                        # 전체 데이터 플롯\n",
    "                        if (clustering_info['cluster_centroids'] is not None and \n",
    "                            len(cluster_assignments) > 0 and len(attention_maps) > 0):\n",
    "                            \n",
    "                            try:\n",
    "                                from sklearn.manifold import TSNE\n",
    "                                \n",
    "                                cluster_assignments = np.array(cluster_assignments)\n",
    "                                attention_labels = np.array(attention_labels)\n",
    "                                \n",
    "                                # attention maps를 numpy로 변환\n",
    "                                attention_np = torch.stack(attention_maps).detach().cpu().numpy()\n",
    "                                n_maps, seq_len, seq_len2 = attention_np.shape\n",
    "                                \n",
    "                                # 평탄화해서 t-SNE 적용\n",
    "                                flattened_maps = attention_np.reshape(n_maps, -1)\n",
    "                                \n",
    "                                if n_maps >= 2:\n",
    "                                    perplexity = min(30, n_maps-1, max(1, n_maps//3))\n",
    "                                    \n",
    "                                    # Centroid 처리\n",
    "                                    if clustering_info['cluster_centroids'] is not None:\n",
    "                                        cluster_centroids = clustering_info['cluster_centroids']\n",
    "                                        \n",
    "                                        if isinstance(cluster_centroids, torch.Tensor):\n",
    "                                            centroids_np = cluster_centroids.detach().cpu().numpy()\n",
    "                                        else:\n",
    "                                            centroids_np = torch.stack(cluster_centroids).detach().cpu().numpy()\n",
    "                                        \n",
    "                                        centroids_flat = centroids_np.reshape(len(centroids_np), -1)\n",
    "                                        \n",
    "                                        # 전체 데이터(attention maps + centroids)를 함께 t-SNE 변환\n",
    "                                        all_data = np.vstack([flattened_maps, centroids_flat])\n",
    "                                        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "                                        tsne_all_embeddings = tsne.fit_transform(all_data)\n",
    "                                        \n",
    "                                        # 원본 데이터와 centroid 분리\n",
    "                                        tsne_embeddings = tsne_all_embeddings[:n_maps]\n",
    "                                        centroid_embeddings = tsne_all_embeddings[n_maps:]\n",
    "                                    else:\n",
    "                                        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "                                        tsne_embeddings = tsne.fit_transform(flattened_maps)\n",
    "                                        centroid_embeddings = None\n",
    "                                    \n",
    "                                    # 시각화\n",
    "                                    unique_clusters = np.unique(cluster_assignments)\n",
    "                                    unique_labels = np.unique(attention_labels)\n",
    "                                    base_colors = plt.cm.Set3(np.linspace(0, 1, max(len(unique_clusters), 1)))\n",
    "                                    \n",
    "                                    for i, cluster_id in enumerate(unique_clusters):\n",
    "                                        cluster_mask = cluster_assignments == cluster_id\n",
    "                                        cluster_points = tsne_embeddings[cluster_mask]\n",
    "                                        cluster_labels = attention_labels[cluster_mask]\n",
    "                                        \n",
    "                                        if len(cluster_points) > 0:\n",
    "                                            for label in unique_labels:\n",
    "                                                label_mask = cluster_labels == label\n",
    "                                                if np.any(label_mask):\n",
    "                                                    label_points = cluster_points[label_mask]\n",
    "                                                    \n",
    "                                                    marker = 'o' if label == 0 else 's'\n",
    "                                                    marker_name = 'Label 0' if label == 0 else 'Label 1'\n",
    "                                                    \n",
    "                                                    axes[0].scatter(label_points[:, 0], label_points[:, 1], \n",
    "                                                            color=base_colors[i], \n",
    "                                                            label=f'Cluster {cluster_id} ({marker_name})', \n",
    "                                                            alpha=0.7, s=50, marker=marker)\n",
    "                                    \n",
    "                                    # Centroid 표시\n",
    "                                    if centroid_embeddings is not None:\n",
    "                                        for i, cluster_id in enumerate(unique_clusters):\n",
    "                                            if i < len(centroid_embeddings):\n",
    "                                                axes[0].scatter(centroid_embeddings[i, 0], centroid_embeddings[i, 1], \n",
    "                                                        marker='*', s=300, c='black', \n",
    "                                                        edgecolors=base_colors[i], linewidth=3,\n",
    "                                                        label='Centroids' if i == 0 else \"\", zorder=5)\n",
    "                                    \n",
    "                                    axes[0].set_title(f'All Data (Epoch {epoch})', fontsize=14)\n",
    "                                    axes[0].set_xlabel('t-SNE Dimension 1', fontsize=10)\n",
    "                                    axes[0].set_ylabel('t-SNE Dimension 2', fontsize=10)\n",
    "                                    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "                                    axes[0].grid(True, alpha=0.3)\n",
    "                                else:\n",
    "                                    axes[0].text(0.5, 0.5, f'All Data\\nNeed more data\\nCurrent: {n_maps}', \n",
    "                                        ha='center', va='center', transform=axes[0].transAxes, fontsize=12,\n",
    "                                        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "                                    axes[0].set_title('All Data', fontsize=14)\n",
    "                                    axes[0].axis('off')\n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"All data clustering visualization error: {e}\")\n",
    "                                axes[0].text(0.5, 0.5, f'All Data\\nError: {str(e)}', \n",
    "                                    ha='center', va='center', transform=axes[0].transAxes, fontsize=12,\n",
    "                                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                                axes[0].set_title('All Data', fontsize=14)\n",
    "                                axes[0].axis('off')\n",
    "                        else:\n",
    "                            axes[0].text(0.5, 0.5, 'All Data\\nNo clustering data', \n",
    "                                ha='center', va='center', transform=axes[0].transAxes, fontsize=12,\n",
    "                                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                            axes[0].set_title('All Data', fontsize=14)\n",
    "                            axes[0].axis('off')\n",
    "                        \n",
    "                        # 플롯 2: Label 0만 클러스터링\n",
    "                        attention_maps_0 = clustering_info['attention_maps_label_0']\n",
    "                        cluster_assignments_0 = clustering_info['cluster_assignments_label_0']\n",
    "                        cluster_centroids_0 = clustering_info['cluster_centroids_label_0']\n",
    "                        \n",
    "                        # if (cluster_centroids_0 is not None and \n",
    "                        #     len(cluster_assignments_0) > 0 and len(attention_maps_0) > 0):\n",
    "                            \n",
    "                        #     try:\n",
    "                        #         cluster_assignments_0 = np.array(cluster_assignments_0)\n",
    "                                \n",
    "                        #         # attention maps를 numpy로 변환\n",
    "                        #         attention_np_0 = torch.stack(attention_maps_0).detach().cpu().numpy()\n",
    "                        #         n_maps_0, seq_len, seq_len2 = attention_np_0.shape\n",
    "                                \n",
    "                        #         flattened_maps_0 = attention_np_0.reshape(n_maps_0, -1)\n",
    "                                \n",
    "                        #         if n_maps_0 >= 2:\n",
    "                        #             perplexity_0 = min(30, n_maps_0-1, max(1, n_maps_0//3))\n",
    "                                    \n",
    "                        #             if isinstance(cluster_centroids_0, torch.Tensor):\n",
    "                        #                 centroids_np_0 = cluster_centroids_0.detach().cpu().numpy()\n",
    "                        #             else:\n",
    "                        #                 centroids_np_0 = torch.stack(cluster_centroids_0).detach().cpu().numpy()\n",
    "                                    \n",
    "                        #             centroids_flat_0 = centroids_np_0.reshape(len(centroids_np_0), -1)\n",
    "                                    \n",
    "                        #             all_data_0 = np.vstack([flattened_maps_0, centroids_flat_0])\n",
    "                        #             tsne_0 = TSNE(n_components=2, random_state=42, perplexity=perplexity_0)\n",
    "                        #             tsne_all_embeddings_0 = tsne_0.fit_transform(all_data_0)\n",
    "                                    \n",
    "                        #             tsne_embeddings_0 = tsne_all_embeddings_0[:n_maps_0]\n",
    "                        #             centroid_embeddings_0 = tsne_all_embeddings_0[n_maps_0:]\n",
    "                                    \n",
    "                        #             unique_clusters_0 = np.unique(cluster_assignments_0)\n",
    "                        #             base_colors_0 = plt.cm.Set3(np.linspace(0, 1, max(len(unique_clusters_0), 1)))\n",
    "                                    \n",
    "                        #             for i, cluster_id in enumerate(unique_clusters_0):\n",
    "                        #                 cluster_mask_0 = cluster_assignments_0 == cluster_id\n",
    "                        #                 cluster_points_0 = tsne_embeddings_0[cluster_mask_0]\n",
    "                                        \n",
    "                        #                 if len(cluster_points_0) > 0:\n",
    "                        #                     axes[1].scatter(cluster_points_0[:, 0], cluster_points_0[:, 1], \n",
    "                        #                             color=base_colors_0[i], \n",
    "                        #                             label=f'Cluster {cluster_id}', \n",
    "                        #                             alpha=0.7, s=50, marker='o')\n",
    "                                    \n",
    "                        #             # Centroid 표시\n",
    "                        #             for i, cluster_id in enumerate(unique_clusters_0):\n",
    "                        #                 if i < len(centroid_embeddings_0):\n",
    "                        #                     axes[1].scatter(centroid_embeddings_0[i, 0], centroid_embeddings_0[i, 1], \n",
    "                        #                             marker='*', s=300, c='black', \n",
    "                        #                             edgecolors=base_colors_0[i], linewidth=3,\n",
    "                        #                             label='Centroids' if i == 0 else \"\", zorder=5)\n",
    "                                    \n",
    "                        #             axes[1].set_title(f'Label 0 Only (Epoch {epoch})', fontsize=14)\n",
    "                        #             axes[1].set_xlabel('t-SNE Dimension 1', fontsize=10)\n",
    "                        #             axes[1].set_ylabel('t-SNE Dimension 2', fontsize=10)\n",
    "                        #             axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "                        #             axes[1].grid(True, alpha=0.3)\n",
    "                        #         else:\n",
    "                        #             axes[1].text(0.5, 0.5, f'Label 0 Only\\nNeed more data\\nCurrent: {n_maps_0}', \n",
    "                        #                 ha='center', va='center', transform=axes[1].transAxes, fontsize=12,\n",
    "                        #                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "                        #             axes[1].set_title('Label 0 Only', fontsize=14)\n",
    "                        #             axes[1].axis('off')\n",
    "                        #     except Exception as e:\n",
    "                        #         logger.error(f\"Label 0 clustering visualization error: {e}\")\n",
    "                        #         axes[1].text(0.5, 0.5, f'Label 0 Only\\nError: {str(e)}', \n",
    "                        #             ha='center', va='center', transform=axes[1].transAxes, fontsize=12,\n",
    "                        #             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                        #         axes[1].set_title('Label 0 Only', fontsize=14)\n",
    "                        #         axes[1].axis('off')\n",
    "                        # else:\n",
    "                        #     axes[1].text(0.5, 0.5, 'Label 0 Only\\nNo clustering data', \n",
    "                        #         ha='center', va='center', transform=axes[1].transAxes, fontsize=12,\n",
    "                        #         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                        #     axes[1].set_title('Label 0 Only', fontsize=14)\n",
    "                        #     axes[1].axis('off')\n",
    "                        \n",
    "                        # # 플롯 3: Label 1만 클러스터링\n",
    "                        # attention_maps_1 = clustering_info['attention_maps_label_1']\n",
    "                        # cluster_assignments_1 = clustering_info['cluster_assignments_label_1']\n",
    "                        # cluster_centroids_1 = clustering_info['cluster_centroids_label_1']\n",
    "                        \n",
    "                        # if (cluster_centroids_1 is not None and \n",
    "                        #     len(cluster_assignments_1) > 0 and len(attention_maps_1) > 0):\n",
    "                            \n",
    "                        #     try:\n",
    "                        #         cluster_assignments_1 = np.array(cluster_assignments_1)\n",
    "                                \n",
    "                        #         # attention maps를 numpy로 변환\n",
    "                        #         attention_np_1 = torch.stack(attention_maps_1).detach().cpu().numpy()\n",
    "                        #         n_maps_1, seq_len, seq_len2 = attention_np_1.shape\n",
    "                                \n",
    "                        #         flattened_maps_1 = attention_np_1.reshape(n_maps_1, -1)\n",
    "                                \n",
    "                        #         if n_maps_1 >= 2:\n",
    "                        #             perplexity_1 = min(30, n_maps_1-1, max(1, n_maps_1//3))\n",
    "                                    \n",
    "                        #             if isinstance(cluster_centroids_1, torch.Tensor):\n",
    "                        #                 centroids_np_1 = cluster_centroids_1.detach().cpu().numpy()\n",
    "                        #             else:\n",
    "                        #                 centroids_np_1 = torch.stack(cluster_centroids_1).detach().cpu().numpy()\n",
    "                                    \n",
    "                        #             centroids_flat_1 = centroids_np_1.reshape(len(centroids_np_1), -1)\n",
    "                                    \n",
    "                        #             all_data_1 = np.vstack([flattened_maps_1, centroids_flat_1])\n",
    "                        #             tsne_1 = TSNE(n_components=2, random_state=42, perplexity=perplexity_1)\n",
    "                        #             tsne_all_embeddings_1 = tsne_1.fit_transform(all_data_1)\n",
    "                                    \n",
    "                        #             tsne_embeddings_1 = tsne_all_embeddings_1[:n_maps_1]\n",
    "                        #             centroid_embeddings_1 = tsne_all_embeddings_1[n_maps_1:]\n",
    "                                    \n",
    "                        #             unique_clusters_1 = np.unique(cluster_assignments_1)\n",
    "                        #             base_colors_1 = plt.cm.Set3(np.linspace(0, 1, max(len(unique_clusters_1), 1)))\n",
    "                                    \n",
    "                        #             for i, cluster_id in enumerate(unique_clusters_1):\n",
    "                        #                 cluster_mask_1 = cluster_assignments_1 == cluster_id\n",
    "                        #                 cluster_points_1 = tsne_embeddings_1[cluster_mask_1]\n",
    "                                        \n",
    "                        #                 if len(cluster_points_1) > 0:\n",
    "                        #                     axes[2].scatter(cluster_points_1[:, 0], cluster_points_1[:, 1], \n",
    "                        #                             color=base_colors_1[i], \n",
    "                        #                             label=f'Cluster {cluster_id}', \n",
    "                        #                             alpha=0.7, s=50, marker='s')\n",
    "                                    \n",
    "                        #             # Centroid 표시\n",
    "                        #             for i, cluster_id in enumerate(unique_clusters_1):\n",
    "                        #                 if i < len(centroid_embeddings_1):\n",
    "                        #                     axes[2].scatter(centroid_embeddings_1[i, 0], centroid_embeddings_1[i, 1], \n",
    "                        #                             marker='*', s=300, c='black', \n",
    "                        #                             edgecolors=base_colors_1[i], linewidth=3,\n",
    "                        #                             label='Centroids' if i == 0 else \"\", zorder=5)\n",
    "                                    \n",
    "                        #             axes[2].set_title(f'Label 1 Only (Epoch {epoch})', fontsize=14)\n",
    "                        #             axes[2].set_xlabel('t-SNE Dimension 1', fontsize=10)\n",
    "                        #             axes[2].set_ylabel('t-SNE Dimension 2', fontsize=10)\n",
    "                        #             axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "                        #             axes[2].grid(True, alpha=0.3)\n",
    "                        #         else:\n",
    "                        #             axes[2].text(0.5, 0.5, f'Label 1 Only\\nNeed more data\\nCurrent: {n_maps_1}', \n",
    "                        #                 ha='center', va='center', transform=axes[2].transAxes, fontsize=12,\n",
    "                        #                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "                        #             axes[2].set_title('Label 1 Only', fontsize=14)\n",
    "                        #             axes[2].axis('off')\n",
    "                        #     except Exception as e:\n",
    "                        #         logger.error(f\"Label 1 clustering visualization error: {e}\")\n",
    "                        #         axes[2].text(0.5, 0.5, f'Label 1 Only\\nError: {str(e)}', \n",
    "                        #             ha='center', va='center', transform=axes[2].transAxes, fontsize=12,\n",
    "                        #             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                        #         axes[2].set_title('Label 1 Only', fontsize=14)\n",
    "                        #         axes[2].axis('off')\n",
    "                        # else:\n",
    "                        #     axes[2].text(0.5, 0.5, 'Label 1 Only\\nNo clustering data', \n",
    "                        #         ha='center', va='center', transform=axes[2].transAxes, fontsize=12,\n",
    "                        #         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "                        #     axes[2].set_title('Label 1 Only', fontsize=14)\n",
    "                        #     axes[2].axis('off')\n",
    "                        \n",
    "                        # plt.tight_layout()\n",
    "                        \n",
    "                        # clustering 폴더에 저장\n",
    "                        clustering_path = os.path.join(clustering_dir, f'epoch_{epoch}.png')\n",
    "                        fig.savefig(clustering_path, dpi=300, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "                        logger.info(f\"Epoch {epoch} - 1x3 클러스터링 저장: {clustering_path}\")\n",
    "\n",
    "                sample_count += 1\n",
    "                if sample_count >= max_samples:\n",
    "                    break\n",
    "            \n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "                # 2. 그래프 구조 시각화\n",
    "                if args.viz_graph:\n",
    "                    # 각 레이어별로 시각화 수행\n",
    "                    for layer_idx in range(len(model.layers)):\n",
    "                        # 1) Attention 가중치(헤드 평균)\n",
    "                        attn_weights = model.layers[layer_idx].attn_weights[sample_idx]  # [n_heads, seq, seq]\n",
    "                        attn_weights_mean = attn_weights.mean(dim=0).cpu()\n",
    "\n",
    "                        # 원본 adjacency 사용 (히트맵과 일치하는 값)\n",
    "                        adjacency = model.layers[layer_idx].adjacency[sample_idx].cpu()\n",
    "                        # adj_row_sums = adjacency.sum(axis=1, keepdims=True) + 1e-9\n",
    "                        # adjacency = adjacency / adj_row_sums\n",
    "                        new_seq = attn_weights_mean.shape[0]\n",
    "                        graph_matrix = torch.zeros((new_seq, new_seq), device=attn_weights_mean.device, dtype = torch.float)\n",
    "\n",
    "                        graph_matrix[1:, 1:] = adjacency  # 변수 간 연결은 원본 adjacency 사용\n",
    "                        graph_matrix[0, 1:] = 1.0  # CLS->변수 연결\n",
    "                        graph_matrix[1:, 0] = 0.0  # 변수->CLS 연결\n",
    "                        \n",
    "                        mask = (graph_matrix == 0)\n",
    "                        final_graph_matrix = (attn_weights_mean * graph_matrix).numpy()\n",
    "                        final_graph_matrix[mask.numpy()] = 0.0\n",
    "                        # row_sums = final_graph_matrix.sum(axis=1, keepdims=True)\n",
    "                        # final_graph_matrix = final_graph_matrix / (row_sums + 1e-9)  # stability 위해 1e-9 더함 \n",
    "                        n_nodes = final_graph_matrix.shape[0]\n",
    "                        \n",
    "                        # 2) Edge 리스트(모든 i->j) 수집 (Barplot용)\n",
    "                        cls_edges_info = []  # CLS에서 나가는 엣지\n",
    "                        var_edges_info = []  # 나머지 엣지\n",
    "                        \n",
    "                        for i in range(n_nodes):\n",
    "                            for j in range(n_nodes):\n",
    "                                if i != j:\n",
    "                                    w = final_graph_matrix[i, j]\n",
    "                                    if i == 0:\n",
    "                                        cls_edges_info.append((f\"{i}->{j}\", w))\n",
    "                                    else:\n",
    "                                        var_edges_info.append((f\"{i}->{j}\", w))\n",
    "                        \n",
    "                        # topK 적용\n",
    "                        top_k = min(10, len(var_edges_info))\n",
    "                        var_edges_info.sort(key=lambda x: x[1], reverse=True)\n",
    "                        var_edges_info = var_edges_info[:top_k]\n",
    "                        \n",
    "                        # 전체 합치기\n",
    "                        edges_info = cls_edges_info + var_edges_info\n",
    "                        edges_info.sort(key=lambda x: x[1], reverse=True)\n",
    "                        edge_labels = [x[0] for x in edges_info]\n",
    "                        edge_weights = [x[1] for x in edges_info]\n",
    "                        \n",
    "                        # CLS 엣지와 일반 엣지 구분을 위한 색상 리스트\n",
    "                        bar_colors = []\n",
    "                        for label in edge_labels:\n",
    "                            if label.startswith(\"0->\"):\n",
    "                                bar_colors.append(\"crimson\")  # CLS 엣지는 빨간색\n",
    "                            else:\n",
    "                                bar_colors.append(\"cornflowerblue\")  # 일반 엣지는 파란색\n",
    "                        \n",
    "                        # 노드 이름 매핑\n",
    "                        node_name_map = {0: \"CLS\"}\n",
    "                        for i in range(1, n_nodes):\n",
    "                            idx_feat = i - 1\n",
    "                            if idx_feat < len(feature_names):\n",
    "                                node_name_map[i] = feature_names[idx_feat]\n",
    "                            else:\n",
    "                                node_name_map[i] = f\"feature_{i}\"\n",
    "                                \n",
    "                        # x축 라벨에 사용할 이름 변환\n",
    "                        display_edge_labels = []\n",
    "                        for label in edge_labels:\n",
    "                            i, j = map(int, label.split('->'))\n",
    "                            display_edge_labels.append(f\"{node_name_map[i]}->{node_name_map[j]}\")\n",
    "                        \n",
    "                        # Figure & 2 Subplots 생성\n",
    "                        fig, axes = plt.subplots(2, 2, figsize=(24, 20))\n",
    "                        ax_bar = axes[0, 0]\n",
    "                        \n",
    "                        # -----(A) Left Subplot: Barplot)-----\n",
    "                        bars = ax_bar.bar(range(len(edge_weights)), edge_weights, color=bar_colors)\n",
    "                        \n",
    "                        # 각 바 위에 attention score 값 표시\n",
    "                        for i, (weight, label) in enumerate(zip(edge_weights, edge_labels)):\n",
    "                            ax_bar.text(i, weight + 0.01, f\"{weight:.3f}\", \n",
    "                                      ha='center', va='bottom', rotation=45, \n",
    "                                      fontsize=7, color='black')\n",
    "                        \n",
    "                        ax_bar.set_title(f'Top Edge Weights - Layer {layer_idx}', fontsize=12)\n",
    "                        ax_bar.set_xlabel('Edge (i->j)')\n",
    "                        ax_bar.set_ylabel('Attention Weight')\n",
    "                        # x축 라벨 (너무 많으면 회전)\n",
    "                        ax_bar.set_xticks(range(len(edge_labels)))\n",
    "                        ax_bar.set_xticklabels(display_edge_labels, rotation=90, fontsize=8)\n",
    "                        \n",
    "                        # -----(B) Right Subplot: Network Graph)-----\n",
    "                        ax_graph = axes[0, 1]\n",
    "                        G = nx.DiGraph()\n",
    "                        node_labels = {}\n",
    "\n",
    "                        for i in range(n_nodes):\n",
    "                            if i == 0:\n",
    "                                node_name = \"CLS\"\n",
    "                                node_color = \"red\"\n",
    "                            else:\n",
    "                                idx_feat = i - 1\n",
    "                                if idx_feat < len(feature_names):\n",
    "                                    node_name = feature_names[idx_feat]\n",
    "                                    node_color = \"blue\"\n",
    "                                else:\n",
    "                                    node_name = f\"feature_{i}\"\n",
    "                                    node_color = \"blue\"\n",
    "\n",
    "                            G.add_node(i, name=node_name, color=node_color)\n",
    "                            node_labels[i] = node_name\n",
    "\n",
    "                        # CLS->Var / Var->Var 구분해서 그리기\n",
    "                        cls_min_edge_weight = 0.001\n",
    "                        min_edge_weight = 0.001\n",
    "                        for i in range(n_nodes):\n",
    "                            for j in range(n_nodes):\n",
    "                                if i == j:\n",
    "                                    continue\n",
    "\n",
    "                                w = final_graph_matrix[i, j]\n",
    "                                if i == 0 and j != 0:\n",
    "                                    # CLS->Var\n",
    "                                    if w > cls_min_edge_weight:\n",
    "                                        G.add_edge(i, j, weight=w, cls_to_var=True)\n",
    "                                elif j == 0:\n",
    "                                    # Var->CLS는 표시 안 함\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    if w > min_edge_weight:\n",
    "                                        # Var->Var\n",
    "                                        G.add_edge(i, j, weight=w, cls_to_var=False)\n",
    "\n",
    "                        pos = {}\n",
    "                        pos[0] = np.array([0, 0])\n",
    "                        non_center_nodes = n_nodes - 1\n",
    "                        radius = 1.0\n",
    "                        for i_ in range(1, n_nodes):\n",
    "                            angle_ = 2 * np.pi * (i_ - 1) / non_center_nodes\n",
    "                            pos[i_] = np.array([radius * np.cos(angle_), radius * np.sin(angle_)])\n",
    "\n",
    "                        # 배경 그리드\n",
    "                        for r_ in [0.25, 0.5, 0.75, 1.0]:\n",
    "                            circle = plt.Circle((0, 0), r_, fill=False, color='lightgray', linestyle='--', alpha=0.5)\n",
    "                            ax_graph.add_patch(circle)\n",
    "                        for i_ in range(1, n_nodes):\n",
    "                            angle__ = 2 * np.pi * (i_ - 1) / non_center_nodes\n",
    "                            x_ = 1.1 * np.cos(angle__)\n",
    "                            y_ = 1.1 * np.sin(angle__)\n",
    "                            ax_graph.plot([0, x_], [0, y_], color='lightgray', linestyle='--', alpha=0.5)\n",
    "\n",
    "                        node_colors = [d[\"color\"] for _, d in G.nodes(data=True)]\n",
    "                        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=800, ax=ax_graph, edgecolors='gray')\n",
    "\n",
    "                        cls_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('cls_to_var')]\n",
    "                        var_edges = [(u, v) for u, v, d in G.edges(data=True) if not d.get('cls_to_var')]\n",
    "\n",
    "                        cls_weights = [G[u][v]['weight'] for (u, v) in cls_edges]\n",
    "                        var_weights = [G[u][v]['weight'] for (u, v) in var_edges]\n",
    "\n",
    "                        # CLS->Var: 빨강 굵은선\n",
    "                        if cls_edges:\n",
    "                            nx.draw_networkx_edges(\n",
    "                                G, pos,\n",
    "                                edgelist=cls_edges,\n",
    "                                width=[2 + w * 5 for w in cls_weights],\n",
    "                                alpha=0.7,\n",
    "                                edge_color='crimson',\n",
    "                                connectionstyle='arc3,rad=0.1',  \n",
    "                                arrowstyle='-|>',  # 화살표 스타일 변경\n",
    "                                arrowsize=15,      # 화살표 크기 키우기 (기본값보다 크게)\n",
    "                                node_size=800,\n",
    "                                ax=ax_graph\n",
    "                            )\n",
    "\n",
    "                        # Var->Var: 파랑 점선\n",
    "                        if var_edges:\n",
    "                            nx.draw_networkx_edges(\n",
    "                                G, pos,\n",
    "                                edgelist=var_edges,\n",
    "                                width=[1 + w * 2 for w in var_weights],\n",
    "                                edge_color='blue',\n",
    "                                style='dashed',\n",
    "                                arrowstyle='-|>',\n",
    "                                arrowsize=30,\n",
    "                                alpha=0.5,\n",
    "                                ax=ax_graph,\n",
    "                                arrows=True\n",
    "                            )\n",
    "\n",
    "                        label_options = {\n",
    "                            \"font_size\": 9,\n",
    "                            \"font_color\": \"black\",\n",
    "                            \"bbox\": dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n",
    "                        }\n",
    "                        nx.draw_networkx_labels(G, pos, labels=node_labels, ax=ax_graph, **label_options)\n",
    "\n",
    "                        ax_graph.set_title(f'Graph Structure - Layer {layer_idx} - Epoch {epoch} - Sample {sample_count}', fontsize=12)\n",
    "                        ax_graph.axis('off')\n",
    "                        ax_graph.set_aspect('equal')\n",
    "                        ax_graph.set_xlim([-1.2, 1.2])\n",
    "                        ax_graph.set_ylim([-1.2, 1.2])\n",
    "\n",
    "                        # 3. 확장된 graph matrix heatmap\n",
    "                        ax_graph_matrix = axes[1, 0]\n",
    "                        graph_matrix_np = graph_matrix.cpu().numpy() \n",
    "                        im_graph = ax_graph_matrix.imshow(graph_matrix_np, cmap=\"Blues\", interpolation='nearest')\n",
    "                        ax_graph_matrix.set_title(\"Graph Matrix (with CLS)\", fontsize=14)\n",
    "                        fig.colorbar(im_graph, ax=ax_graph_matrix)\n",
    "\n",
    "                        all_node_names = [\"CLS\"] + feature_names \n",
    "                        ax_graph_matrix.set_xticks(np.arange(len(all_node_names)))\n",
    "                        ax_graph_matrix.set_yticks(np.arange(len(all_node_names)))\n",
    "                        ax_graph_matrix.set_xticklabels(all_node_names, rotation=90, fontsize=8)\n",
    "                        ax_graph_matrix.set_yticklabels(all_node_names, fontsize=8)\n",
    "\n",
    "                        for i in range(len(all_node_names)):\n",
    "                            for j in range(len(all_node_names)):\n",
    "                                ax_graph_matrix.text(j, i, f\"{graph_matrix_np[i,j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\" if graph_matrix_np[i,j] < 0.5 else \"white\", fontsize=8)\n",
    "\n",
    "                        ax_final = axes[1, 1]\n",
    "                        vmax = final_graph_matrix.max()\n",
    "                        vmin = 0.0  # 0부터 시작하도록 설정\n",
    "\n",
    "                        # 다른 컬러맵 사용 및 범위 조정\n",
    "                        im_final = ax_final.imshow(final_graph_matrix, \n",
    "                                                cmap='YlOrRd',  # 'YlOrRd', 'hot', 'OrRd' 등 시도해볼 수 있음\n",
    "                                                interpolation='nearest',\n",
    "                                                vmin=vmin, \n",
    "                                                vmax=vmax)\n",
    "                        ax_final.set_title(\"Final Graph Matrix (Attention * Graph_matrix)\", fontsize=14)\n",
    "                        fig.colorbar(im_final, ax=ax_final)\n",
    "                        \n",
    "                        ax_final.set_xticks(np.arange(len(all_node_names)))\n",
    "                        ax_final.set_yticks(np.arange(len(all_node_names)))\n",
    "                        ax_final.set_xticklabels(all_node_names, rotation=90, fontsize=8)\n",
    "                        ax_final.set_yticklabels(all_node_names, fontsize=8)\n",
    "                        \n",
    "                        # 각 셀에 값 표시\n",
    "                        for i in range(len(all_node_names)):\n",
    "                            for j in range(len(all_node_names)):\n",
    "                                # 상대적인 값에 따라 텍스트 색상 결정 (0에 가까울수록 검정, 최대값에 가까울수록 흰색)\n",
    "                                relative_value = final_graph_matrix[i,j] / vmax if vmax > 0 else 0\n",
    "                                text_color = \"black\" if relative_value < 0.7 else \"white\"\n",
    "                                \n",
    "                                # 값이 0일 경우 빈 문자열 표시할 수도 있음\n",
    "                                value_text = f\"{final_graph_matrix[i,j]:.3f}\"\n",
    "                                \n",
    "                                ax_final.text(j, i, value_text, \n",
    "                                            ha=\"center\", va=\"center\", \n",
    "                                            color=text_color, \n",
    "                                            fontsize=7)\n",
    "                        \n",
    "                        # 전체 제목 설정\n",
    "                        fig.suptitle(f'Layer {layer_idx} - Epoch {epoch} - Sample {sample_count}', fontsize=18)\n",
    "                        fig.tight_layout(rect=[0, 0.03, 1, 0.97])  # suptitle을 위한 여백 확보\n",
    "                        \n",
    "                        # # 레이어별 폴더에 저장\n",
    "                        # layer_dir = os.path.join(sample_dirs[sample_count], f'layer_{layer_idx}')\n",
    "                        # graph_path = os.path.join(layer_dir, f'epoch_{epoch}_complete.png')\n",
    "                        # fig.savefig(graph_path, dpi=300, bbox_inches='tight')\n",
    "                        # plt.close(fig)\n",
    "                        \n",
    "                        # logger.info(f\"샘플 {sample_count} - 레이어 {layer_idx} - 에포크 {epoch} 종합 시각화 저장: {graph_path}\")\n",
    "                        graph_path = os.path.join(sample_dirs[sample_count], 'graph', f'layer_{layer_idx}', f'epoch_{epoch}_complete.png')\n",
    "                        fig.savefig(graph_path, dpi=300, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "                        logger.info(f\"샘플 {sample_count} - 레이어 {layer_idx} - 에포크 {epoch} 그래프 시각화 저장: {graph_path}\")\n",
    "                sample_count += 1\n",
    "                if sample_count >= max_samples:\n",
    "                    break\n",
    "\n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_gmm_clusters(gmm, embeddings, output_dir=\"visualizations/gmm_clusters\", step=None, filename=None):\n",
    "    \"\"\"\n",
    "    t-SNE 시각화만 제공하는 함수입니다.\n",
    "    \n",
    "    Args:\n",
    "        gmm: GMM 또는 GMM2 모델 인스턴스\n",
    "        embeddings: 입력 임베딩 [batch_size, input_dim]\n",
    "        output_dir: 결과 저장 디렉토리\n",
    "        step: 스텝 번호 (파일명용)\n",
    "        filename: 사용자 지정 파일명 (선택사항)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.manifold import TSNE\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # 디바이스 설정\n",
    "        device = embeddings.device\n",
    "        \n",
    "        # GMM으로 클러스터 할당 확률 계산\n",
    "        if hasattr(gmm, 'forward'):\n",
    "            with torch.no_grad():\n",
    "                if 'GMM2' in gmm.__class__.__name__:\n",
    "                    r, _, _ = gmm(embeddings, is_train=False)\n",
    "                else:\n",
    "                    r, _ = gmm(embeddings, is_train=False)\n",
    "                \n",
    "                # 가장 확률이 높은 클러스터 할당\n",
    "                cluster_assignments = torch.argmax(r, dim=1).cpu().numpy()\n",
    "                # 할당 확률 (신뢰도)\n",
    "                confidences = torch.max(r, dim=1)[0].cpu().numpy()\n",
    "                # 프로토타입 가져오기\n",
    "                prototypes = gmm.running_prototypes.detach().cpu().numpy()\n",
    "        else:\n",
    "            # GMM 객체가 아닌 경우 (디버깅용)\n",
    "            prototypes = np.random.randn(32, embeddings.shape[1])\n",
    "            cluster_assignments = np.random.randint(0, 32, size=embeddings.shape[0])\n",
    "            confidences = np.random.rand(embeddings.shape[0])\n",
    "        \n",
    "        # 임베딩을 CPU로 이동\n",
    "        embeddings_np = embeddings.detach().cpu().numpy()\n",
    "        \n",
    "        # prototypes가 3차원인 경우 2차원으로 변환\n",
    "        if prototypes.ndim == 3:  # [1, num_prototypes, input_dim] 형태인 경우\n",
    "            prototypes = prototypes.squeeze(0)  # [num_prototypes, input_dim] 형태로 변환\n",
    "        \n",
    "        # t-SNE를 사용하여 프로토타입과 임베딩을 2D로 투영\n",
    "        combined_data = np.vstack([prototypes, embeddings_np])\n",
    "        \n",
    "        # t-SNE 차원 축소 (perplexity 조정)\n",
    "        tsne = TSNE(n_components=2, perplexity=min(30, len(combined_data)-1), \n",
    "                   random_state=42, learning_rate=200)\n",
    "        combined_reduced = tsne.fit_transform(combined_data)\n",
    "        \n",
    "        # 프로토타입과 임베딩으로 다시 분리\n",
    "        reduced_prototypes = combined_reduced[:len(prototypes)]\n",
    "        reduced_embeddings = combined_reduced[len(prototypes):]\n",
    "        \n",
    "        # ===== t-SNE 시각화만 생성 =====\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # 주요 프로토타입 추적 (시각화용)\n",
    "        key_prototypes = set(cluster_assignments)\n",
    "        \n",
    "        # 클러스터 통계 계산\n",
    "        cluster_counts = {}\n",
    "        for c in cluster_assignments:\n",
    "            cluster_counts[c] = cluster_counts.get(c, 0) + 1\n",
    "        \n",
    "        largest_cluster = max(cluster_counts.items(), key=lambda x: x[1]) if cluster_counts else (-1, 0)\n",
    "        empty_clusters = len(prototypes) - len(cluster_counts)\n",
    "        \n",
    "        # 프로토타입 시각화\n",
    "        for i, (x, y) in enumerate(reduced_prototypes):\n",
    "            # 주요 프로토타입 강조 (샘플이 할당된 것들)\n",
    "            if i in key_prototypes:\n",
    "                marker_size = 250\n",
    "                edge_width = 2.5\n",
    "                zorder = 10\n",
    "                marker_style = '*'\n",
    "                plt.scatter(x, y, s=marker_size, c=f'C{i % 10}', marker=marker_style, \n",
    "                           edgecolors='black', linewidths=edge_width, alpha=0.9,\n",
    "                           zorder=zorder, label=f'P{i+1} (Main)')\n",
    "                \n",
    "                # 프로토타입 레이블 (눈에 띄게)\n",
    "                plt.annotate(f'P{i+1}', (x, y), fontsize=14, fontweight='bold',\n",
    "                            ha='center', va='center', color='white',\n",
    "                            bbox=dict(boxstyle='round,pad=0.4', fc=f'C{i % 10}', alpha=0.8),\n",
    "                            zorder=zorder+1)\n",
    "            else:\n",
    "                # 비활성 프로토타입 (샘플이 할당되지 않은 것들)\n",
    "                marker_size = 100\n",
    "                plt.scatter(x, y, s=marker_size, c=f'C{i % 10}', marker='o', \n",
    "                           alpha=0.4, zorder=5)\n",
    "                plt.annotate(f'P{i+1}', (x, y), fontsize=9, ha='center', va='center', \n",
    "                            zorder=6)\n",
    "        \n",
    "        # 샘플 시각화 (다이아몬드 모양)\n",
    "        for j, (x, y) in enumerate(reduced_embeddings):\n",
    "            cluster_id = cluster_assignments[j]\n",
    "            confidence = confidences[j]\n",
    "            \n",
    "            # 샘플 마커\n",
    "            plt.scatter(x, y, s=250, c='white', marker='D', edgecolors='black', \n",
    "                       linewidths=2, alpha=0.9, zorder=15)\n",
    "            \n",
    "            # 샘플 ID\n",
    "            plt.annotate(f'S{j+1}', (x, y), fontsize=12, fontweight='bold', \n",
    "                        ha='center', va='center', zorder=16)\n",
    "            \n",
    "            for i, (proto_x, proto_y) in enumerate(reduced_prototypes):\n",
    "                if i == cluster_id:\n",
    "                    # 할당 확률에 비례하는 선 두께\n",
    "                    line_width = 1.5 + 5 * confidence\n",
    "                    plt.plot([x, proto_x], [y, proto_y], '--', \n",
    "                            linewidth=line_width, alpha=0.7, \n",
    "                            c=f'C{i % 10}', zorder=1)\n",
    "                    \n",
    "                    # 할당 확률 표시\n",
    "                    mid_x = (x + proto_x) * 0.6 + (proto_x + x) * 0.4\n",
    "                    mid_y = (y + proto_y) * 0.6 + (proto_y + y) * 0.4\n",
    "                    plt.annotate(f'{confidence:.2f}', (mid_x, mid_y), \n",
    "                                fontsize=11, fontweight='bold',\n",
    "                                bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.9),\n",
    "                                zorder=7)\n",
    "        \n",
    "        # 클러스터링 통계 정보 추가\n",
    "        info_text = f\"Largest cluster: {largest_cluster[1]} samples (P{largest_cluster[0]+1})\\n\"\n",
    "        info_text += f\"Empty clusters: {empty_clusters}\\n\"\n",
    "        \n",
    "        for c, count in sorted(cluster_counts.items()):\n",
    "            info_text += f\"Cluster {c+1}: {count} samples\\n\"\n",
    "        \n",
    "        plt.figtext(0.02, 0.02, info_text, fontsize=10,\n",
    "                  bbox=dict(boxstyle='round', fc='whitesmoke', alpha=0.9))\n",
    "        \n",
    "        # 그래프 제목 및 레이블\n",
    "        plt.title('t-SNE Visualization of Sample-Prototype Assignments', fontsize=14)\n",
    "        plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "        plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        if key_prototypes:\n",
    "            plt.legend(loc='upper right', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 파일 저장\n",
    "        if filename is None:\n",
    "            if step is not None:\n",
    "                filename = f\"tsne_only_{step}.png\"\n",
    "            else:\n",
    "                filename = \"tsne_only.png\"\n",
    "                \n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"t-SNE visualization saved to {os.path.join(output_dir, filename)}\")\n",
    "        \n",
    "        return {\n",
    "            'cluster_assignments': cluster_assignments,\n",
    "            'confidences': confidences,\n",
    "            'prototypes': reduced_prototypes,\n",
    "            'embeddings': reduced_embeddings\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"시각화 중 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_results(args, results, exp_dir):\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # few-shot이 8 이상일 때는 few-shot 결과만 시각화\n",
    "    if args.few_shot > 4:\n",
    "        fig, (ax3, ax4) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Few-shot의 Train vs Valid\n",
    "        ax3.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_train_few_losses\"], label='Train Loss')\n",
    "        ax3.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_val_few_losses\"], label='Valid Loss')\n",
    "        ax3.set_xlabel('Epochs')\n",
    "        ax3.set_ylabel('Loss')\n",
    "        ax3.set_title('Few-shot: Train vs Valid Loss')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "\n",
    "        ax4.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_train_few_auc\"], label='Train AUC')\n",
    "        ax4.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_val_few_auc\"], label='Valid AUC')\n",
    "        ax4.set_xlabel('Epochs')\n",
    "        ax4.set_ylabel('AUC')\n",
    "        ax4.set_title('Few-shot: Train vs Valid AUC')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "\n",
    "    # few-shot이 4일 때는 full과 few-shot 모두 시각화\n",
    "    else:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Full dataset의 Train vs Valid\n",
    "        ax1.plot(results[\"Full_results\"][\"Ours\"][\"Ours_train_full_losses\"], label='Train Loss')\n",
    "        ax1.plot(results[\"Full_results\"][\"Ours\"][\"Ours_val_full_losses\"], label='Valid Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Full Dataset: Train vs Valid Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(results[\"Full_results\"][\"Ours\"][\"Ours_train_full_auc\"], label='Train AUC')\n",
    "        ax2.plot(results[\"Full_results\"][\"Ours\"][\"Ours_val_full_auc\"], label='Valid AUC')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('AUC')\n",
    "        ax2.set_title('Full Dataset: Train vs Valid AUC')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # Few-shot의 Train vs Valid\n",
    "        ax3.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_train_few_losses\"], label='Train Loss')\n",
    "        ax3.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_val_few_losses\"], label='Valid Loss')\n",
    "        ax3.set_xlabel('Epochs')\n",
    "        ax3.set_ylabel('Loss')\n",
    "        ax3.set_title('Few-shot: Train vs Valid Loss')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "\n",
    "        ax4.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_train_few_auc\"], label='Train AUC')\n",
    "        ax4.plot(results[\"Full_results\"][\"Ours_few\"][\"Ours_val_few_auc\"], label='Valid AUC')\n",
    "        ax4.set_xlabel('Epochs')\n",
    "        ax4.set_ylabel('AUC')\n",
    "        ax4.set_title('Few-shot: Train vs Valid AUC')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "\n",
    "    plt.suptitle(f'Training Progress - {args.source_dataset_name} (K={args.few_shot})', y=1.02, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    metrics_plot_path = os.path.join(exp_dir, f\"f{args.few_shot}_b{args.batch_size}_l{args.num_layers}_h{args.n_heads}_{timestamp}.png\")\n",
    "    plt.savefig(metrics_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Metrics plot saved as {metrics_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
