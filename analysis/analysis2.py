"""
Comprehensive Clustering Metrics Analysis (Using Saved Attention Maps)

ì €ì¥ëœ attention mapsë¥¼ ë¡œë“œí•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ë©”íŠ¸ë¦­ì„ ë¶„ì„í•©ë‹ˆë‹¤.

Usage:
    python analysis4.py --attention_map_dir /path/to/attention_map/dir
"""

import os
# CUDA deterministic ì„¤ì •ì„ ê°€ì¥ ë¨¼ì € ì„¤ì •
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
from scipy.signal import find_peaks
from scipy.ndimage import gaussian_filter1d
import torch
import argparse
import numpy as np
import logging
from pathlib import Path
import json
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples, calinski_harabasz_score, davies_bouldin_score
from sklearn.manifold import TSNE
import seaborn as sns
from scipy.spatial.distance import cdist
import glob
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def ensure_deterministic():
    """ì™„ì „í•œ deterministic ì„¤ì •"""
    # Random seeds
    np.random.seed(42)
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    
    # CUDA deterministic
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.use_deterministic_algorithms(True)
    
    # Environment variables
    os.environ['PYTHONHASHSEED'] = '42'
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    
    logger.info("âœ… Deterministic mode enabled")

class SavedAttentionMapAnalyzer:
    def __init__(self, attention_map_dir):
        """
        Args:
            attention_map_dir (str): ì €ì¥ëœ attention maps ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        ensure_deterministic()
        
        self.attention_map_dir = Path(attention_map_dir)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self.metadata_path = self.attention_map_dir / "metadata.json"
        if not self.metadata_path.exists():
            raise FileNotFoundError(f"Metadata file not found: {self.metadata_path}")
        
        with open(self.metadata_path, 'r') as f:
            self.metadata = json.load(f)
        
        self.feature_names = self.metadata['feature_names']
        self.total_samples = self.metadata['total_samples']
        self.num_layers = self.metadata['num_layers']
        
        logger.info(f"Loaded metadata from {self.metadata_path}")
        logger.info(f"Total samples: {self.total_samples}")
        logger.info(f"Number of layers: {self.num_layers}")
        logger.info(f"Feature names: {len(self.feature_names)} features")
        
    def load_attention_maps(self):
        """
        ì €ì¥ëœ attention mapsë¥¼ ëª¨ë‘ ë¡œë“œ
        
        Returns:
            dict: ë ˆì´ì–´ë³„ attention mapsì™€ ë©”íƒ€ë°ì´í„°
        """
        logger.info("Loading saved attention maps...")
        
        # ìƒ˜í”Œ íŒŒì¼ë“¤ ì°¾ê¸°
        sample_files = list(self.attention_map_dir.glob("sample_*_label_*.npz"))
        sample_files.sort(key=lambda x: int(re.search(r'sample_(\d+)_', x.name).group(1)))
        
        if len(sample_files) != self.total_samples:
            logger.warning(f"Expected {self.total_samples} sample files, found {len(sample_files)}")
        
        # ë™ì ìœ¼ë¡œ ë ˆì´ì–´ ìˆ˜ì— ë§ì¶° ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬ ìƒì„±
        attention_data = {
            'labels': [],
            'sample_ids': [],
            'feature_names': self.feature_names
        }
        
        # ë ˆì´ì–´ë³„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        for layer_idx in range(self.num_layers):
            attention_data[f'layer_{layer_idx}'] = []
        
        # ê° ìƒ˜í”Œ íŒŒì¼ ë¡œë“œ
        for sample_file in sample_files:
            sample_data = np.load(sample_file)
            
            # ê° ë ˆì´ì–´ë³„ attention map ì €ì¥
            for layer_idx in range(self.num_layers):
                attention_map = sample_data[f'layer_{layer_idx}']
                attention_data[f'layer_{layer_idx}'].append(attention_map)
            
            # ë¼ë²¨ê³¼ ìƒ˜í”Œ ID ì €ì¥
            attention_data['labels'].append(sample_data['label'])
            attention_data['sample_ids'].append(sample_data['sample_id'])
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        for layer_idx in range(self.num_layers):
            attention_data[f'layer_{layer_idx}'] = np.stack(attention_data[f'layer_{layer_idx}'])
        
        attention_data['labels'] = np.array(attention_data['labels'])
        attention_data['sample_ids'] = np.array(attention_data['sample_ids'])
        
        logger.info(f"âœ… Loaded {len(sample_files)} attention maps")
        logger.info(f"   Shape per layer: {attention_data['layer_0'].shape}")
        
        return attention_data

    def calculate_within_between_variance(self, data, cluster_labels):
        """
        Within-cluster Sum of Squares (WCSS)ì™€ Between-cluster Sum of Squares (BCSS) ê³„ì‚°
        
        Args:
            data: í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° [n_samples, n_features]
            cluster_labels: í´ëŸ¬ìŠ¤í„° í• ë‹¹ ë¼ë²¨ [n_samples]
            
        Returns:
            dict: WCSS, BCSS, Total SS, ë¹„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­
        """
        n_samples = len(data)
        k = len(np.unique(cluster_labels))
        
        # ì „ì²´ ë°ì´í„°ì˜ ì¤‘ì‹¬ì  (Grand Centroid)
        grand_centroid = np.mean(data, axis=0)
        
        # Total Sum of Squares (TSS)
        tss = np.sum((data - grand_centroid) ** 2)
        
        # Within-cluster Sum of Squares (WCSS)
        wcss = 0
        cluster_centroids = []
        cluster_sizes = []
        
        for cluster_id in np.unique(cluster_labels):
            cluster_mask = cluster_labels == cluster_id
            cluster_data = data[cluster_mask]
            cluster_centroid = np.mean(cluster_data, axis=0)
            cluster_centroids.append(cluster_centroid)
            cluster_sizes.append(len(cluster_data))
            
            # í´ëŸ¬ìŠ¤í„° ë‚´ ë¶„ì‚° ê³„ì‚°
            cluster_wcss = np.sum((cluster_data - cluster_centroid) ** 2)
            wcss += cluster_wcss
        
        cluster_centroids = np.array(cluster_centroids)
        cluster_sizes = np.array(cluster_sizes)
        
        # Between-cluster Sum of Squares (BCSS)
        bcss = 0
        for i, centroid in enumerate(cluster_centroids):
            bcss += cluster_sizes[i] * np.sum((centroid - grand_centroid) ** 2)
        
        # ê²€ì¦: TSS = WCSS + BCSS (ë°˜ë“œì‹œ ì„±ë¦½í•´ì•¼ í•¨)
        tss_check = wcss + bcss
        
        return {
            'wcss': float(wcss),
            'bcss': float(bcss),
            'tss': float(tss),
            'tss_check': float(tss_check),
            'bcss_wcss_ratio': float(bcss / wcss) if wcss > 0 else float('inf'),
            'explained_variance_ratio': float(bcss / tss) if tss > 0 else 0.0,
            'n_clusters': int(k),
            'n_samples': int(n_samples),
            'cluster_sizes': cluster_sizes.tolist(),
            'verification_error': float(abs(tss - tss_check))
        }

    def calculate_comprehensive_metrics(self, data, cluster_labels):
        """
        ëª¨ë“  í´ëŸ¬ìŠ¤í„°ë§ ë©”íŠ¸ë¦­ì„ ì¢…í•©ì ìœ¼ë¡œ ê³„ì‚°
        
        Args:
            data: í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„°
            cluster_labels: í´ëŸ¬ìŠ¤í„° í• ë‹¹ ë¼ë²¨
            
        Returns:
            dict: ëª¨ë“  ë©”íŠ¸ë¦­ ê²°ê³¼
        """
        metrics = {}
        
        # 1. Silhouette Metrics
        silhouette_avg = silhouette_score(data, cluster_labels)
        silhouette_samples_scores = silhouette_samples(data, cluster_labels)
        
        metrics['silhouette'] = {
            'average_score': float(silhouette_avg),
            'sample_scores': silhouette_samples_scores.tolist(),
            'min_score': float(np.min(silhouette_samples_scores)),
            'max_score': float(np.max(silhouette_samples_scores)),
            'std_score': float(np.std(silhouette_samples_scores))
        }
        
        # 2. Within/Between Variance Analysis
        variance_metrics = self.calculate_within_between_variance(data, cluster_labels)
        metrics['variance_analysis'] = variance_metrics
        
        # 3. Calinski-Harabasz Index (ë¶„ì‚°ë¹„ ê¸°ì¤€)
        ch_score = calinski_harabasz_score(data, cluster_labels)
        metrics['calinski_harabasz'] = {
            'score': float(ch_score),
            'interpretation': 'higher_is_better'
        }
        
        # 4. Davies-Bouldin Index
        db_score = davies_bouldin_score(data, cluster_labels)
        metrics['davies_bouldin'] = {
            'score': float(db_score),
            'interpretation': 'lower_is_better'
        }
        
        # 5. Inertia (K-means WCSS)
        kmeans_temp = KMeans(n_clusters=len(np.unique(cluster_labels)), random_state=42, n_init=1)
        kmeans_temp.fit(data)
        metrics['inertia'] = {
            'score': float(kmeans_temp.inertia_),
            'interpretation': 'lower_is_better'
        }
        
        # 6. Dunn Index (ìµœì†Œ inter-cluster distance / ìµœëŒ€ intra-cluster distance)
        dunn_score = self._calculate_dunn_index(data, cluster_labels)
        metrics['dunn_index'] = {
            'score': float(dunn_score),
            'interpretation': 'higher_is_better'
        }
        
        # 7. í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„
        cluster_stats = self._calculate_cluster_statistics(data, cluster_labels)
        metrics['cluster_statistics'] = cluster_stats
        
        return metrics

    def _calculate_dunn_index(self, data, cluster_labels):
        """Dunn Index ê³„ì‚°"""
        unique_labels = np.unique(cluster_labels)
        
        # Inter-cluster distances (í´ëŸ¬ìŠ¤í„° ê°„ ìµœì†Œ ê±°ë¦¬)
        inter_cluster_distances = []
        for i in range(len(unique_labels)):
            for j in range(i + 1, len(unique_labels)):
                cluster_i = data[cluster_labels == unique_labels[i]]
                cluster_j = data[cluster_labels == unique_labels[j]]
                
                # ë‘ í´ëŸ¬ìŠ¤í„° ê°„ ëª¨ë“  ì ë“¤ì˜ ìµœì†Œ ê±°ë¦¬
                distances = cdist(cluster_i, cluster_j, metric='euclidean')
                min_distance = np.min(distances)
                inter_cluster_distances.append(min_distance)
        
        # Intra-cluster distances (í´ëŸ¬ìŠ¤í„° ë‚´ ìµœëŒ€ ê±°ë¦¬)
        intra_cluster_distances = []
        for label in unique_labels:
            cluster_data = data[cluster_labels == label]
            if len(cluster_data) > 1:
                distances = cdist(cluster_data, cluster_data, metric='euclidean')
                # ëŒ€ê°ì„  ì œì™¸í•˜ê³  ìµœëŒ€ ê±°ë¦¬
                np.fill_diagonal(distances, 0)
                max_distance = np.max(distances)
                intra_cluster_distances.append(max_distance)
        
        if len(inter_cluster_distances) == 0 or len(intra_cluster_distances) == 0:
            return 0.0
        
        min_inter = np.min(inter_cluster_distances)
        max_intra = np.max(intra_cluster_distances)
        
        return min_inter / max_intra if max_intra > 0 else 0.0

    def _calculate_cluster_statistics(self, data, cluster_labels):
        """í´ëŸ¬ìŠ¤í„°ë³„ ì„¸ë¶€ í†µê³„ ê³„ì‚°"""
        unique_labels = np.unique(cluster_labels)
        cluster_stats = {}
        
        for label in unique_labels:
            cluster_data = data[cluster_labels == label]
            centroid = np.mean(cluster_data, axis=0)
            
            # í´ëŸ¬ìŠ¤í„° ë‚´ ê±°ë¦¬ë“¤
            if len(cluster_data) > 1:
                distances_to_centroid = np.linalg.norm(cluster_data - centroid, axis=1)
                intra_cluster_distances = cdist(cluster_data, cluster_data, metric='euclidean')
                np.fill_diagonal(intra_cluster_distances, np.inf)  # ìê¸° ìì‹  ì œì™¸
                
                cluster_stats[f'cluster_{label}'] = {
                    'size': int(len(cluster_data)),
                    'centroid': centroid.tolist(),
                    'mean_distance_to_centroid': float(np.mean(distances_to_centroid)),
                    'max_distance_to_centroid': float(np.max(distances_to_centroid)),
                    'std_distance_to_centroid': float(np.std(distances_to_centroid)),
                    'min_intra_distance': float(np.min(intra_cluster_distances)),
                    'mean_intra_distance': float(np.mean(intra_cluster_distances[intra_cluster_distances != np.inf])),
                    'max_intra_distance': float(np.max(intra_cluster_distances[intra_cluster_distances != np.inf]))
                }
            else:
                cluster_stats[f'cluster_{label}'] = {
                    'size': 1,
                    'centroid': centroid.tolist(),
                    'mean_distance_to_centroid': 0.0,
                    'max_distance_to_centroid': 0.0,
                    'std_distance_to_centroid': 0.0,
                    'min_intra_distance': 0.0,
                    'mean_intra_distance': 0.0,
                    'max_intra_distance': 0.0
                }
        
        return cluster_stats

    def calculate_elbow_point(self, k_values, wcss_scores, method='knee'):
        """
        WCSS ê°’ì—ì„œ elbow pointë¥¼ ìë™ìœ¼ë¡œ ì°¾ëŠ” í•¨ìˆ˜
        
        Args:
            k_values: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ë¦¬ìŠ¤íŠ¸
            wcss_scores: ê° kì— ëŒ€í•œ WCSS ê°’
            method: 'knee', 'derivative', 'variance' ì¤‘ ì„ íƒ
        
        Returns:
            dict: elbow point ì •ë³´
        """
        k_values = np.array(k_values)
        wcss_scores = np.array(wcss_scores)
        
        if method == 'knee':
            # Knee detection using the "knee/elbow" method
            # ì²« ë²ˆì§¸ ì ê³¼ ë§ˆì§€ë§‰ ì ì„ ì‡ëŠ” ì§ì„ ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬ê°€ ìµœëŒ€ì¸ ì  ì°¾ê¸°
            
            # Normalize data to [0,1] for better knee detection
            k_norm = (k_values - k_values.min()) / (k_values.max() - k_values.min())
            wcss_norm = (wcss_scores - wcss_scores.min()) / (wcss_scores.max() - wcss_scores.min())
            
            # ì²« ì ê³¼ ë§ˆì§€ì ì„ ì‡ëŠ” ì§ì„ 
            line_start = np.array([k_norm[0], wcss_norm[0]])
            line_end = np.array([k_norm[-1], wcss_norm[-1]])
            
            # ê° ì ì—ì„œ ì§ì„ ê¹Œì§€ì˜ ê±°ë¦¬ ê³„ì‚°
            distances = []
            for i in range(len(k_norm)):
                point = np.array([k_norm[i], wcss_norm[i]])
                # ì ì—ì„œ ì§ì„ ê¹Œì§€ì˜ ê±°ë¦¬ ê³µì‹
                distance = np.abs(np.cross(line_end - line_start, line_start - point)) / np.linalg.norm(line_end - line_start)
                distances.append(distance)
            
            elbow_idx = np.argmax(distances)
            elbow_k = k_values[elbow_idx]
            elbow_score = wcss_scores[elbow_idx]
            max_distance = distances[elbow_idx]
            
            return {
                'method': 'knee',
                'elbow_k': int(elbow_k),
                'elbow_wcss': float(elbow_score),
                'confidence': float(max_distance),
                'all_distances': distances
            }
        
        elif method == 'derivative':
            # Second derivative method
            if len(wcss_scores) < 3:
                return {'method': 'derivative', 'elbow_k': k_values[0], 'elbow_wcss': wcss_scores[0], 'confidence': 0.0}
            
            # 1ì°¨ ë¯¸ë¶„ (ê¸°ìš¸ê¸°)
            first_derivative = np.diff(wcss_scores)
            # 2ì°¨ ë¯¸ë¶„ (ê¸°ìš¸ê¸°ì˜ ë³€í™”)
            second_derivative = np.diff(first_derivative)
            
            # 2ì°¨ ë¯¸ë¶„ì´ ìµœëŒ€ì¸ ì§€ì  (ê°€ì¥ ê¸‰ê²©í•˜ê²Œ ê¸°ìš¸ê¸°ê°€ ë³€í•˜ëŠ” ì§€ì )
            elbow_idx = np.argmax(second_derivative) + 1  # +1 because of diff operations
            elbow_k = k_values[elbow_idx]
            elbow_score = wcss_scores[elbow_idx]
            
            return {
                'method': 'derivative',
                'elbow_k': int(elbow_k),
                'elbow_wcss': float(elbow_score),
                'confidence': float(second_derivative[elbow_idx-1]),
                'first_derivative': first_derivative.tolist(),
                'second_derivative': second_derivative.tolist()
            }
        
        elif method == 'variance':
            # Variance-based method: ê¸°ìš¸ê¸°ì˜ ë¶„ì‚°ì´ ì¤„ì–´ë“œëŠ” ì§€ì 
            if len(wcss_scores) < 4:
                return {'method': 'variance', 'elbow_k': k_values[0], 'elbow_wcss': wcss_scores[0], 'confidence': 0.0}
            
            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ê¸°ìš¸ê¸°ì˜ ë¶„ì‚° ê³„ì‚°
            window_size = min(3, len(wcss_scores) // 2)
            variances = []
            
            for i in range(window_size, len(wcss_scores) - window_size):
                window_slopes = []
                for j in range(i - window_size, i + window_size):
                    if j < len(wcss_scores) - 1:
                        slope = (wcss_scores[j+1] - wcss_scores[j]) / (k_values[j+1] - k_values[j])
                        window_slopes.append(slope)
                variances.append(np.var(window_slopes))
            
            # ë¶„ì‚°ì´ ìµœì†Œì¸ ì§€ì  (ê¸°ìš¸ê¸°ê°€ ì•ˆì •í™”ë˜ëŠ” ì§€ì )
            elbow_idx = np.argmin(variances) + window_size
            elbow_k = k_values[elbow_idx]
            elbow_score = wcss_scores[elbow_idx]
            
            return {
                'method': 'variance',
                'elbow_k': int(elbow_k),
                'elbow_wcss': float(elbow_score),
                'confidence': float(1.0 / (variances[elbow_idx - window_size] + 1e-10)),
                'variances': variances
            }

    def perform_comprehensive_analysis_with_elbow(self, attention_data, layer_idx, k_range=(2, 15), output_dir=None):
        """
        Elbow methodê°€ í¬í•¨ëœ ì¢…í•©ì ì¸ í´ëŸ¬ìŠ¤í„°ë§ ë©”íŠ¸ë¦­ ë¶„ì„ ìˆ˜í–‰
        """
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # íŠ¹ì • ë ˆì´ì–´ì˜ attention maps ì¶”ì¶œ
        attention_maps = attention_data[f'layer_{layer_idx}']
        labels = attention_data['labels']
        feature_names = attention_data['feature_names']
        
        logger.info(f"Performing comprehensive analysis (with Elbow) on layer {layer_idx} with {len(attention_maps)} samples")
        logger.info(f"Testing cluster range: {k_range[0]} to {k_range[1]}")
        
        # í‰íƒ„í™” (ë²¡í„°í™”)
        flattened_maps = attention_maps.reshape(len(attention_maps), -1)
        
        min_k, max_k = k_range
        k_values = list(range(min_k, max_k + 1))
        
        # ê° kê°’ì— ëŒ€í•´ ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚°
        comprehensive_results = {
            'layer_idx': layer_idx,
            'k_values': k_values,
            'feature_names': feature_names,
            'results_by_k': {}
        }
        
        wcss_scores = []
        
        for k in k_values:
            logger.info(f"Testing k={k}...")
            
            # Deterministic K-means
            np.random.seed(42)
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, algorithm='lloyd')
            cluster_labels = kmeans.fit_predict(flattened_maps)
            
            # ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚°
            all_metrics = self.calculate_comprehensive_metrics(flattened_maps, cluster_labels)
            
            # WCSS ì €ì¥
            wcss_scores.append(all_metrics['variance_analysis']['wcss'])
            
            # ê²°ê³¼ ì €ì¥
            comprehensive_results['results_by_k'][k] = {
                'cluster_assignments': cluster_labels.tolist(),
                'metrics': all_metrics,
                'kmeans_centers': kmeans.cluster_centers_.tolist()
            }
            
            # ì£¼ìš” ë©”íŠ¸ë¦­ ë¡œê·¸ ì¶œë ¥
            silhouette = all_metrics['silhouette']['average_score']
            ch_score = all_metrics['calinski_harabasz']['score']
            db_score = all_metrics['davies_bouldin']['score']
            wcss = all_metrics['variance_analysis']['wcss']
            bcss = all_metrics['variance_analysis']['bcss']
            
            logger.info(f"k={k}: Silhouette={silhouette:.4f}, CH={ch_score:.2f}, DB={db_score:.4f}, WCSS={wcss:.2f}, BCSS={bcss:.2f}")
        
        # Elbow method ì ìš©
        elbow_knee = self.calculate_elbow_point(k_values, wcss_scores, method='knee')
        elbow_derivative = self.calculate_elbow_point(k_values, wcss_scores, method='derivative')
        elbow_variance = self.calculate_elbow_point(k_values, wcss_scores, method='variance')
        
        comprehensive_results['elbow_analysis'] = {
            'knee_method': elbow_knee,
            'derivative_method': elbow_derivative,
            'variance_method': elbow_variance,
            'wcss_scores': wcss_scores
        }
        
        # ê¸°ì¡´ ë©”íŠ¸ë¦­ ê¸°ì¤€ ìµœì  k
        silhouette_scores = [comprehensive_results['results_by_k'][k]['metrics']['silhouette']['average_score'] for k in k_values]
        ch_scores = [comprehensive_results['results_by_k'][k]['metrics']['calinski_harabasz']['score'] for k in k_values]
        db_scores = [comprehensive_results['results_by_k'][k]['metrics']['davies_bouldin']['score'] for k in k_values]
        
        comprehensive_results['best_k_silhouette'] = k_values[np.argmax(silhouette_scores)]
        comprehensive_results['best_silhouette_score'] = max(silhouette_scores)
        comprehensive_results['best_k_calinski_harabasz'] = k_values[np.argmax(ch_scores)]
        comprehensive_results['best_k_davies_bouldin'] = k_values[np.argmin(db_scores)]
        
        # Elbow method ê¸°ì¤€ ìµœì  k (ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ë°©ë²• ì„ íƒ)
        elbow_methods = [elbow_knee, elbow_derivative, elbow_variance]
        best_elbow = max(elbow_methods, key=lambda x: x['confidence'])
        comprehensive_results['best_k_elbow'] = best_elbow['elbow_k']
        comprehensive_results['best_elbow_method'] = best_elbow['method']
        
        logger.info(f"ğŸ¯ Best k for layer {layer_idx}:")
        logger.info(f"   Silhouette: k={comprehensive_results['best_k_silhouette']} (score: {max(silhouette_scores):.4f})")
        logger.info(f"   Calinski-Harabasz: k={comprehensive_results['best_k_calinski_harabasz']} (score: {max(ch_scores):.2f})")
        logger.info(f"   Davies-Bouldin: k={comprehensive_results['best_k_davies_bouldin']} (score: {min(db_scores):.4f})")
        logger.info(f"   ğŸ”¥ Elbow Method: k={comprehensive_results['best_k_elbow']} (method: {comprehensive_results['best_elbow_method']})")
        
        # ì‹œê°í™” ìƒì„±
        if output_dir:
            self._create_comprehensive_visualizations_with_elbow(comprehensive_results, flattened_maps, labels, output_dir)
            
            # ê²°ê³¼ JSON ì €ì¥
            results_json = comprehensive_results.copy()
            results_json['feature_names'] = list(feature_names)
            
            with open(output_dir / f'layer_{layer_idx}_comprehensive_results_with_elbow.json', 'w') as f:
                json.dump(results_json, f, indent=2)
            
            logger.info(f"âœ… Comprehensive analysis (with Elbow) results saved to {output_dir}")
        
        return comprehensive_results

    def _create_comprehensive_visualizations_with_elbow(self, results, flattened_maps, labels, output_dir):
        """Elbow method í¬í•¨ ì¢…í•© ì‹œê°í™” ìƒì„±"""
        layer_idx = results['layer_idx']
        k_values = results['k_values']
        
        # 1. Elbow Method ì‹œê°í™”
        plt.figure(figsize=(15, 12))
        
        # WCSS Elbow Plot
        plt.subplot(2, 3, 1)
        wcss_scores = results['elbow_analysis']['wcss_scores']
        plt.plot(k_values, wcss_scores, 'bo-', linewidth=2, markersize=8)
        
        # Elbow í¬ì¸íŠ¸ë“¤ í‘œì‹œ
        knee_k = results['elbow_analysis']['knee_method']['elbow_k']
        derivative_k = results['elbow_analysis']['derivative_method']['elbow_k']
        variance_k = results['elbow_analysis']['variance_method']['elbow_k']
        
        plt.axvline(x=knee_k, color='red', linestyle='--', alpha=0.7, label=f'Knee Method (k={knee_k})')
        plt.axvline(x=derivative_k, color='green', linestyle='--', alpha=0.7, label=f'Derivative Method (k={derivative_k})')
        plt.axvline(x=variance_k, color='orange', linestyle='--', alpha=0.7, label=f'Variance Method (k={variance_k})')
        
        plt.xlabel('Number of Clusters (k)')
        plt.ylabel('WCSS')
        plt.title(f'Layer {layer_idx}: Elbow Method Analysis')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. Silhouette Score vs k
        plt.subplot(2, 3, 2)
        silhouette_scores = [results['results_by_k'][k]['metrics']['silhouette']['average_score'] for k in k_values]
        plt.plot(k_values, silhouette_scores, 'go-', linewidth=2, markersize=8)
        best_k_sil = results['best_k_silhouette']
        plt.axvline(x=best_k_sil, color='red', linestyle='--', alpha=0.7, label=f'Best k={best_k_sil}')
        plt.xlabel('Number of Clusters (k)')
        plt.ylabel('Silhouette Score')
        plt.title(f'Layer {layer_idx}: Silhouette Analysis')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. Calinski-Harabasz Index vs k
        plt.subplot(2, 3, 3)
        ch_scores = [results['results_by_k'][k]['metrics']['calinski_harabasz']['score'] for k in k_values]
        plt.plot(k_values, ch_scores, 'mo-', linewidth=2, markersize=8)
        best_k_ch = results['best_k_calinski_harabasz']
        plt.axvline(x=best_k_ch, color='red', linestyle='--', alpha=0.7, label=f'Best k={best_k_ch}')
        plt.xlabel('Number of Clusters (k)')
        plt.ylabel('Calinski-Harabasz Index')
        plt.title(f'Layer {layer_idx}: Calinski-Harabasz Analysis')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 4. Davies-Bouldin Index vs k
        plt.subplot(2, 3, 4)
        db_scores = [results['results_by_k'][k]['metrics']['davies_bouldin']['score'] for k in k_values]
        plt.plot(k_values, db_scores, 'co-', linewidth=2, markersize=8)
        best_k_db = results['best_k_davies_bouldin']
        plt.axvline(x=best_k_db, color='red', linestyle='--', alpha=0.7, label=f'Best k={best_k_db}')
        plt.xlabel('Number of Clusters (k)')
        plt.ylabel('Davies-Bouldin Index')
        plt.title(f'Layer {layer_idx}: Davies-Bouldin Analysis')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 5. ëª¨ë“  ë©”íŠ¸ë¦­ ì •ê·œí™” ë¹„êµ
        plt.subplot(2, 3, 5)
        # ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
        sil_norm = (np.array(silhouette_scores) - np.min(silhouette_scores)) / (np.max(silhouette_scores) - np.min(silhouette_scores))
        ch_norm = (np.array(ch_scores) - np.min(ch_scores)) / (np.max(ch_scores) - np.min(ch_scores))
        db_norm = 1 - (np.array(db_scores) - np.min(db_scores)) / (np.max(db_scores) - np.min(db_scores))  # DBëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        wcss_norm = 1 - (np.array(wcss_scores) - np.min(wcss_scores)) / (np.max(wcss_scores) - np.min(wcss_scores))  # WCSSë„ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        
        plt.plot(k_values, sil_norm, 'g-', label='Silhouette (normalized)', linewidth=2)
        plt.plot(k_values, ch_norm, 'm-', label='Calinski-Harabasz (normalized)', linewidth=2)
        plt.plot(k_values, db_norm, 'c-', label='Davies-Bouldin (inverted & normalized)', linewidth=2)
        plt.plot(k_values, wcss_norm, 'r-', label='WCSS (inverted & normalized)', linewidth=2)
        
        plt.xlabel('Number of Clusters (k)')
        plt.ylabel('Normalized Score')
        plt.title(f'Layer {layer_idx}: All Metrics Comparison')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 6. t-SNE ì‹œê°í™” (ìµœì  kë¡œ)
        plt.subplot(2, 3, 6)
        best_k = results['best_k_elbow']
        best_cluster_labels = results['results_by_k'][best_k]['cluster_assignments']
        
        # t-SNE ì°¨ì› ì¶•ì†Œ (ìƒ˜í”Œì´ ë§ìœ¼ë©´ subset ì‚¬ìš©)
        if len(flattened_maps) > 1000:
            sample_indices = np.random.choice(len(flattened_maps), 1000, replace=False)
            tsne_data = flattened_maps[sample_indices]
            tsne_labels = np.array(best_cluster_labels)[sample_indices]
        else:
            tsne_data = flattened_maps
            tsne_labels = np.array(best_cluster_labels)
        
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(tsne_data)-1))
        tsne_result = tsne.fit_transform(tsne_data)
        
        scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=tsne_labels, cmap='viridis', alpha=0.7)
        plt.colorbar(scatter)
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')
        plt.title(f'Layer {layer_idx}: t-SNE Visualization (k={best_k})')
        
        plt.tight_layout()
        plt.savefig(output_dir / f'layer_{layer_idx}_comprehensive_analysis_with_elbow.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # ê°œë³„ Elbow ë©”ì„œë“œ ì„¸ë¶€ ì‹œê°í™”
        self._create_elbow_detail_plots(results, output_dir)
        
        logger.info(f"âœ… Visualizations saved for layer {layer_idx}")

    def _create_elbow_detail_plots(self, results, output_dir):
        """Elbow ë©”ì„œë“œ ì„¸ë¶€ ë¶„ì„ í”Œë¡¯"""
        layer_idx = results['layer_idx']
        k_values = results['k_values']
        wcss_scores = results['elbow_analysis']['wcss_scores']
        
        plt.figure(figsize=(18, 6))
        
        # 1. Knee Method ì„¸ë¶€
        plt.subplot(1, 3, 1)
        knee_data = results['elbow_analysis']['knee_method']
        distances = knee_data['all_distances']
        
        plt.plot(k_values, wcss_scores, 'bo-', linewidth=2, markersize=8, label='WCSS')
        plt.axvline(x=knee_data['elbow_k'], color='red', linestyle='--', linewidth=2, 
                   label=f"Knee Point (k={knee_data['elbow_k']})")
        
        # ê±°ë¦¬ ê·¸ë˜í”„ (ë³´ì¡° yì¶•)
        ax2 = plt.gca().twinx()
        ax2.plot(k_values, distances, 'ro-', alpha=0.7, label='Distance to Line')
        ax2.set_ylabel('Distance to Line', color='red')
        
        plt.xlabel('Number of Clusters (k)')
        plt.ylabel('WCSS')
        plt.title(f'Layer {layer_idx}: Knee Method Detail\n(Confidence: {knee_data["confidence"]:.4f})')
        plt.legend(loc='upper right')
        plt.grid(True, alpha=0.3)
        
        # 2. Derivative Method ì„¸ë¶€
        plt.subplot(1, 3, 2)
        derivative_data = results['elbow_analysis']['derivative_method']
        
        plt.plot(k_values, wcss_scores, 'bo-', linewidth=2, markersize=8, label='WCSS')
        plt.axvline(x=derivative_data['elbow_k'], color='green', linestyle='--', linewidth=2,
                   label=f"Derivative Point (k={derivative_data['elbow_k']})")
        
        # 1ì°¨ ë¯¸ë¶„ ê·¸ë˜í”„ (ë³´ì¡° yì¶•)
        if 'first_derivative' in derivative_data:
            ax2 = plt.gca().twinx()
            first_deriv = derivative_data['first_derivative']
            ax2.plot(k_values[1:], first_deriv, 'go-', alpha=0.7, label='1st Derivative')
            ax2.set_ylabel('1st Derivative', color='green')
        
        plt.xlabel('Number of Clusters (k)')
        plt.ylabel('WCSS')
        plt.title(f'Layer {layer_idx}: Derivative Method Detail\n(Confidence: {derivative_data["confidence"]:.4f})')
        plt.legend(loc='upper right')
        plt.grid(True, alpha=0.3)
        
        # 3. Variance Method ì„¸ë¶€
        plt.subplot(1, 3, 3)
        variance_data = results['elbow_analysis']['variance_method']
        
        plt.plot(k_values, wcss_scores, 'bo-', linewidth=2, markersize=8, label='WCSS')
        plt.axvline(x=variance_data['elbow_k'], color='orange', linestyle='--', linewidth=2,
                   label=f"Variance Point (k={variance_data['elbow_k']})")
        
        # ë¶„ì‚° ê·¸ë˜í”„ (ë³´ì¡° yì¶•)
        if 'variances' in variance_data and len(variance_data['variances']) > 0:
            ax2 = plt.gca().twinx()
            variances = variance_data['variances']
            
            # variance methodì—ì„œ ì‚¬ìš©ëœ ì‹¤ì œ window_size ê³„ì‚°
            # calculate_elbow_pointì˜ variance method ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ
            window_size = min(3, len(k_values) // 2)
            
            # variance ê°’ë“¤ì— ëŒ€ì‘í•˜ëŠ” k ê°’ë“¤ (window_sizeë§Œí¼ ì•ë’¤ ì œì™¸)
            if len(variances) > 0:
                var_k_start = window_size
                var_k_end = len(k_values) - window_size
                var_k_values = k_values[var_k_start:var_k_end]
                
                # ê¸¸ì´ ë§ì¶”ê¸° (ì•ˆì „ì¥ì¹˜)
                min_len = min(len(var_k_values), len(variances))
                var_k_values = var_k_values[:min_len]
                variances_plot = variances[:min_len]
                
                if len(var_k_values) > 0 and len(variances_plot) > 0:
                    ax2.plot(var_k_values, variances_plot, 'o-', color='orange', alpha=0.7, label='Slope Variance')
                    ax2.set_ylabel('Slope Variance', color='orange')
                else:
                    # variance ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                    ax2.text(0.5, 0.5, 'No variance data available', 
                            ha='center', va='center', transform=ax2.transAxes, color='orange')
        
        plt.xlabel('Number of Clusters (k)')
        plt.ylabel('WCSS')
        plt.title(f'Layer {layer_idx}: Variance Method Detail\n(Confidence: {variance_data["confidence"]:.4f})')
        plt.legend(loc='upper right')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / f'layer_{layer_idx}_elbow_methods_detail.png', dpi=300, bbox_inches='tight')
        plt.close()

    def analyze_all_layers_with_elbow(self, k_range=(2, 15), output_dir=None):
        """
        ëª¨ë“  ë ˆì´ì–´ì— ëŒ€í•´ Elbow methodê°€ í¬í•¨ëœ ì¢…í•©ì ì¸ ë©”íŠ¸ë¦­ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì €ì¥ëœ Attention maps ë¡œë“œ
        logger.info("Loading saved attention maps...")
        attention_data = self.load_attention_maps()
        
        # ëª¨ë“  ë ˆì´ì–´ ë¶„ì„ (Elbow í¬í•¨)
        all_results = {}
        for layer_idx in range(self.num_layers):
            logger.info(f"\n{'='*60}")
            logger.info(f"Analyzing Layer {layer_idx} (WITH ELBOW METHOD)")
            logger.info(f"{'='*60}")
            
            layer_output_dir = output_dir / f'layer_{layer_idx}' if output_dir else None
            results = self.perform_comprehensive_analysis_with_elbow(
                attention_data, 
                layer_idx, 
                k_range=k_range,
                output_dir=layer_output_dir
            )
            all_results[f'layer_{layer_idx}'] = results
        
        # ì „ì²´ ìš”ì•½ ìƒì„± (Elbow í¬í•¨)
        if output_dir:
            self._generate_comprehensive_summary_with_elbow(all_results, output_dir)
        
        return all_results

    def _generate_comprehensive_summary_with_elbow(self, all_results, output_dir):
        """Elbow method í¬í•¨ ì¢…í•© ìš”ì•½ ìƒì„±"""
        logger.info("Generating comprehensive summary with elbow method...")
        
        # ë ˆì´ì–´ë³„ ìµœì  k ìš”ì•½
        summary = {
            'analysis_timestamp': str(np.datetime64('now')),
            'total_layers': len(all_results),
            'layer_summary': {},
            'cross_layer_analysis': {}
        }
        
        # ê° ë ˆì´ì–´ë³„ ìš”ì•½
        for layer_key, layer_results in all_results.items():
            layer_idx = layer_results['layer_idx']
            
            summary['layer_summary'][layer_key] = {
                'best_k_silhouette': layer_results['best_k_silhouette'],
                'best_silhouette_score': layer_results['best_silhouette_score'],
                'best_k_calinski_harabasz': layer_results['best_k_calinski_harabasz'],
                'best_k_davies_bouldin': layer_results['best_k_davies_bouldin'],
                'best_k_elbow': layer_results['best_k_elbow'],
                'best_elbow_method': layer_results['best_elbow_method'],
                'elbow_methods': {
                    'knee': layer_results['elbow_analysis']['knee_method']['elbow_k'],
                    'derivative': layer_results['elbow_analysis']['derivative_method']['elbow_k'],
                    'variance': layer_results['elbow_analysis']['variance_method']['elbow_k']
                }
            }
        
        # ë ˆì´ì–´ ê°„ ë¹„êµ ë¶„ì„
        silhouette_bests = [summary['layer_summary'][f'layer_{i}']['best_k_silhouette'] for i in range(len(all_results))]
        elbow_bests = [summary['layer_summary'][f'layer_{i}']['best_k_elbow'] for i in range(len(all_results))]
        
        summary['cross_layer_analysis'] = {
            'silhouette_consistency': len(set(silhouette_bests)) == 1,
            'elbow_consistency': len(set(elbow_bests)) == 1,
            'most_common_k_silhouette': int(np.bincount(silhouette_bests).argmax()),
            'most_common_k_elbow': int(np.bincount(elbow_bests).argmax()),
            'k_variance_silhouette': float(np.var(silhouette_bests)),
            'k_variance_elbow': float(np.var(elbow_bests))
        }
        
        # JSON ì €ì¥
        with open(output_dir / 'comprehensive_summary_with_elbow.json', 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        # ìš”ì•½ ì‹œê°í™”
        self._create_summary_visualizations_with_elbow(all_results, summary, output_dir)
        
        logger.info(f"âœ… Comprehensive summary with elbow method saved to {output_dir}")
        
        # ì½˜ì†” ìš”ì•½ ì¶œë ¥
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š COMPREHENSIVE ANALYSIS SUMMARY (WITH ELBOW METHOD)")
        logger.info("="*80)
        
        for layer_key in summary['layer_summary']:
            layer_summary = summary['layer_summary'][layer_key]
            logger.info(f"\nğŸ” {layer_key.upper()}:")
            logger.info(f"   Silhouette Best k: {layer_summary['best_k_silhouette']} (score: {layer_summary['best_silhouette_score']:.4f})")
            logger.info(f"   Elbow Best k: {layer_summary['best_k_elbow']} (method: {layer_summary['best_elbow_method']})")
            logger.info(f"   Calinski-Harabasz Best k: {layer_summary['best_k_calinski_harabasz']}")
            logger.info(f"   Davies-Bouldin Best k: {layer_summary['best_k_davies_bouldin']}")
        
        cross_analysis = summary['cross_layer_analysis']
        logger.info(f"\nğŸ”„ CROSS-LAYER ANALYSIS:")
        logger.info(f"   Silhouette Consistency: {'âœ…' if cross_analysis['silhouette_consistency'] else 'âŒ'}")
        logger.info(f"   Elbow Consistency: {'âœ…' if cross_analysis['elbow_consistency'] else 'âŒ'}")
        logger.info(f"   Most Common k (Silhouette): {cross_analysis['most_common_k_silhouette']}")
        logger.info(f"   Most Common k (Elbow): {cross_analysis['most_common_k_elbow']}")
        
        return summary

    def _create_summary_visualizations_with_elbow(self, all_results, summary, output_dir):
        """ìš”ì•½ ì‹œê°í™” ìƒì„± - ì²« ë²ˆì§¸ ê·¸ë¦¼ê³¼ ì™„ì „íˆ ë™ì¼í•œ ìŠ¤íƒ€ì¼ë¡œ"""
        layers = list(range(len(all_results)))
        
        # 2x3 ë ˆì´ì•„ì›ƒ
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. ë©”íŠ¸ë¦­ë³„ ìµœì  k ë¹„êµ (ë°” ì°¨íŠ¸ - ì²« ë²ˆì§¸ ê·¸ë¦¼ê³¼ ë™ì¼)
        ax = axes[0, 0]
        silhouette_ks = [summary['layer_summary'][f'layer_{i}']['best_k_silhouette'] for i in layers]
        ch_ks = [summary['layer_summary'][f'layer_{i}']['best_k_calinski_harabasz'] for i in layers]
        db_ks = [summary['layer_summary'][f'layer_{i}']['best_k_davies_bouldin'] for i in layers]
        elbow_ks = [summary['layer_summary'][f'layer_{i}']['best_k_elbow'] for i in layers]
        
        x = np.arange(len(layers))
        width = 0.2
        
        ax.bar(x - 1.5*width, silhouette_ks, width, label='Silhouette', alpha=0.8, color='tab:blue')
        ax.bar(x - 0.5*width, ch_ks, width, label='Calinski-Harabasz', alpha=0.8, color='tab:orange')
        ax.bar(x + 0.5*width, db_ks, width, label='Davies-Bouldin', alpha=0.8, color='tab:green')
        ax.bar(x + 1.5*width, elbow_ks, width, label='Elbow', alpha=0.8, color='tab:red')
        
        ax.set_xlabel('Layer')
        ax.set_ylabel('Optimal k')
        ax.set_title('Optimal k by Different Metrics (Including Elbow)')
        ax.set_xticks(x)
        ax.set_xticklabels([f'Layer {layer}' for layer in layers])
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. Silhouette vs Elbow ìƒê´€ê´€ê³„
        ax = axes[0, 1]
        ax.scatter(silhouette_ks, elbow_ks, s=100, alpha=0.7, c=layers, cmap='viridis')
        for i, layer in enumerate(layers):
            ax.annotate(f'L{layer}', (silhouette_ks[i], elbow_ks[i]), xytext=(5, 5), 
                    textcoords='offset points', fontsize=10)
        
        ax.set_xlabel('Silhouette Optimal k')
        ax.set_ylabel('Elbow Optimal k')
        ax.set_title('Optimal k Correlation: Silhouette vs Elbow')
        ax.grid(True, alpha=0.3)
        
        # ëŒ€ê°ì„  ì°¸ì¡°ì„ 
        min_k = min(min(silhouette_ks), min(elbow_ks))
        max_k = max(max(silhouette_ks), max(elbow_ks))
        ax.plot([min_k, max_k], [min_k, max_k], 'r--', alpha=0.5, label='Perfect Agreement')
        ax.legend()
        
        # 3. Elbow Method Confidence by Layer
        ax = axes[0, 2]
        elbow_confidences = []
        elbow_methods = []
        for i in layers:
            layer_key = f'layer_{i}'
            layer_results = all_results[layer_key]
            knee_conf = layer_results['elbow_analysis']['knee_method']['confidence']
            deriv_conf = layer_results['elbow_analysis']['derivative_method']['confidence']
            var_conf = layer_results['elbow_analysis']['variance_method']['confidence']
            
            best_conf = max(knee_conf, deriv_conf, var_conf)
            elbow_confidences.append(best_conf)
            
            if best_conf == knee_conf:
                method = 'knee'
            elif best_conf == deriv_conf:
                method = 'derivative'
            else:
                method = 'variance'
            elbow_methods.append(method)
        
        bars = ax.bar(layers, elbow_confidences, alpha=0.7, color='red')
        for i, (layer, conf, method, k_val) in enumerate(zip(layers, elbow_confidences, elbow_methods, elbow_ks)):
            ax.text(bars[i].get_x() + bars[i].get_width()/2, bars[i].get_height() + max(elbow_confidences)*0.02,
                f'{method}\nk={k_val}', ha='center', va='bottom', fontsize=9, fontweight='bold')
        
        ax.set_xlabel('Layer')
        ax.set_ylabel('Elbow Confidence Score')
        ax.set_title('Elbow Method Confidence by Layer')
        ax.grid(True, alpha=0.3)
        
        # 4. Method Agreement Analysis
        ax = axes[1, 0]
        
        # ê° ë ˆì´ì–´ë³„ ë©”íŠ¸ë¦­ ì¼ì¹˜ë„ ê³„ì‚°
        agreement_scores = []
        for layer in layers:
            layer_ks = [silhouette_ks[layer], ch_ks[layer], db_ks[layer], elbow_ks[layer]]
            unique_ks = len(set(layer_ks))
            agreement_score = (4 - unique_ks + 1) / 4  # 1.0 = perfect agreement, 0.25 = no agreement
            agreement_scores.append(agreement_score)
        
        bars = ax.bar(layers, agreement_scores, alpha=0.7, color='green')
        for i, (layer, score) in enumerate(zip(layers, agreement_scores)):
            ax.text(bars[i].get_x() + bars[i].get_width()/2, bars[i].get_height() + 0.02,
                f'{score:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        ax.set_xlabel('Layer')
        ax.set_ylabel('Method Agreement Score')
        ax.set_title('Cross-Method Agreement by Layer')
        ax.set_ylim(0, 1.1)
        ax.grid(True, alpha=0.3)
        
        # 5. Silhouette Quality vs Elbow Recommendation
        ax = axes[1, 1]
        silhouette_scores = [summary['layer_summary'][f'layer_{i}']['best_silhouette_score'] for i in layers]
        
        # ìƒ‰ìƒì€ elbow confidenceë¡œ
        scatter = ax.scatter(silhouette_scores, elbow_ks, s=100, c=elbow_confidences, 
                            cmap='Reds', alpha=0.7, edgecolors='black')
        
        for i, layer in enumerate(layers):
            ax.annotate(f'L{layer}', (silhouette_scores[i], elbow_ks[i]), xytext=(5, 5), 
                    textcoords='offset points', fontsize=10)
        
        ax.set_xlabel('Best Silhouette Score')
        ax.set_ylabel('Elbow Optimal k')
        ax.set_title('Silhouette Quality vs Elbow Recommendation')
        ax.grid(True, alpha=0.3)
        
        # ì»¬ëŸ¬ë°”
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('Elbow Confidence')
        
        # 6. ì¢…í•© ìš”ì•½ (Elbow í¬í•¨)
        ax = axes[1, 2]
        ax.axis('off')
        
        summary_text = "Comprehensive Analysis Summary\n(Including Elbow Method)\n\n"
        summary_text += f"Layers analyzed: {len(layers)}\n"
        summary_text += f"Metrics compared: 4 (Sil, CH, DB, Elbow)\n\n"
        
        # ì „ì²´ ë©”íŠ¸ë¦­ ì¼ì¹˜ë„ ë¶„ì„
        all_ks = silhouette_ks + ch_ks + db_ks + elbow_ks
        complete_agreement = sum(1 for i in range(len(layers)) 
                            if len(set([silhouette_ks[i], ch_ks[i], db_ks[i], elbow_ks[i]])) == 1)
        
        summary_text += f"Complete agreement: {complete_agreement}/{len(layers)} layers\n"
        summary_text += f"Avg agreement score: {np.mean(agreement_scores):.2f}\n\n"
        
        # Elbow vs Silhouette ì¼ì¹˜ë„
        elbow_sil_agreement = sum(1 for i in range(len(layers)) if silhouette_ks[i] == elbow_ks[i])
        summary_text += f"Elbow-Silhouette agreement: {elbow_sil_agreement}/{len(layers)}\n\n"
        
        # ìµœê³  ì„±ëŠ¥ ë ˆì´ì–´ë“¤
        best_sil_layer = layers[np.argmax(silhouette_scores)]
        best_elbow_conf_layer = layers[np.argmax(elbow_confidences)]
        best_agreement_layer = layers[np.argmax(agreement_scores)]
        
        summary_text += f"Best Silhouette: Layer {best_sil_layer}\n"
        summary_text += f"Most confident Elbow: Layer {best_elbow_conf_layer}\n"
        summary_text += f"Best agreement: Layer {best_agreement_layer}\n\n"
        
        # ìµœì¢… ì¶”ì²œ
        most_common_k = max(set(all_ks), key=all_ks.count)
        summary_text += f"ğŸ¯ FINAL RECOMMENDATIONS:\n"
        summary_text += f"Most frequent k: {most_common_k}\n"
        
        if elbow_sil_agreement >= len(layers) // 2:
            summary_text += f"âœ… Elbow & Silhouette agree\n"
            summary_text += f"Recommended: Use Elbow method"
        else:
            summary_text += f"âš ï¸ Mixed results across methods\n"
            summary_text += f"Recommended: Layer {best_agreement_layer}"
        
        ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightcyan", alpha=0.8))
        
        plt.suptitle('Cross-Layer Comprehensive Analysis (with Elbow Method)', 
                    fontsize=16, y=0.98)
        plt.tight_layout()
        plt.savefig(output_dir / 'comprehensive_summary_with_elbow.png', dpi=300, bbox_inches='tight')
        plt.close()


def extract_config_from_attention_path(attention_map_dir):
    """Attention map ë””ë ‰í† ë¦¬ ê²½ë¡œì—ì„œ ì„¤ì • ì •ë³´ ì¶”ì¶œ"""
    path_parts = Path(attention_map_dir).parts
    
    # /attention_map/gpt2_mean/heart/Full/Embed-carte_desc_Edge-False_A-att í˜•íƒœì—ì„œ
    # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ config
    config_str = path_parts[-1]
    
    return config_str

def main():
    parser = argparse.ArgumentParser(description='Comprehensive Clustering Metrics Analysis with Elbow Method')
    parser.add_argument('--attention_map_dir', type=str, required=True,
                       help='Directory containing saved attention maps')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='Output directory for analysis results (default: same as attention_map_dir + "_analysis")')
    parser.add_argument('--k_min', type=int, default=2,
                       help='Minimum number of clusters to test (default: 2)')
    parser.add_argument('--k_max', type=int, default=100,
                       help='Maximum number of clusters to test (default: 15)')
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ê²€ì¦
    attention_map_dir = Path(args.attention_map_dir)
    if not attention_map_dir.exists():
        logger.error(f"Attention map directory not found: {attention_map_dir}")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir is None:
        # attention_map ê²½ë¡œë¥¼ visualization ê²½ë¡œë¡œ ë³€í™˜
        config_str = extract_config_from_attention_path(attention_map_dir)
        
        # /experiments/attention_map/gpt2_mean/heart/Full/Embed-carte_desc_Edge-False_A-att
        # â†’ /experiments/visualization/gpt2_mean/heart/Full/Embed-carte_desc_Edge-False_A-att_analysis
        output_dir = Path(str(attention_map_dir).replace('/attention_map/', '/visualization/') + '_analysis')
    else:
        output_dir = Path(args.output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ì„¤ì • ì •ë³´ ì¶”ì¶œ ë° ë¡œê¹…
    config_str = extract_config_from_attention_path(attention_map_dir)
    
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ” COMPREHENSIVE CLUSTERING ANALYSIS WITH ELBOW METHOD")
    logger.info(f"{'='*80}")
    logger.info(f"ğŸ“ Input: {attention_map_dir}")
    logger.info(f"ğŸ“Š Config: {config_str}")
    logger.info(f"ğŸ’¾ Output: {output_dir}")
    logger.info(f"ğŸ¯ Cluster range: {args.k_min} to {args.k_max}")
    logger.info(f"{'='*80}")
    
    try:
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = SavedAttentionMapAnalyzer(attention_map_dir)
        
        # ëª¨ë“  ë ˆì´ì–´ì— ëŒ€í•´ ì¢…í•© ë¶„ì„ ìˆ˜í–‰
        all_results = analyzer.analyze_all_layers_with_elbow(
            k_range=(args.k_min, args.k_max),
            output_dir=output_dir
        )
        
        logger.info(f"\nğŸ‰ Analysis completed successfully!")
        logger.info(f"ğŸ“Š Results saved to: {output_dir}")
        
        # ìµœì¢… ìš”ì•½ ì¶œë ¥
        logger.info(f"\nğŸ“‹ FINAL SUMMARY:")
        for layer_key, layer_results in all_results.items():
            best_k_elbow = layer_results['best_k_elbow']
            best_k_silhouette = layer_results['best_k_silhouette']
            best_silhouette_score = layer_results['best_silhouette_score']
            best_elbow_method = layer_results['best_elbow_method']
            
            logger.info(f"   {layer_key}: Elbow k={best_k_elbow} ({best_elbow_method}), Silhouette k={best_k_silhouette} (score={best_silhouette_score:.4f})")
        
    except Exception as e:
        logger.error(f"âŒ Analysis failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())