"""
Complete Clustering Pipeline: Train clustering + Valid/Test mapping
"""

import os
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'

import torch
import argparse
import numpy as np
import pandas as pd
import logging
from pathlib import Path
import json
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances
import pickle

current_dir = Path(__file__).resolve().parent
import sys
# analysis/ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ (ì¦‰, ProtoLLM/)
root_dir = current_dir.parent
sys.path.append(str(root_dir))  # models, dataset, utils ë“±ì´ ìœ„ì¹˜í•œ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì¶”ê°€

from models.TabularFLM import Model
from dataset.data_dataloaders import prepare_embedding_dataloaders
from utils.util import setup_logger, fix_seed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ClusteringInference:
    def __init__(self, checkpoint_dir, device='cuda'):
        """
        Args:
            checkpoint_dir (str): ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            device (str): 'cuda' ë˜ëŠ” 'cpu'
        """
        self.checkpoint_dir = checkpoint_dir
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        self.checkpoint = torch.load(checkpoint_dir, map_location=self.device)
        self.args = self.checkpoint['args']
        
        logger.info(f"Loaded checkpoint from {checkpoint_dir}")
        logger.info(f"Checkpoint epoch: {self.checkpoint['epoch']}, Val AUC: {self.checkpoint['val_auc']:.4f}")
        
        # ëª¨ë¸ ì´ˆê¸°í™” ë° ë¡œë“œ
        self._load_model()
        
        # ë°ì´í„°ë¡œë” ì¤€ë¹„
        self._prepare_dataloaders()
        
        # ì›ë³¸ tabular ë°ì´í„° ë¡œë“œ
        self._load_original_tabular_data()
        
    def _load_model(self):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        experiment_id = "inference"
        mode = "inference"
        
        self.model = Model(
            self.args, 
            self.args.input_dim, 
            self.args.hidden_dim, 
            self.args.output_dim, 
            self.args.num_layers, 
            self.args.dropout_rate, 
            self.args.llm_model,
            experiment_id,
            mode
        ).to(self.device)
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model.load_state_dict(self.checkpoint['model_state_dict'])
        self.model.eval()
        
        logger.info("Model loaded and set to evaluation mode")
    
    def _prepare_dataloaders(self):
        """ë°ì´í„°ë¡œë” ì¤€ë¹„"""
        fix_seed(self.args.random_seed)
        
        # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë” ì¤€ë¹„
        results = prepare_embedding_dataloaders(self.args, self.args.source_data)
        self.train_loader, self.val_loader, self.test_loader = results['loaders']
        self.num_classes = results['num_classes']
        
        logger.info(f"Dataloaders prepared for dataset: {self.args.source_data}")
        logger.info(f"Training data size: {len(self.train_loader.dataset)}")
        logger.info(f"Validation data size: {len(self.val_loader.dataset)}")
        logger.info(f"Test data size: {len(self.test_loader.dataset)}")
    
    def _load_original_tabular_data(self):
        """ì›ë³¸ tabular ë°ì´í„° ë¡œë“œ - make_embed_dataset.pyì˜ ë¡œì§ í™œìš©"""
        
        # TabularToEmbeddingDataset í´ë˜ìŠ¤ import ë° ì´ˆê¸°í™”
        from make_embed_dataset import TabularToEmbeddingDataset
        
        dataset_loader = TabularToEmbeddingDataset(self.args)
        
        # ì›ë³¸ CSV íŒŒì¼ ë¡œë“œ (TabularToEmbeddingDatasetì™€ ë™ì¼í•œ ê²½ë¡œ)
        base_path = "/storage/personal/eungyeop/dataset/table/"
        data_source =  "origin_table"
        csv_path = os.path.join(base_path, data_source, f"{self.args.source_data}.csv")
        
        if os.path.exists(csv_path):
            raw_data = pd.read_csv(csv_path)
            
            # ì „ì²˜ë¦¬ ì ìš© (ì„ë² ë”© ìƒì„± ì‹œì™€ ë™ì¼í•˜ê²Œ)
            X, y = dataset_loader.preprocessing(raw_data, self.args.source_data)
            
            # Xì™€ yë¥¼ í•©ì³ì„œ ì „ì²´ ë°ì´í„° ìƒì„±
            self.original_data = X.copy()
            self.original_data['target_binary'] = y
            
            logger.info(f"Loaded original tabular data from: {csv_path}")
            logger.info(f"Original data shape: {self.original_data.shape}")
            logger.info(f"Columns: {list(self.original_data.columns)}")
            
        else:
            raise FileNotFoundError(f"Original data not found: {csv_path}")
    
    def extract_attention_maps(self, data_loader, split_name):
        """
        ë°ì´í„°ë¡œë”ì—ì„œ attention maps ì¶”ì¶œ
        
        Args:
            data_loader: ë°ì´í„°ë¡œë”
            split_name: 'train', 'valid', 'test'
            
        Returns:
            dict: ë ˆì´ì–´ë³„ attention mapsì™€ ë©”íƒ€ë°ì´í„°
        """
        attention_data = {
            'layer_0': [],
            'layer_1': [], 
            'layer_2': [],
            'labels': [],
            'sample_ids': [],
            'feature_names': None
        }
        
        sample_count = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(data_loader):
                # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                batch_on_device = {
                    k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                    for k, v in batch.items()
                }
                
                # ëª¨ë¸ forward (attention weights ì¶”ì¶œ)
                pred, attention_weights = self._extract_attention_from_model(batch_on_device)
                
                # Feature names ì¶”ì¶œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ)
                if attention_data['feature_names'] is None:
                    feature_names = self._extract_feature_names(batch_on_device)
                    attention_data['feature_names'] = ["CLS"] + feature_names
                
                # ë°°ì¹˜ í¬ê¸° í™•ì¸
                batch_size = attention_weights[0].shape[0]
                
                for sample_idx in range(batch_size):
                    # ê° ë ˆì´ì–´ë³„ attention map ì €ì¥
                    for layer_idx, layer_attention in enumerate(attention_weights):
                        # Multi-head attentionì„ í‰ê· ë‚´ì–´ ë‹¨ì¼ attention mapìœ¼ë¡œ ë³€í™˜
                        attention_map = layer_attention[sample_idx].mean(dim=0)  # [seq_len, seq_len]
                        attention_numpy = attention_map.detach().cpu().numpy()
                        attention_data[f'layer_{layer_idx}'].append(attention_numpy)
                    
                    # ë¼ë²¨ê³¼ ìƒ˜í”Œ ID ì €ì¥
                    if 'y' in batch:
                        label = batch['y'][sample_idx].item()
                    else:
                        label = -1
                    attention_data['labels'].append(label)
                    
                    # ìƒ˜í”Œ ID (s_idx ì‚¬ìš©)
                    if 's_idx' in batch:
                        sample_id = batch['s_idx'][sample_idx].item()
                    else:
                        sample_id = sample_count
                    attention_data['sample_ids'].append(sample_id)
                    
                    sample_count += 1
                
                if batch_idx % 10 == 0:
                    logger.info(f"Processed {sample_count} {split_name} samples...")
        
        logger.info(f"Extracted attention maps for {sample_count} {split_name} samples")
        return attention_data
    
    def _extract_attention_from_model(self, batch):
        """ëª¨ë¸ì—ì„œ attention weightsì™€ ì˜ˆì¸¡ê°’ì„ ì¶”ì¶œ"""
        label_description_embeddings = batch['label_description_embeddings'].to(self.device)
        desc_embeddings = [] 
        name_value_embeddings = [] 
        
        if all(k in batch for k in ['cat_name_value_embeddings', 'cat_desc_embeddings']):
            cat_name_value_embeddings = batch['cat_name_value_embeddings'].to(self.device).squeeze(-2)
            cat_desc_embeddings = batch['cat_desc_embeddings'].to(self.device).squeeze(-2)
            
            name_value_embeddings.append(cat_name_value_embeddings)
            desc_embeddings.append(cat_desc_embeddings)
            
        if all(k in batch for k in ['num_prompt_embeddings', 'num_desc_embeddings']):
            num_prompt_embeddings = batch['num_prompt_embeddings'].to(self.device).squeeze(-2)
            num_desc_embeddings = batch['num_desc_embeddings'].to(self.device).squeeze(-2)
            name_value_embeddings.append(num_prompt_embeddings)
            desc_embeddings.append(num_desc_embeddings)
            
        if not desc_embeddings or not name_value_embeddings:
            raise ValueError("No categorical or numerical features found in batch")

        desc_embeddings = torch.cat(desc_embeddings, dim = 1)
        name_value_embeddings = torch.cat(name_value_embeddings, dim = 1)
        
        # [CLS] Token ì¶”ê°€
        attention_weights = [] 
        cls_token = self.model.cls.expand(name_value_embeddings.size(0), -1, -1)
        name_value_embeddings = torch.cat([cls_token, name_value_embeddings], dim=1)
        x = name_value_embeddings
        
        # Graph Attention Layers
        for i, layer in enumerate(self.model.layers):
            norm_x = self.model.layer_norms[i](x)
            attn_output, attn_weights = layer(desc_embeddings, norm_x)
            attention_weights.append(attn_weights)
            x = x + attn_output
        
        # ì˜ˆì¸¡ê°’ ê³„ì‚°
        pred = x[:, 0, :]
        pred = self.model.predictor(pred)

        return pred, attention_weights
    
    def _extract_feature_names(self, batch):
        """Feature names ì¶”ì¶œ (ì„ì‹œ êµ¬í˜„)"""
        feature_count = 0
        
        if 'cat_name_value_embeddings' in batch:
            feature_count += batch['cat_name_value_embeddings'].shape[1]
        if 'num_prompt_embeddings' in batch:
            feature_count += batch['num_prompt_embeddings'].shape[1]
        
        return [f"feature_{i}" for i in range(feature_count)]
    
    def perform_train_clustering(self, layer_idx=2, n_clusters=8, output_dir=None):
        """
        Train setì˜ attention mapsì— ëŒ€í•´ K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            layer_idx: í´ëŸ¬ìŠ¤í„°ë§í•  ë ˆì´ì–´ ì¸ë±ìŠ¤
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            
        Returns:
            dict: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # Train attention maps ì¶”ì¶œ
        attention_data = self.extract_attention_maps(self.train_loader, 'train')
        
        # íŠ¹ì • ë ˆì´ì–´ì˜ attention maps ì‚¬ìš©
        attention_maps = np.stack(attention_data[f'layer_{layer_idx}'])
        labels = np.array(attention_data['labels'])
        sample_ids = np.array(attention_data['sample_ids'])
        
        logger.info(f"Performing clustering on layer {layer_idx} with {len(attention_maps)} train samples")
        
        # í‰íƒ„í™” (ë²¡í„°í™”)
        flattened_maps = attention_maps.reshape(len(attention_maps), -1)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        #kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)
        #cluster_assignments = kmeans.fit_predict(flattened_maps)
        checkpoint_file = Path(self.checkpoint_dir)
        config_folder = extract_checkpoint_config_for_folder(self.checkpoint_dir)

        # clustering1.pyì—ì„œ ì €ì¥ëœ KMeans ëª¨ë¸ ê²½ë¡œ êµ¬ì„±
        checkpoint_parent_str = str(checkpoint_file.parent)
        if '/checkpoints/' in checkpoint_parent_str:
            clustering_parent_str = checkpoint_parent_str.replace('/checkpoints/', '/clustering/')
        else:
            clustering_parent_str = '/storage/personal/eungyeop/experiments/clustering/gpt2_mean/heart/Full'

        kmeans_model_path = (Path(clustering_parent_str) / config_folder / 
                            f'clustering1_{n_clusters}/clustering_results/layer_{layer_idx}' / 
                            f'layer_{layer_idx}_kmeans_model.pkl')

        if kmeans_model_path.exists():

            # ê¸°ì¡´ KMeans ëª¨ë¸ ë¡œë“œ
            with open(kmeans_model_path, 'rb') as f:
                kmeans = pickle.load(f)
            logger.info(f"ğŸ”¥ Loaded existing KMeans model from: {kmeans_model_path}")
            
            # Train ë°ì´í„°ë¥¼ ê¸°ì¡´ ëª¨ë¸ë¡œ í´ëŸ¬ìŠ¤í„° í• ë‹¹ (fití•˜ì§€ ì•Šê³  predictë§Œ)
            cluster_assignments = kmeans.predict(flattened_maps)
            
        else:
            pdb.set_trace()
            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (fallback)
            logger.warning(f"KMeans model not found at: {kmeans_model_path}")
            logger.warning("Creating new KMeans model...")
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)
            cluster_assignments = kmeans.fit_predict(flattened_maps)
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì¶œë ¥
        unique_labels = np.unique(labels)
        for cluster_id in range(n_clusters):
            cluster_mask = cluster_assignments == cluster_id
            cluster_samples = np.sum(cluster_mask)
            
            label_dist = {}
            for label in unique_labels:
                count = np.sum((cluster_assignments == cluster_id) & (labels == label))
                label_dist[f'label_{label}'] = count
            
            logger.info(f"Cluster {cluster_id}: {cluster_samples} samples, distribution: {label_dist}")
        
        clustering_results = {
            'cluster_assignments': cluster_assignments,
            'cluster_centers': kmeans.cluster_centers_,
            'labels': labels,
            'sample_ids': sample_ids,
            'layer_idx': layer_idx,
            'kmeans_model': kmeans
        }
        
        # CSV ìƒì„±
        if output_dir:
            self.save_cluster_csvs(clustering_results, output_dir, n_clusters, 'train')
        
        return clustering_results
    
    def map_to_clusters_by_distance(self, attention_data, train_centroids, layer_idx):
        """
        Train centroidì™€ì˜ ê±°ë¦¬ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° í• ë‹¹
        
        Args:
            attention_data: extract_attention_maps ê²°ê³¼
            train_centroids: Trainìœ¼ë¡œ í•™ìŠµëœ í´ëŸ¬ìŠ¤í„° centroids
            layer_idx: ì‚¬ìš©í•  ë ˆì´ì–´ ì¸ë±ìŠ¤
            
        Returns:
            dict: í´ëŸ¬ìŠ¤í„° í• ë‹¹ ê²°ê³¼
        """
        attention_maps = np.stack(attention_data[f'layer_{layer_idx}'])
        labels = np.array(attention_data['labels'])
        sample_ids = np.array(attention_data['sample_ids'])
        
        # í‰íƒ„í™”
        flattened_maps = attention_maps.reshape(len(attention_maps), -1)
        
        # ê° ìƒ˜í”Œê³¼ train centroids ê°„ì˜ ê±°ë¦¬ ê³„ì‚°
        distances = pairwise_distances(flattened_maps, train_centroids, metric='euclidean')
        
        # ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„° í• ë‹¹
        cluster_assignments = np.argmin(distances, axis=1)
        
        return {
            'cluster_assignments': cluster_assignments,
            'labels': labels,
            'sample_ids': sample_ids,
            'distances': distances
        }
    
    def save_cluster_csvs(self, clustering_results, output_dir, n_clusters, split_name):
        """
        í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì›ë³¸ tabular ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ CSV ì €ì¥
        
        Args:
            clustering_results: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
            output_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜
            split_name: 'train', 'valid', 'test'
        """
        cluster_assignments = clustering_results['cluster_assignments']
        sample_ids = clustering_results['sample_ids']
        labels = clustering_results['labels']
        
        # {split}_clustering_{n_clusters} í´ë” ìƒì„±
        split_dir = output_dir / f'{split_name}_clustering_{n_clusters}'
        split_dir.mkdir(parents=True, exist_ok=True)
        
        for cluster_id in range(n_clusters):
            # ê° í´ëŸ¬ìŠ¤í„°ë³„ í´ë” ìƒì„±
            cluster_dir = split_dir / f'cluster_{cluster_id}'
            cluster_dir.mkdir(parents=True, exist_ok=True)
            
            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ìƒ˜í”Œë“¤ì˜ ì¸ë±ìŠ¤
            cluster_mask = cluster_assignments == cluster_id
            cluster_sample_ids = sample_ids[cluster_mask]
            cluster_labels = labels[cluster_mask]
            
            if len(cluster_sample_ids) > 0:
                # ì›ë³¸ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ìƒ˜í”Œë“¤ ì¶”ì¶œ
                cluster_data = self.original_data.iloc[cluster_sample_ids].copy()
                
                # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ê°€
                cluster_data['cluster_id'] = cluster_id
                cluster_data['original_index'] = cluster_sample_ids
                
                # CSV ì €ì¥ (í´ëŸ¬ìŠ¤í„° í´ë” ì•ˆì—)
                csv_filename = f'cluster_{cluster_id}_{split_name}.csv'
                csv_path = cluster_dir / csv_filename
                cluster_data.to_csv(csv_path, index=False)
                
                logger.info(f"Saved {split_name} cluster {cluster_id}: {len(cluster_data)} samples to {csv_path}")
                
            else:
                logger.warning(f"{split_name} cluster {cluster_id} has no samples")
        
        # ì „ì²´ ìš”ì•½ ì •ë³´ ì €ì¥
        summary = {
            'total_samples': len(sample_ids),
            'n_clusters': n_clusters,
            'split': split_name,
            'cluster_summary': {}
        }
        
        for cluster_id in range(n_clusters):
            cluster_mask = cluster_assignments == cluster_id
            cluster_count = np.sum(cluster_mask)
            cluster_labels = labels[cluster_mask]
            
            summary['cluster_summary'][f'cluster_{cluster_id}'] = {
                'sample_count': int(cluster_count),
                'label_distribution': {int(k): int(v) for k, v in zip(*np.unique(cluster_labels, return_counts=True))}
            }
        
        summary_path = split_dir / 'clustering_summary.json'
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        logger.info(f"{split_name} clustering summary saved to: {summary_path}")
        logger.info(f"âœ… All {split_name} cluster CSVs saved in: {split_dir}")

    def save_whole_test_set(self, test_attention_data, output_dir):
        """
        Test setì„ ì „ì²´ë¡œ ì €ì¥ (í´ëŸ¬ìŠ¤í„° ë¶„í•  ì—†ìŒ)
        
        Args:
            test_attention_data: extract_attention_maps ê²°ê³¼
            output_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        test_sample_ids = np.array(test_attention_data['sample_ids'])
        test_labels = np.array(test_attention_data['labels'])
        
        # Test ë°ì´í„° ì¶”ì¶œ
        test_data = self.original_data.iloc[test_sample_ids].copy()
        test_data['original_index'] = test_sample_ids
        
        # Test í´ë” ìƒì„± ë° ì €ì¥
        test_dir = output_dir / 'test_full'
        test_dir.mkdir(parents=True, exist_ok=True)
        
        test_csv_path = test_dir / 'test_full.csv'
        test_data.to_csv(test_csv_path, index=False)
        
        logger.info(f"Saved full test set: {len(test_data)} samples to {test_csv_path}")
        
        # Test set ìš”ì•½ ì •ë³´ ì €ì¥
        test_summary = {
            'total_samples': len(test_sample_ids),
            'label_distribution': {int(k): int(v) for k, v in zip(*np.unique(test_labels, return_counts=True))}
        }
        
        test_summary_path = test_dir / 'test_summary.json'
        with open(test_summary_path, 'w') as f:
            json.dump(test_summary, f, indent=2)
        
        logger.info(f"Test summary saved to: {test_summary_path}")

    def save_full_population_sets(self, train_results, valid_results, output_dir):
        """
        ì „ì²´ populationìš© train/valid set ì €ì¥ (í´ëŸ¬ìŠ¤í„° êµ¬ë¶„ ì—†ìŒ)
        
        Args:
            train_results: Train í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
            valid_results: Valid í´ëŸ¬ìŠ¤í„° í• ë‹¹ ê²°ê³¼
            output_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        # Full population í´ë” ìƒì„±
        full_pop_dir = output_dir / 'full_population'
        full_pop_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. Full train set ì €ì¥
        train_sample_ids = train_results['sample_ids']
        train_data = self.original_data.iloc[train_sample_ids].copy()
        train_data['original_index'] = train_sample_ids
        
        train_csv_path = full_pop_dir / 'train_full.csv'
        train_data.to_csv(train_csv_path, index=False)
        logger.info(f"Saved full train set: {len(train_data)} samples to {train_csv_path}")
        
        # 2. Full valid set ì €ì¥
        valid_sample_ids = valid_results['sample_ids']
        valid_data = self.original_data.iloc[valid_sample_ids].copy()
        valid_data['original_index'] = valid_sample_ids
        
        valid_csv_path = full_pop_dir / 'valid_full.csv'
        valid_data.to_csv(valid_csv_path, index=False)
        logger.info(f"Saved full valid set: {len(valid_data)} samples to {valid_csv_path}")
        
        # 3. ìš”ì•½ ì •ë³´ ì €ì¥
        full_pop_summary = {
            'train_samples': len(train_sample_ids),
            'valid_samples': len(valid_sample_ids),
            'train_label_distribution': {int(k): int(v) for k, v in zip(*np.unique(train_results['labels'], return_counts=True))},
            'valid_label_distribution': {int(k): int(v) for k, v in zip(*np.unique(valid_results['labels'], return_counts=True))}
        }
        
        summary_path = full_pop_dir / 'full_population_summary.json'
        with open(summary_path, 'w') as f:
            json.dump(full_pop_summary, f, indent=2)
        
        logger.info(f"Full population summary saved to: {summary_path}")
        logger.info(f"âœ… Full population train/valid sets saved in: {full_pop_dir}")

    def run_complete_pipeline(self, layer_idx=2, n_clusters=8, output_dir=None):
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: Train clustering + Valid/Test mapping
        
        Args:
            layer_idx: í´ëŸ¬ìŠ¤í„°ë§í•  ë ˆì´ì–´ ì¸ë±ìŠ¤
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. Train í´ëŸ¬ìŠ¤í„°ë§
        logger.info("=== Step 1: Train Clustering ===")
        train_results = self.perform_train_clustering(layer_idx, n_clusters, output_dir)
        
        # Train centroids ì €ì¥
        centroids_path = output_dir / 'cluster_centroids.npy'
        np.save(centroids_path, train_results['cluster_centers'])
        logger.info(f"Train centroids saved to: {centroids_path}")
        
        # 2. Valid attention maps ì¶”ì¶œ ë° í´ëŸ¬ìŠ¤í„° ë§¤í•‘
        logger.info("=== Step 2: Valid Cluster Mapping ===")
        valid_attention_data = self.extract_attention_maps(self.val_loader, 'valid')
        valid_results = self.map_to_clusters_by_distance(
            valid_attention_data, 
            train_results['cluster_centers'], 
            layer_idx
        )
        
        # Valid CSV ì €ì¥
        self.save_cluster_csvs(valid_results, output_dir, n_clusters, 'valid')
        
        # 3. Test attention maps ì¶”ì¶œ ë° í´ëŸ¬ìŠ¤í„° ë§¤í•‘
        logger.info("=== Step 3: Test Cluster Mapping ===")
        test_attention_data = self.extract_attention_maps(self.test_loader, 'test')
        self.save_whole_test_set(test_attention_data, output_dir)
        logger.info("=== Step 4: Saving Full Population Train/Valid Sets ===")
        self.save_full_population_sets(train_results, valid_results, output_dir)
        
        logger.info("âœ… Complete clustering pipeline finished!")
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("\n=== CLUSTERING SUMMARY ===")
        logger.info(f"Train samples: {len(train_results['sample_ids'])}")
        logger.info(f"Valid samples: {len(valid_results['sample_ids'])}")
        logger.info(f"Test samples: {len(test_attention_data['sample_ids'])}")

def extract_checkpoint_config_for_folder(checkpoint_path):
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…ì—ì„œ ì„¤ì • ì •ë³´ë¥¼ ì¶”ì¶œí•´ì„œ í´ë”ëª…ìœ¼ë¡œ ë³€í™˜"""
    filename = Path(checkpoint_path).stem
    
    # ë‚ ì§œ/ì‹œê°„ íŒ¨í„´ ì œê±° (20250617_173832 í˜•íƒœ)
    import re
    filename_clean = re.sub(r'_\d{8}_\d{6}', '', filename)
    
    # "Embed:carte_desc_Edge:mlp_A:att" í˜•íƒœë¥¼ íŒŒì‹±
    pattern = r'Embed:([^:_]+(?:_[^:_]+)*?)_Edge:([^:_]+)_A:([^:_]+)'
    match = re.match(pattern, filename_clean)
    
    if match:
        embed_type = match.group(1)  # carte, carte_desc, ours, ours2
        edge_attr = match.group(2)   # mlp, no_use, normal
        attn_type = match.group(3)   # att, gat
        
        # í´ë”ëª… ìƒì„±: Embed-carte_desc_Edge-mlp_A-att
        folder_name = f"Embed-{embed_type}_Edge-{edge_attr}_A-{attn_type}"
        return folder_name
    else:
        # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©í•˜ë˜ ì½œë¡ ì„ ëŒ€ì‹œë¡œ ë³€ê²½
        logger.warning(f"Could not parse config from filename: {filename_clean}")
        return filename_clean.replace(':', '-')


def main():
    parser = argparse.ArgumentParser(description='Complete Clustering Pipeline')
    parser.add_argument('--checkpoint_dir', type=str, required=True,
                       help='Path to model checkpoint')
    parser.add_argument('--layer_idx', type=int, default=2,
                       help='Layer index for clustering (default: 2)')
    parser.add_argument('--n_clusters', type=int, default=8,
                       help='Number of clusters for K-means')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='Output directory for results')
    parser.add_argument('--train_only', action='store_true',
                       help='Only perform train clustering (skip valid/test)')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •: clustering í´ë”ë¡œ ë³€ê²½
    if args.output_dir is None:
        checkpoint_file = Path(args.checkpoint_dir)
        config_folder = extract_checkpoint_config_for_folder(args.checkpoint_dir)
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ê°€ì ¸ì™€ì„œ ë³€í™˜
        checkpoint_parent_str = str(checkpoint_file.parent)
        
        # checkpointsë¥¼ clusteringìœ¼ë¡œ ë³€ê²½
        if '/checkpoints/' in checkpoint_parent_str:
            clustering_parent_str = checkpoint_parent_str.replace('/checkpoints/', '/clustering/')
        else:
            # fallback: ì§ì ‘ ê²½ë¡œ êµ¬ì„±
            clustering_parent_str = '/storage/personal/eungyeop/experiments/clustering/gpt2_mean/heart/Full'
        
        # ìµœì¢… ì¶œë ¥ ê²½ë¡œ: .../clustering/.../config_folder/clustering_{n_clusters}/
        args.output_dir = Path(clustering_parent_str) / config_folder / f'clustering_{args.n_clusters}'
    
    logger.info(f"ğŸ“ Results will be saved to: {args.output_dir}")

    # Pipeline ì‹¤í–‰
    pipeline = ClusteringInference(args.checkpoint_dir)
    
    if args.train_only:
        # Trainë§Œ ìˆ˜í–‰
        logger.info("Performing train-only clustering...")
        train_results = pipeline.perform_train_clustering(
            layer_idx=args.layer_idx,
            n_clusters=args.n_clusters,
            output_dir=args.output_dir
        )
        
        # Train centroids ì €ì¥
        centroids_path = args.output_dir / 'cluster_centroids.npy'
        np.save(centroids_path, train_results['cluster_centers'])
        logger.info(f"Train centroids saved to: {centroids_path}")
        
        logger.info(f"Train clustering completed! Results saved to {args.output_dir}")
    else:
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline.run_complete_pipeline(
            layer_idx=args.layer_idx,
            n_clusters=args.n_clusters,
            output_dir=args.output_dir
        )
        
        logger.info(f"Complete clustering pipeline completed! Results saved to {args.output_dir}")

if __name__ == "__main__":
    main()