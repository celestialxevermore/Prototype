"""
Complete Cosine Similarity Matrix Analyzer: ëª¨ë“  í´ëŸ¬ìŠ¤í„° ìŒì˜ cosine similarity ë§¤íŠ¸ë¦­ìŠ¤ ë¶„ì„
"""

import os
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'

import torch
import argparse
import numpy as np
import pandas as pd
import logging
from pathlib import Path
import json
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

current_dir = Path(__file__).resolve().parent
import sys
# analysis/ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ (ì¦‰, ProtoLLM/)
root_dir = current_dir.parent
sys.path.append(str(root_dir))

from models.TabularFLM import Model
from dataset.data_dataloaders import prepare_embedding_dataloaders
from utils.util import setup_logger, fix_seed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CompleteCosineSimilarityAnalyzer:
    def __init__(self, checkpoint_dir, clustering_dir, device='cuda'):
        """
        Args:
            checkpoint_dir (str): ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            clustering_dir (str): í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë””ë ‰í† ë¦¬
            device (str): 'cuda' ë˜ëŠ” 'cpu'
        """
        self.checkpoint_dir = checkpoint_dir
        self.clustering_dir = Path(clustering_dir)
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        self.checkpoint = torch.load(checkpoint_dir, map_location=self.device)
        self.args = self.checkpoint['args']
        
        logger.info(f"Loaded checkpoint from {checkpoint_dir}")
        logger.info(f"Clustering directory: {clustering_dir}")
        
        # ëª¨ë¸ ì´ˆê¸°í™” ë° ë¡œë“œ
        self._load_model()
        
        # ë°ì´í„°ë¡œë” ì¤€ë¹„
        self._prepare_dataloaders()
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¡œë“œ
        self._load_clustering_results()
        
        # ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì˜ embedding ì¶”ì¶œ
        self._extract_all_cluster_embeddings()
        
    def _load_model(self):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        experiment_id = "inference"
        mode = "inference"
        
        self.model = Model(
            self.args, 
            self.args.input_dim, 
            self.args.hidden_dim, 
            self.args.output_dim, 
            self.args.num_layers, 
            self.args.dropout_rate, 
            self.args.llm_model,
            experiment_id,
            mode
        ).to(self.device)
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model.load_state_dict(self.checkpoint['model_state_dict'])
        self.model.eval()
        
        logger.info("Model loaded and set to evaluation mode")
    
    def _prepare_dataloaders(self):
        """ë°ì´í„°ë¡œë” ì¤€ë¹„"""
        fix_seed(self.args.random_seed)
        
        # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë” ì¤€ë¹„
        results = prepare_embedding_dataloaders(self.args, self.args.source_data)
        self.train_loader, self.val_loader, self.test_loader = results['loaders']
        self.num_classes = results['num_classes']
        
        logger.info(f"Dataloaders prepared for dataset: {self.args.source_data}")
    
    def _load_clustering_results(self):
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¡œë“œ"""
        # train_label_clustering_{n_clusters} í´ë”ì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´ ë¡œë“œ
        train_dirs = list(self.clustering_dir.glob('train_label_clustering_*'))
        if not train_dirs:
            raise FileNotFoundError(f"No train clustering directories found in {self.clustering_dir}")
        
        train_dir = train_dirs[0]  # ì²« ë²ˆì§¸ ë°œê²¬ëœ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        
        # ìš”ì•½ ì •ë³´ ë¡œë“œ
        summary_path = train_dir / 'label_clustering_summary.json'
        if not summary_path.exists():
            raise FileNotFoundError(f"Clustering summary not found: {summary_path}")
        
        with open(summary_path, 'r') as f:
            self.clustering_summary = json.load(f)
        
        logger.info(f"Loaded clustering summary from: {summary_path}")
        
        # ë¼ë²¨ë³„ í´ëŸ¬ìŠ¤í„° ì •ë³´ íŒŒì‹±
        self.cluster_info = {}
        for label_key, label_data in self.clustering_summary['label_cluster_summary'].items():
            label_num = int(label_key.split('_')[1])
            self.cluster_info[label_num] = {
                'n_clusters': label_data['n_clusters'],
                'total_samples': label_data['total_samples'],
                'clusters': {}
            }
            
            # ê° í´ëŸ¬ìŠ¤í„°ë³„ ìƒ˜í”Œ ID ë¡œë“œ
            for cluster_key, cluster_data in label_data['cluster_summary'].items():
                cluster_num = int(cluster_key.split('_')[1])
                
                # CSV íŒŒì¼ì—ì„œ ì‹¤ì œ ìƒ˜í”Œ IDë“¤ ë¡œë“œ
                csv_path = train_dir / f'label_{label_num}' / f'cluster_{cluster_num}' / f'label_{label_num}_cluster_{cluster_num}_train.csv'
                if csv_path.exists():
                    cluster_df = pd.read_csv(csv_path)
                    sample_ids = cluster_df['original_index'].values
                    self.cluster_info[label_num]['clusters'][cluster_num] = {
                        'sample_ids': sample_ids,
                        'sample_count': len(sample_ids)
                    }
                    logger.info(f"Label {label_num} Cluster {cluster_num}: {len(sample_ids)} samples")
        
        logger.info("âœ… Clustering results loaded successfully")
    
    def _extract_all_cluster_embeddings(self):
        """ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì˜ embeddingë“¤ì„ ë¯¸ë¦¬ ì¶”ì¶œí•´ì„œ ì €ì¥"""
        logger.info("Extracting embeddings for all clusters...")
        
        self.cluster_embeddings = {}
        
        for label_num, label_data in self.cluster_info.items():
            self.cluster_embeddings[label_num] = {}
            
            for cluster_num, cluster_data in label_data['clusters'].items():
                sample_ids = cluster_data['sample_ids']
                
                logger.info(f"Extracting embeddings for Label {label_num} Cluster {cluster_num} ({len(sample_ids)} samples)...")
                embedding_data = self.extract_embeddings_for_samples(sample_ids, split='train')
                
                if embedding_data['embeddings'].size > 0:
                    self.cluster_embeddings[label_num][cluster_num] = embedding_data['embeddings']
                    logger.info(f"âœ… Label {label_num} Cluster {cluster_num}: {embedding_data['embeddings'].shape}")
                else:
                    logger.warning(f"âš ï¸ No embeddings found for Label {label_num} Cluster {cluster_num}")
        
        logger.info("âœ… All cluster embeddings extracted")
    
    def extract_embeddings_for_samples(self, sample_ids, split='train'):
        """
        íŠ¹ì • ìƒ˜í”Œ IDë“¤ì— ëŒ€í•œ embedding ì¶”ì¶œ
        
        Args:
            sample_ids (list): ì¶”ì¶œí•  ìƒ˜í”Œ ID ë¦¬ìŠ¤íŠ¸
            split (str): 'train', 'valid', 'test'
            
        Returns:
            dict: {'embeddings': embeddings, 'sample_ids': actual_sample_ids}
        """
        if split == 'train':
            data_loader = self.train_loader
        elif split == 'valid':
            data_loader = self.val_loader
        else:
            data_loader = self.test_loader
        
        # ëª©í‘œ ìƒ˜í”Œ IDë¥¼ setìœ¼ë¡œ ë³€í™˜ (ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´)
        target_sample_ids = set(sample_ids)
        
        extracted_embeddings = []
        extracted_sample_ids = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(data_loader):
                # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                batch_on_device = {
                    k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                    for k, v in batch.items()
                }
                
                # ë°°ì¹˜ ë‚´ ìƒ˜í”Œ í™•ì¸
                batch_sample_ids = batch.get('s_idx', None)
                if batch_sample_ids is None:
                    continue
                
                # ëª©í‘œ ìƒ˜í”Œì´ ìˆëŠ”ì§€ í™•ì¸
                batch_mask = []
                for i, sample_id in enumerate(batch_sample_ids):
                    if sample_id.item() in target_sample_ids:
                        batch_mask.append(i)
                        extracted_sample_ids.append(sample_id.item())
                
                if not batch_mask:
                    continue  # ì´ ë°°ì¹˜ì—ëŠ” ëª©í‘œ ìƒ˜í”Œì´ ì—†ìŒ
                
                # ëª¨ë¸ì—ì„œ embedding ì¶”ì¶œ (CLS token embedding ì‚¬ìš©)
                embeddings = self._extract_cls_embeddings(batch_on_device)
                
                # ëª©í‘œ ìƒ˜í”Œë“¤ì˜ embeddingë§Œ ì¶”ì¶œ
                for idx in batch_mask:
                    extracted_embeddings.append(embeddings[idx].detach().cpu().numpy())
        
        if not extracted_embeddings:
            return {'embeddings': np.array([]), 'sample_ids': []}
        
        embeddings_array = np.stack(extracted_embeddings)
        
        return {
            'embeddings': embeddings_array,
            'sample_ids': extracted_sample_ids
        }
    
    def _extract_cls_embeddings(self, batch):
        """ëª¨ë¸ì—ì„œ CLS token embedding ì¶”ì¶œ"""
        label_description_embeddings = batch['label_description_embeddings'].to(self.device)
        desc_embeddings = [] 
        name_value_embeddings = [] 
        
        if all(k in batch for k in ['cat_name_value_embeddings', 'cat_desc_embeddings']):
            cat_name_value_embeddings = batch['cat_name_value_embeddings'].to(self.device)
            cat_desc_embeddings = batch['cat_desc_embeddings'].to(self.device)
            
            name_value_embeddings.append(cat_name_value_embeddings)
            desc_embeddings.append(cat_desc_embeddings)
            
        if all(k in batch for k in ['num_prompt_embeddings', 'num_desc_embeddings']):
            num_prompt_embeddings = batch['num_prompt_embeddings'].to(self.device)
            num_desc_embeddings = batch['num_desc_embeddings'].to(self.device)
            name_value_embeddings.append(num_prompt_embeddings)
            desc_embeddings.append(num_desc_embeddings)
            
        if not desc_embeddings or not name_value_embeddings:
            raise ValueError("No categorical or numerical features found in batch")

        desc_embeddings = torch.cat(desc_embeddings, dim=1)
        name_value_embeddings = torch.cat(name_value_embeddings, dim=1)
        
        # [CLS] Token ì¶”ê°€
        cls_token = self.model.cls.expand(name_value_embeddings.size(0), -1, -1)
        name_value_embeddings = torch.cat([cls_token, name_value_embeddings], dim=1)
        x = name_value_embeddings
        
        # Graph Attention Layers í†µê³¼
        for i, layer in enumerate(self.model.layers):
            norm_x = self.model.layer_norms[i](x)
            attn_output, _ = layer(desc_embeddings, norm_x)
            x = x + attn_output
        
        # CLS token embedding ë°˜í™˜ (ì²« ë²ˆì§¸ í† í°)
        cls_embeddings = x[:, 0, :]  # [batch_size, hidden_dim]
        
        return cls_embeddings
    
    def compute_complete_cosine_similarity_matrix(self, output_dir=None):
        """
        ëª¨ë“  í´ëŸ¬ìŠ¤í„° ìŒ ê°„ì˜ cosine similarity matrix ê³„ì‚° ë° ì‹œê°í™”
        
        Returns:
            dict: ì™„ì „í•œ ë¶„ì„ ê²°ê³¼
        """
        logger.info("Computing complete cosine similarity matrix...")
        
        # ë¼ë²¨ë³„ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ í™•ì¸
        label_0_clusters = list(self.cluster_embeddings[0].keys())
        label_1_clusters = list(self.cluster_embeddings[1].keys())
        
        n_clusters_0 = len(label_0_clusters)
        n_clusters_1 = len(label_1_clusters)
        
        logger.info(f"Label 0: {n_clusters_0} clusters, Label 1: {n_clusters_1} clusters")
        logger.info(f"Total comparisons: {n_clusters_0} Ã— {n_clusters_1} = {n_clusters_0 * n_clusters_1}")
        
        # ì „ì²´ ê²°ê³¼ êµ¬ì„±
        complete_results = {
            'label_0_clusters': label_0_clusters,
            'label_1_clusters': label_1_clusters,
            'n_clusters_0': n_clusters_0,
            'n_clusters_1': n_clusters_1
        }
        
        # ì‹œê°í™”
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            self._create_cosine_similarity_grid(complete_results, output_dir)
        
        return complete_results
    
    def _create_cosine_similarity_grid(self, results, output_dir):
        """ëª¨ë“  í´ëŸ¬ìŠ¤í„° ìŒì˜ cosine similarity matrixë¥¼ subplotìœ¼ë¡œ ì‹œê°í™”"""
        
        n_clusters_0 = results['n_clusters_0']
        n_clusters_1 = results['n_clusters_1']
        
        # í´ëŸ¬ìŠ¤í„° ìŒë³„ë¡œ ê°œë³„ cosine similarity matrix ìƒì„±
        fig, axes = plt.subplots(n_clusters_0, n_clusters_1, 
                                figsize=(4*n_clusters_1, 4*n_clusters_0))
        
        # axesë¥¼ 2D ë°°ì—´ë¡œ ë§Œë“¤ê¸°
        if n_clusters_0 == 1 and n_clusters_1 == 1:
            axes = np.array([[axes]])
        elif n_clusters_0 == 1:
            axes = axes.reshape(1, -1)
        elif n_clusters_1 == 1:
            axes = axes.reshape(-1, 1)
        
        # ê° í´ëŸ¬ìŠ¤í„° ìŒì— ëŒ€í•´ cosine similarity matrix ê·¸ë¦¬ê¸°
        for i, cluster_0 in enumerate(results['label_0_clusters']):
            for j, cluster_1 in enumerate(results['label_1_clusters']):
                
                ax = axes[i, j]
                
                # í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ìŒì˜ embedding ê°€ì ¸ì˜¤ê¸°
                embeddings_0 = self.cluster_embeddings[0][cluster_0]
                embeddings_1 = self.cluster_embeddings[1][cluster_1]
                
                # Cosine similarity matrix ê³„ì‚°
                cosine_sim_matrix = cosine_similarity(embeddings_0, embeddings_1)
                
                # Heatmap ê·¸ë¦¬ê¸°
                im = ax.imshow(cosine_sim_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
                
                # ì œëª©
                ax.set_title(f'L0 C{cluster_0} vs L1 C{cluster_1}', 
                           fontsize=12, fontweight='bold')
                
                # ì¶• ë¼ë²¨ (ì‘ê²Œ)
                if i == n_clusters_0 - 1:  # ë§ˆì§€ë§‰ í–‰ì—ë§Œ xì¶• ë¼ë²¨
                    ax.set_xlabel(f'L1 C{cluster_1}', fontsize=10)
                if j == 0:  # ì²« ë²ˆì§¸ ì—´ì—ë§Œ yì¶• ë¼ë²¨
                    ax.set_ylabel(f'L0 C{cluster_0}', fontsize=10)
                
                # í‹± ì œê±° (ê¹”ë”í•˜ê²Œ)
                ax.set_xticks([])
                ax.set_yticks([])
        
        plt.suptitle('Cosine Similarity Matrices for All Cluster Pairs', 
                    fontsize=16, fontweight='bold', y=0.95)
        
        # ê³µí†µ ì»¬ëŸ¬ë°”
        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])
        cbar = fig.colorbar(im, cax=cbar_ax)
        cbar.set_label('Cosine Similarity', fontsize=12, fontweight='bold')
        cbar.ax.tick_params(labelsize=10)
        
        plt.tight_layout(rect=[0, 0, 0.90, 0.93])
        
        # ì €ì¥
        plt.savefig(output_dir / 'all_cosine_similarity_matrices.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        logger.info(f"âœ… Cosine similarity grid saved to: {output_dir}")
        
        # ê°œë³„ ë§¤íŠ¸ë¦­ìŠ¤ë“¤ë„ ë”°ë¡œ ì €ì¥
        self._save_individual_matrices(results, output_dir)
    
    def _save_individual_matrices(self, results, output_dir):
        """ê° í´ëŸ¬ìŠ¤í„° ìŒì˜ cosine similarity matrixë¥¼ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥"""
        individual_dir = output_dir / 'individual_matrices'
        individual_dir.mkdir(parents=True, exist_ok=True)
        
        for cluster_0 in results['label_0_clusters']:
            for cluster_1 in results['label_1_clusters']:
                
                # í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ìŒì˜ embedding ê°€ì ¸ì˜¤ê¸°
                embeddings_0 = self.cluster_embeddings[0][cluster_0]
                embeddings_1 = self.cluster_embeddings[1][cluster_1]
                
                # Cosine similarity matrix ê³„ì‚°
                cosine_sim_matrix = cosine_similarity(embeddings_0, embeddings_1)
                
                # ê°œë³„ í”Œë¡¯ ìƒì„±
                fig, ax = plt.subplots(1, 1, figsize=(10, 8))
                
                im = ax.imshow(cosine_sim_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
                
                ax.set_title(f'Cosine Similarity: L0 C{cluster_0} vs L1 C{cluster_1}\n'
                           f'({len(embeddings_0)} Ã— {len(embeddings_1)} samples)', 
                           fontsize=14, fontweight='bold', pad=20)
                
                ax.set_xlabel(f'Label 1 Cluster {cluster_1} Samples', fontsize=12)
                ax.set_ylabel(f'Label 0 Cluster {cluster_0} Samples', fontsize=12)
                
                # ì»¬ëŸ¬ë°”
                cbar = plt.colorbar(im, ax=ax, shrink=0.8)
                cbar.set_label('Cosine Similarity', fontsize=12)
                
                plt.tight_layout()
                
                # ì €ì¥
                filename = f'L0_C{cluster_0}_vs_L1_C{cluster_1}.png'
                plt.savefig(individual_dir / filename, dpi=300, bbox_inches='tight')
                plt.close()
                
                # numpy ë°°ì—´ë„ ì €ì¥
                np.save(individual_dir / f'L0_C{cluster_0}_vs_L1_C{cluster_1}.npy', 
                       cosine_sim_matrix)
        
        logger.info(f"âœ… Individual matrices saved to: {individual_dir}")
    



def main():
    parser = argparse.ArgumentParser(description='Complete Cosine Similarity Matrix Visualization')
    parser.add_argument('--checkpoint_dir', type=str, required=True,
                       help='Path to model checkpoint')
    parser.add_argument('--clustering_dir', type=str, required=True,
                       help='Path to clustering results directory')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='Output directory for results')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use (cuda/cpu)')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir is None:
        clustering_dir = Path(args.clustering_dir)
        args.output_dir = clustering_dir / 'cosine_matrices'
    
    logger.info(f"ğŸ” Creating cosine similarity matrices for all cluster pairs")
    logger.info(f"ğŸ“ Results will be saved to: {args.output_dir}")
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = CompleteCosineSimilarityAnalyzer(args.checkpoint_dir, args.clustering_dir, args.device)
    results = analyzer.compute_complete_cosine_similarity_matrix(args.output_dir)
    
    if results:
        logger.info("âœ… Cosine similarity matrix visualization completed successfully!")
        logger.info(f"   â€¢ Generated {results['n_clusters_0']} Ã— {results['n_clusters_1']} matrices")
    else:
        logger.error("âŒ Analysis failed!")
    
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir is None:
        clustering_dir = Path(args.clustering_dir)
        args.output_dir = clustering_dir / 'complete_cosine_analysis'
    
    logger.info(f"ğŸ” Computing complete cosine similarity matrix for all cluster pairs")
    logger.info(f"ğŸ“ Results will be saved to: {args.output_dir}")
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = CompleteCosineSimilarityAnalyzer(args.checkpoint_dir, args.clustering_dir, args.device)
    results = analyzer.compute_complete_cosine_similarity_matrix(args.output_dir)
    
    if results:
        logger.info("âœ… Complete cosine similarity matrix analysis completed successfully!")
        
        # ì£¼ìš” ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        #mean_matrix = results['mean_similarity_matrix']
        # logger.info(f"\nğŸ¯ Key Findings:")
        # logger.info(f"   â€¢ Matrix Size: {mean_matrix.shape[0]} Ã— {mean_matrix.shape[1]}")
        # logger.info(f"   â€¢ Global Mean Similarity: {np.mean(mean_matrix):.4f}")
        # logger.info(f"   â€¢ Most Similar Pair: {np.max(mean_matrix):.4f}")
        # logger.info(f"   â€¢ Most Different Pair: {np.min(mean_matrix):.4f}")
        
    else:
        logger.error("âŒ Analysis failed!")

if __name__ == "__main__":
    main()