"""
ê°„ì†Œí™”ëœ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

inference.pyì—ì„œ ìƒì„±ëœ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬
ì €ì¥ëœ NPZ íŒŒì¼ë§Œì„ ì°¸ì¡°í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë³„ ë¼ë²¨ ë¶„í¬ì™€ ë¹„ìœ¨ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
ì˜ˆì¸¡ê°’ì´ë‚˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„ì€ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

Usage:
   python analysis1.py --clustering_dir /path/to/clustering/results
"""

import os
import argparse
import numpy as np
import pandas as pd
import logging
from pathlib import Path
import json
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimplifiedClusterAnalyzer:
   def __init__(self, clustering_dir):
       """
       Args:
           clustering_dir (str): clustering results ë””ë ‰í† ë¦¬ ê²½ë¡œ
       """
       self.clustering_dir = Path(clustering_dir)
       self.layer_results = {}
       
       # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¡œë“œ
       self._load_clustering_results()
       
       logger.info(f"SimplifiedClusterAnalyzer initialized for {len(self.layer_results)} layers")
   
   def _load_clustering_results(self):
       """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë””ë ‰í† ë¦¬ì—ì„œ NPZ ë°ì´í„°ë§Œ ë¡œë“œ"""
       
       # clustering_results í´ë” í™•ì¸
       clustering_results_dir = self.clustering_dir / 'clustering_results'
       if not clustering_results_dir.exists():
           clustering_results_dir = self.clustering_dir
           logger.info("Using legacy clustering directory structure")
       else:
           logger.info("Using new clustering_results directory structure")
       
       # ê° ë ˆì´ì–´ë³„ ê²°ê³¼ ë¡œë“œ
       for layer_dir in clustering_results_dir.glob('layer_*'):
           if not layer_dir.is_dir():
               continue
               
           layer_idx = int(layer_dir.name.split('_')[1])
           logger.info(f"Loading clustering results for layer {layer_idx}...")
           
           # í´ëŸ¬ìŠ¤í„°ë³„ ìƒ˜í”Œ ì •ë³´ ìˆ˜ì§‘
           cluster_data = {}
           total_samples = 0
           
           for cluster_dir in layer_dir.glob('cluster_*'):
               if not cluster_dir.is_dir():
                   continue
                   
               cluster_id = int(cluster_dir.name.split('_')[1])
               samples = []
               
               # í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  ìƒ˜í”Œ ë¡œë“œ (NPZ íŒŒì¼ëª…ì—ì„œ ì§ì ‘ ì •ë³´ ì¶”ì¶œ)
               for sample_file in cluster_dir.glob('sample_*.npz'):
                   # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ: sample_{id}_label_{label}.npz
                   filename = sample_file.stem  # .npz ì œê±°
                   parts = filename.split('_')
                   
                   try:
                       sample_id = int(parts[1])  # sample_ë¶€ë¶„ ë‹¤ìŒ
                       label = int(parts[3])      # label_ë¶€ë¶„ ë‹¤ìŒ
                       
                       # NPZ íŒŒì¼ë„ ë¡œë“œí•´ì„œ ì¶”ê°€ ì •ë³´ í™•ì¸
                       data = np.load(sample_file)
                       
                       sample_info = {
                           'sample_id': sample_id,
                           'label': label,
                           'cluster_id': cluster_id,
                           'filename': sample_file.name,
                           'attention_map_shape': data['attention_map'].shape if 'attention_map' in data else None
                       }
                       samples.append(sample_info)
                       total_samples += 1
                       
                   except (ValueError, IndexError) as e:
                       logger.warning(f"Failed to parse filename {sample_file}: {e}")
                       continue
               
               cluster_data[cluster_id] = samples
               logger.info(f"  Cluster {cluster_id}: {len(samples)} samples")
           
           self.layer_results[layer_idx] = {
               'clusters': cluster_data,
               'total_samples': total_samples,
               'n_clusters': len(cluster_data)
           }
           
           logger.info(f"Layer {layer_idx}: {total_samples} samples in {len(cluster_data)} clusters")

   def analyze_layer_label_distribution(self, layer_idx, output_dir):
       """íŠ¹ì • ë ˆì´ì–´ì˜ ë¼ë²¨ ë¶„í¬ ë¶„ì„ ë° ì‹œê°í™” (ë¼ë²¨ ë¶„í¬ë§Œ ë¶„ì„)"""
       if layer_idx not in self.layer_results:
           logger.error(f"Layer {layer_idx} not found in clustering results")
           return
       
       output_dir = Path(output_dir)
       output_dir.mkdir(parents=True, exist_ok=True)
       
       layer_data = self.layer_results[layer_idx]
       clusters = layer_data['clusters']
       
       logger.info(f"Analyzing label distribution for layer {layer_idx}...")
       
       # í´ëŸ¬ìŠ¤í„°ë³„ ë¼ë²¨ ë¶„í¬ ìˆ˜ì§‘
       cluster_label_data = {}
       all_labels = set()
       
       for cluster_id, samples in clusters.items():
           labels = [sample['label'] for sample in samples]
           cluster_label_data[cluster_id] = labels
           all_labels.update(labels)
       
       all_labels = sorted(list(all_labels))
       
       # í†µê³„ì  ê²€ì • ìˆ˜í–‰ (ë¼ë²¨ ë¶„í¬ ì°¨ì´ë§Œ)
       stats_results = self._perform_chi_square_test(cluster_label_data, all_labels, layer_idx)
       
       # ì‹œê°í™” (ë¼ë²¨ ë¶„í¬ë§Œ)
       self._plot_label_distribution_simple(cluster_label_data, all_labels, stats_results, layer_idx, output_dir)
       
       # ê²°ê³¼ ì €ì¥
       self._save_layer_analysis_results(cluster_label_data, all_labels, stats_results, layer_idx, output_dir)
       
       return cluster_label_data, stats_results
   
   def _perform_chi_square_test(self, cluster_label_data, all_labels, layer_idx):
       """Chi-square ê²€ì • ìˆ˜í–‰ (ë¼ë²¨ ë¶„í¬ ì°¨ì´ë§Œ ê²€ì •)"""
       stats_results = {
           'layer_idx': layer_idx,
           'label_chi2': None
       }
       
       cluster_ids = sorted(list(cluster_label_data.keys()))
       
       # ë¼ë²¨ ë¶„í¬ contingency table ìƒì„±
       label_contingency = []
       for cluster_id in cluster_ids:
           labels = cluster_label_data[cluster_id]
           label_counts = {label: 0 for label in all_labels}
           
           for label in labels:
               label_counts[label] += 1
           
           label_row = [label_counts[label] for label in all_labels]
           label_contingency.append(label_row)
       
       # Chi-square ê²€ì • (ë¼ë²¨ ë¶„í¬ì˜ í´ëŸ¬ìŠ¤í„° ê°„ ì°¨ì´)
       if len(label_contingency) > 1 and all(sum(row) > 0 for row in label_contingency):
           try:
               chi2_stat, chi2_p, _, _ = chi2_contingency(label_contingency)
               stats_results['label_chi2'] = {
                   'statistic': float(chi2_stat),
                   'p_value': float(chi2_p),
                   'significant': chi2_p < 0.05,
                   'contingency_table': label_contingency
               }
               logger.info(f"Chi-square test - Statistic: {chi2_stat:.4f}, p-value: {chi2_p:.4f}")
           except Exception as e:
               logger.warning(f"Chi-square test failed: {e}")
       
       return stats_results
   
   def _plot_label_distribution_simple(self, cluster_label_data, all_labels, stats_results, layer_idx, output_dir):
       """ë¼ë²¨ ë¶„í¬ ì‹œê°í™” (NPZ ë°ì´í„°ë§Œ ì‚¬ìš©)"""
       fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
       
       # í´ëŸ¬ìŠ¤í„° IDë¥¼ ìˆ«ììˆœìœ¼ë¡œ ì •ë ¬
       cluster_ids = sorted(list(cluster_label_data.keys()))
       
       # ë°ì´í„° ì¤€ë¹„
       labels_data = []
       for cluster_id in cluster_ids:
           labels = cluster_label_data[cluster_id]
           unique_labels, counts = np.unique(labels, return_counts=True)
           
           for label, count in zip(unique_labels, counts):
               labels_data.append({
                   'Cluster': f'Cluster {cluster_id}',
                   'Label': f'Label {int(label)}',
                   'Count': count,
                   'cluster_id': cluster_id
               })
       
       df_labels = pd.DataFrame(labels_data)
       df_labels = df_labels.sort_values('cluster_id')
       
       # ìŠ¤íƒ ë°” ì°¨íŠ¸ (ê°œìˆ˜)
       pivot_df = df_labels.pivot(index='Cluster', columns='Label', values='Count').fillna(0)
       cluster_order = [f'Cluster {cid}' for cid in cluster_ids]
       pivot_df = pivot_df.reindex(cluster_order)
       
       bars = pivot_df.plot(kind='bar', stacked=True, ax=ax1, color=['skyblue', 'lightcoral'])
       ax1.set_title(f'Layer {layer_idx} - Label Distribution by Cluster\n(Count)')
       ax1.set_xlabel('Cluster')
       ax1.set_ylabel('Count')
       ax1.legend(title='Label')
       ax1.tick_params(axis='x', rotation=45)
       
       # ê° ë°” ìœ„ì— ìˆ«ì í‘œì‹œ
       for container in ax1.containers:
           ax1.bar_label(container, label_type='center', fontsize=10, color='white', weight='bold')
       
       # ìŠ¤íƒ ë°” ì°¨íŠ¸ (ë¹„ìœ¨)
       pivot_df_norm = pivot_df.div(pivot_df.sum(axis=1), axis=0)
       bars2 = pivot_df_norm.plot(kind='bar', stacked=True, ax=ax2, color=['skyblue', 'lightcoral'])
       ax2.set_title(f'Layer {layer_idx} - Label Proportion by Cluster\n(Ratio)')
       ax2.set_xlabel('Cluster')
       ax2.set_ylabel('Proportion')
       ax2.legend(title='Label')
       ax2.tick_params(axis='x', rotation=45)
       
       # ë¹„ìœ¨ í‘œì‹œ
       for container in ax2.containers:
           ax2.bar_label(container, labels=[f'{v:.2f}' if v > 0.05 else '' for v in container.datavalues], 
                       label_type='center', fontsize=9, color='white', weight='bold')
       
       # Chi-square ê²°ê³¼ í‘œì‹œ
       if stats_results['label_chi2']:
           chi2_result = stats_results['label_chi2']
           fig.suptitle(f'Chi-square test: p={chi2_result["p_value"]:.4f} ' + 
                       ('(Significant)' if chi2_result['significant'] else '(Not Significant)'), 
                       fontsize=14)
       
       # í†µê³„ ì •ë³´ í‘œì‹œ
       total_samples = sum(len(labels) for labels in cluster_label_data.values())
       stats_text = f"Total samples: {total_samples}\n"
       stats_text += f"Clusters: {len(cluster_ids)}\n"
       stats_text += f"Labels: {len(all_labels)}"
       
       ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes, 
               fontsize=10, verticalalignment='top',
               bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
       
       plt.tight_layout()
       fig.savefig(output_dir / f'layer_{layer_idx}_label_distribution_only.png', dpi=300, bbox_inches='tight')
       plt.close(fig)
       
       logger.info(f"Label distribution plot saved for layer {layer_idx}")
   
   def _convert_numpy_types(self, obj):
       """numpy íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
       if isinstance(obj, np.integer):
           return int(obj)
       elif isinstance(obj, np.floating):
           return float(obj)
       elif isinstance(obj, np.bool_):
           return bool(obj)
       elif isinstance(obj, np.ndarray):
           return obj.tolist()
       elif isinstance(obj, dict):
           return {k: self._convert_numpy_types(v) for k, v in obj.items()}
       elif isinstance(obj, list):
           return [self._convert_numpy_types(v) for v in obj]
       else:
           return obj
   
   def _save_layer_analysis_results(self, cluster_label_data, all_labels, stats_results, layer_idx, output_dir):
       """ë ˆì´ì–´ ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
       results = {
           'layer_idx': layer_idx,
           'total_samples': sum(len(labels) for labels in cluster_label_data.values()),
           'n_clusters': len(cluster_label_data),
           'n_labels': len(all_labels),
           'cluster_summary': {},
           'statistical_tests': stats_results
       }
       
       # í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ ì •ë³´
       for cluster_id, labels in cluster_label_data.items():
           unique_labels, counts = np.unique(labels, return_counts=True)
           label_distribution = {}
           for label, count in zip(unique_labels, counts):
               label_distribution[f'label_{int(label)}'] = int(count)
           
           results['cluster_summary'][f'cluster_{cluster_id}'] = {
               'n_samples': len(labels),
               'sample_percentage': (len(labels) / results['total_samples']) * 100,
               'label_distribution': label_distribution
           }
       
       # numpy íƒ€ì… ë³€í™˜
       results = self._convert_numpy_types(results)
       
       # JSON ì €ì¥
       results_file = output_dir / f'layer_{layer_idx}_label_distribution_results.json'
       with open(results_file, 'w') as f:
           json.dump(results, f, indent=2)
       
       logger.info(f"Label distribution results saved to {results_file}")

   def cluster_label_specialization_index(self, cluster_label_data):
       """
       Cluster Label Specialization Index (CLSI) ê³„ì‚°
       
       í´ëŸ¬ìŠ¤í„°ë“¤ì´ ë¼ë²¨ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ íŠ¹í™”ë˜ì–´ ìˆëŠ”ì§€ ì¸¡ì •
       0.0 = ëª¨ë“  í´ëŸ¬ìŠ¤í„°ê°€ 50:50 ë¶„í¬ (íŠ¹í™” ì•ˆë¨)
       1.0 = í´ëŸ¬ìŠ¤í„°ë“¤ì´ ëšœë ·í•˜ê²Œ ì„œë¡œ ë‹¤ë¥¸ ë¼ë²¨ë¡œ íŠ¹í™”ë¨
       
       Args:
           cluster_label_data: {cluster_id: [labels]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
           
       Returns:
           float: CLSI ì ìˆ˜ (0.0 ~ 1.0)
       """
       if len(cluster_label_data) <= 1:
           return 0.0
       
       cluster_biases = []
       label_0_ratios = []
       
       for cluster_id, labels in cluster_label_data.items():
           if len(labels) == 0:
               continue
               
           # ë¼ë²¨ ë¹„ìœ¨ ê³„ì‚°
           label_0_count = labels.count(0)
           label_1_count = labels.count(1)
           total = len(labels)
           
           label_0_ratio = label_0_count / total
           label_1_ratio = label_1_count / total
           
           # í¸í–¥ë„: 0.5ì—ì„œ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ”ì§€
           bias = abs(label_0_ratio - 0.5) * 2
           cluster_biases.append(bias)
           label_0_ratios.append(label_0_ratio)
       
       if len(cluster_biases) == 0:
           return 0.0
       
       # í‰ê·  í¸í–¥ë„ (ê°œë³„ í´ëŸ¬ìŠ¤í„°ë“¤ì´ ì–¼ë§ˆë‚˜ íŠ¹í™”ë˜ì—ˆëŠ”ì§€)
       mean_bias = np.mean(cluster_biases)
       
       # íŠ¹í™” ë‹¤ì–‘ì„± (í´ëŸ¬ìŠ¤í„°ë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ íŠ¹í™”ë˜ì—ˆëŠ”ì§€)
       if len(label_0_ratios) <= 1:
           diversity = 0.0
       else:
           # ë¼ë²¨ ë¹„ìœ¨ì˜ ë¶„ì‚° (0~0.25 ë²”ìœ„ë¥¼ 0~1ë¡œ ì •ê·œí™”)
           diversity = np.var(label_0_ratios) * 4
           diversity = min(diversity, 1.0)  # 1.0 ì´ˆê³¼ ë°©ì§€
       
       # ì¢…í•© ì ìˆ˜ (í¸í–¥ë„ì™€ ë‹¤ì–‘ì„±ì˜ ì¡°í™” í‰ê· )
       if mean_bias + diversity == 0:
           clsi = 0.0
       else:
           clsi = 2 * (mean_bias * diversity) / (mean_bias + diversity)
       
       return clsi

   def analyze_layer_specialization(self, output_base_dir):
       """ëª¨ë“  ë ˆì´ì–´ì˜ CLSI ê³„ì‚° ë° ì‹œê°í™”"""
       output_base_dir = Path(output_base_dir)
       output_base_dir.mkdir(parents=True, exist_ok=True)
       
       clsi_results = {}
       detailed_results = {}
       
       logger.info("Starting CLSI (Cluster Label Specialization Index) analysis...")
       
       for layer_idx in sorted(self.layer_results.keys()):
           layer_data = self.layer_results[layer_idx]
           clusters = layer_data['clusters']
           
           # í´ëŸ¬ìŠ¤í„°ë³„ ë¼ë²¨ ë°ì´í„° ì¤€ë¹„
           cluster_label_data = {}
           for cluster_id, samples in clusters.items():
               labels = [sample['label'] for sample in samples]
               cluster_label_data[cluster_id] = labels
           
           # CLSI ê³„ì‚°
           clsi_score = self.cluster_label_specialization_index(cluster_label_data)
           clsi_results[layer_idx] = clsi_score
           
           # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
           cluster_details = []
           for cluster_id, labels in cluster_label_data.items():
               if len(labels) > 0:
                   label_0_count = labels.count(0)
                   label_1_count = labels.count(1)
                   total = len(labels)
                   label_0_ratio = label_0_count / total
                   bias = abs(label_0_ratio - 0.5) * 2
                   
                   cluster_details.append({
                       'cluster_id': cluster_id,
                       'size': total,
                       'label_0_ratio': label_0_ratio,
                       'label_1_ratio': 1 - label_0_ratio,
                       'bias': bias
                   })
           
           detailed_results[layer_idx] = {
               'clsi_score': clsi_score,
               'n_clusters': len(cluster_label_data),
               'cluster_details': cluster_details
           }
           
           logger.info(f"Layer {layer_idx}: CLSI = {clsi_score:.4f}")
       
       # ì‹œê°í™”
       self._plot_clsi_progression(clsi_results, detailed_results, output_base_dir)
       
       # ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥
       self._save_clsi_detailed_report(detailed_results, output_base_dir)
       
       return clsi_results, detailed_results

   def _plot_clsi_progression(self, clsi_results, detailed_results, output_dir):
       """ë ˆì´ì–´ë³„ CLSI ì§„í–‰ ìƒí™© ì‹œê°í™”"""
       
       fig, axes = plt.subplots(2, 2, figsize=(16, 12))
       
       layers = sorted(clsi_results.keys())
       scores = [clsi_results[layer] for layer in layers]
       
       # 1. ë ˆì´ì–´ë³„ CLSI ì ìˆ˜
       ax = axes[0, 0]
       colors = ['red', 'orange', 'green'] if len(layers) == 3 else plt.cm.viridis(np.linspace(0, 1, len(layers)))
       bars = ax.bar(layers, scores, color=colors, alpha=0.7)
       ax.set_title('Cluster Label Specialization Index by Layer', fontsize=14, fontweight='bold')
       ax.set_xlabel('Layer')
       ax.set_ylabel('CLSI Score')
       ax.set_ylim(0, 1)
       ax.grid(True, alpha=0.3)
       
       # ì ìˆ˜ í‘œì‹œ
       for bar, score in zip(bars, scores):
           ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                  f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=11)
       
       # 2. ë ˆì´ì–´ ê°„ ê°œì„ ë„
       ax = axes[0, 1]
       if len(layers) > 1:
           improvements = [scores[i+1] - scores[i] for i in range(len(scores)-1)]
           transition_labels = [f'L{layers[i]}â†’L{layers[i+1]}' for i in range(len(layers)-1)]
           
           colors = ['green' if imp > 0 else 'red' for imp in improvements]
           bars2 = ax.bar(range(len(improvements)), improvements, color=colors, alpha=0.7)
           ax.set_title('CLSI Improvement Between Layers', fontsize=14, fontweight='bold')
           ax.set_xlabel('Layer Transition')
           ax.set_ylabel('CLSI Improvement')
           ax.set_xticks(range(len(improvements)))
           ax.set_xticklabels(transition_labels)
           ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
           ax.grid(True, alpha=0.3)
           
           # ê°œì„ ë„ ê°’ í‘œì‹œ
           for bar, imp in zip(bars2, improvements):
               ax.text(bar.get_x() + bar.get_width()/2, 
                      bar.get_height() + (0.01 if imp > 0 else -0.02),
                      f'{imp:+.3f}', ha='center', 
                      va='bottom' if imp > 0 else 'top', fontweight='bold')
       else:
           ax.text(0.5, 0.5, 'Need at least 2 layers\nfor improvement analysis', 
                  ha='center', va='center', transform=ax.transAxes, fontsize=12)
           ax.set_title('CLSI Improvement Between Layers')
       
       # 3. í´ëŸ¬ìŠ¤í„° í¸í–¥ë„ ë¶„í¬
       ax = axes[1, 0]
       for i, layer_idx in enumerate(layers):
           layer_details = detailed_results[layer_idx]
           biases = [cluster['bias'] for cluster in layer_details['cluster_details']]
           
           if biases:
               ax.hist(biases, alpha=0.6, label=f'Layer {layer_idx}', bins=10, density=True)
       
       ax.set_xlabel('Cluster Bias (0=ê· ë“±, 1=ì™„ì „í¸í–¥)')
       ax.set_ylabel('Density')
       ax.set_title('Cluster Bias Distribution by Layer', fontsize=14, fontweight='bold')
       ax.legend()
       ax.grid(True, alpha=0.3)
       
       # 4. ìš”ì•½ ì •ë³´
       ax = axes[1, 1]
       ax.axis('off')
       
       summary_text = "CLSI Analysis Summary\n\n"
       summary_text += f"Layers analyzed: {len(layers)}\n"
       summary_text += f"CLSI Range: {min(scores):.3f} - {max(scores):.3f}\n\n"
       
       summary_text += "Layer-wise CLSI:\n"
       for layer_idx in layers:
           score = clsi_results[layer_idx]
           n_clusters = detailed_results[layer_idx]['n_clusters']
           summary_text += f"Layer {layer_idx}: {score:.3f} ({n_clusters} clusters)\n"
       
       if len(layers) > 1:
           overall_improvement = scores[-1] - scores[0]
           summary_text += f"\nOverall improvement: {overall_improvement:+.3f}\n"
           
           if overall_improvement > 0.1:
               summary_text += "âœ… Strong specialization progression"
           elif overall_improvement > 0.05:
               summary_text += "ğŸ”„ Moderate specialization progression"
           else:
               summary_text += "âš ï¸ Limited specialization progression"
       
       # í•´ì„ ê°€ì´ë“œ
       summary_text += "\n\nCLSI Interpretation:\n"
       summary_text += "0.0-0.3: Low specialization\n"
       summary_text += "0.3-0.6: Moderate specialization\n"
       summary_text += "0.6-1.0: High specialization"
       
       ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, fontsize=11,
               verticalalignment='top', fontfamily='monospace',
               bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
       
       plt.suptitle('Cluster Label Specialization Analysis\n(GAT Layer Evolution)', 
                   fontsize=16, fontweight='bold', y=0.98)
       plt.tight_layout()
       plt.savefig(output_dir / 'clsi_analysis.png', dpi=300, bbox_inches='tight')
       plt.close(fig)
       
       logger.info("CLSI analysis plot saved")

   def _save_clsi_detailed_report(self, detailed_results, output_dir):
       """CLSI ìƒì„¸ ë¦¬í¬íŠ¸ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
       
       # numpy íƒ€ì… ë³€í™˜
       detailed_results_json = self._convert_numpy_types(detailed_results)
       
       # CLSI ìš”ì•½ ì¶”ê°€
       layers = sorted(detailed_results.keys())
       scores = [detailed_results[layer]['clsi_score'] for layer in layers]
       
       summary = {
           'clsi_summary': {
               'layers_analyzed': layers,
               'clsi_scores': dict(zip(layers, scores)),
               'min_clsi': min(scores),
               'max_clsi': max(scores),
               'clsi_range': max(scores) - min(scores),
               'overall_improvement': scores[-1] - scores[0] if len(scores) > 1 else 0.0
           },
           'detailed_results': detailed_results_json
       }
       
       # JSON ì €ì¥
       results_file = output_dir / 'clsi_detailed_report.json'
       with open(results_file, 'w') as f:
           json.dump(summary, f, indent=2)
       
       logger.info(f"CLSI detailed report saved to {results_file}")
   
   def analyze_all_layers(self, output_base_dir):
       """ëª¨ë“  ë ˆì´ì–´ì— ëŒ€í•´ ë¼ë²¨ ë¶„í¬ ë¶„ì„ ìˆ˜í–‰"""
       output_base_dir = Path(output_base_dir)
       output_base_dir.mkdir(parents=True, exist_ok=True)
       
       all_results = {}
       
       for layer_idx in sorted(self.layer_results.keys()):
           logger.info(f"Starting label distribution analysis for layer {layer_idx}...")
           
           # ë ˆì´ì–´ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬
           layer_output_dir = output_base_dir / f'layer_{layer_idx}_simple'
           
           # ë¶„ì„ ìˆ˜í–‰
           cluster_data, stats_results = self.analyze_layer_label_distribution(layer_idx, layer_output_dir)
           all_results[layer_idx] = {
               'cluster_data': cluster_data,
               'stats_results': stats_results
           }
       
       # ì „ì²´ ë ˆì´ì–´ ë¹„êµ ì‹œê°í™”
       self._create_cross_layer_comparison_simple(all_results, output_base_dir)
       
       logger.info("All layer label distribution analysis completed!")
       return all_results
   
   def _create_cross_layer_comparison_simple(self, all_results, output_dir):
       """ë ˆì´ì–´ ê°„ ë¼ë²¨ ë¶„í¬ ë¹„êµ ì‹œê°í™”"""
       
       # ë ˆì´ì–´ë³„ Chi-square p-value ë¹„êµ
       fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
       
       layers = sorted(all_results.keys())
       chi2_pvalues = []
       n_clusters_list = []
       
       for layer_idx in layers:
           stats = all_results[layer_idx]['stats_results']
           
           if stats['label_chi2']:
               p_val = max(stats['label_chi2']['p_value'], 1e-50)
               chi2_pvalues.append(p_val)
           else:
               chi2_pvalues.append(1.0)
           
           # í´ëŸ¬ìŠ¤í„° ìˆ˜ ì¶”ê°€
           n_clusters_list.append(len(all_results[layer_idx]['cluster_data']))
       
       # Chi-square p-value ì‹œê°í™”
       bars1 = ax1.bar([f'Layer {l}' for l in layers], chi2_pvalues, 
                   color=['red' if p < 0.05 else 'lightcoral' for p in chi2_pvalues])
       ax1.axhline(y=0.05, color='black', linestyle='--', alpha=0.7, label='Î±=0.05')
       ax1.set_title('Chi-square Test p-values\n(Label Distribution Differences)')
       ax1.set_ylabel('p-value')
       ax1.set_yscale('log')
       ax1.set_ylim(1e-50, 1.1)
       ax1.legend()
       ax1.grid(True, alpha=0.3)
       
       # p < 0.05ì¸ ê²½ìš° ë³„í‘œ í‘œì‹œ
       for i, p in enumerate(chi2_pvalues):
           if p < 0.05:
               ax1.text(i, max(p, 1e-45), '*', ha='center', va='bottom', fontsize=16, color='white')
       
       ## ë ˆì´ì–´ë³„ í´ëŸ¬ìŠ¤í„° ìˆ˜
       bars2 = ax2.bar([f'Layer {l}' for l in layers], n_clusters_list, color='lightblue')
       ax2.set_title('Number of Clusters by Layer')
       ax2.set_ylabel('Number of Clusters')
       ax2.grid(True, alpha=0.3)
       
       # í´ëŸ¬ìŠ¤í„° ìˆ˜ í‘œì‹œ
       for bar, count in zip(bars2, n_clusters_list):
           ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,
                   f'{count}', ha='center', va='bottom', fontsize=12, weight='bold')
       
       plt.suptitle('Cross-Layer Label Distribution Analysis', fontsize=16)
       plt.tight_layout()
       fig.savefig(output_dir / 'cross_layer_label_distribution_analysis.png', dpi=300, bbox_inches='tight')
       plt.close(fig)
       
       logger.info("Cross-layer label distribution analysis plot saved")
   
   def print_cluster_summary(self, layer_idx=None):
       """í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì •ë³´ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
       if layer_idx is not None:
           layers_to_print = [layer_idx] if layer_idx in self.layer_results else []
       else:
           layers_to_print = sorted(self.layer_results.keys())
       
       for layer_idx in layers_to_print:
           layer_data = self.layer_results[layer_idx]
           clusters = layer_data['clusters']
           
           print(f"\n{'='*50}")
           print(f"LAYER {layer_idx} SUMMARY")
           print(f"{'='*50}")
           print(f"Total samples: {layer_data['total_samples']}")
           print(f"Number of clusters: {layer_data['n_clusters']}")
           
           for cluster_id, samples in clusters.items():
               labels = [sample['label'] for sample in samples]
               unique_labels, counts = np.unique(labels, return_counts=True)
               
               print(f"\nCluster {cluster_id}: {len(samples)} samples")
               for label, count in zip(unique_labels, counts):
                   percentage = (count / len(samples)) * 100
                   print(f"  Label {label}: {count} samples ({percentage:.1f}%)")

def main():
   parser = argparse.ArgumentParser(description='Simplified Cluster Analysis')
   parser.add_argument('--clustering_dir', type=str, required=True,
                      help='Directory containing clustering results (e.g., clustering_7 folder)')
   parser.add_argument('--output_dir', type=str, default=None,
                      help='Output directory for analysis results')
   parser.add_argument('--layer_idx', type=int, default=None,
                      help='Specific layer to analyze (default: analyze all layers)')
   parser.add_argument('--print_summary', action='store_true',
                      help='Print cluster summary to console')
   parser.add_argument('--clsi_analysis', action='store_true',
                      help='Perform CLSI (Cluster Label Specialization Index) analysis')
   
   args = parser.parse_args()
   
   # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
   if args.output_dir is None:
       clustering_dir = Path(args.clustering_dir)
       args.output_dir = clustering_dir / 'label_analysis'
   
   # ë¶„ì„ê¸° ì´ˆê¸°í™”
   analyzer = SimplifiedClusterAnalyzer(args.clustering_dir)
   
   # ìš”ì•½ ì •ë³´ ì¶œë ¥ (ì˜µì…˜)
   if args.print_summary:
       analyzer.print_cluster_summary(args.layer_idx)
   
   if args.layer_idx is not None:
       # íŠ¹ì • ë ˆì´ì–´ë§Œ ë¶„ì„
       logger.info(f"Analyzing layer {args.layer_idx}...")
       output_dir = Path(args.output_dir) / f'layer_{args.layer_idx}_simple'
       analyzer.analyze_layer_label_distribution(args.layer_idx, output_dir)
   else:
       # ëª¨ë“  ë ˆì´ì–´ ë¶„ì„
       logger.info("Analyzing all layers...")
       analyzer.analyze_all_layers(args.output_dir)

   # CLSI ë¶„ì„ (ì˜µì…˜)
   if args.clsi_analysis:
       logger.info("Starting CLSI analysis...")
       clsi_results, detailed_results = analyzer.analyze_layer_specialization(args.output_dir)
       
       print("\n" + "="*60)
       print("CLSI (Cluster Label Specialization Index) Results")
       print("="*60)
       for layer, score in clsi_results.items():
           print(f"Layer {layer}: {score:.4f}")
       
       if len(clsi_results) > 1:
           layers = sorted(clsi_results.keys())
           improvement = clsi_results[layers[-1]] - clsi_results[layers[0]]
           print(f"\nOverall improvement: {improvement:+.4f}")
           
           if improvement > 0.1:
               print("âœ… Strong specialization progression detected!")
           elif improvement > 0.05:
               print("ğŸ”„ Moderate specialization progression detected.")
           else:
               print("âš ï¸ Limited specialization progression.")

   logger.info(f"Label distribution analysis completed! Results saved to {args.output_dir}")

if __name__ == "__main__":
   main()