import json
import csv
import glob
import os
import numpy as np
from collections import defaultdict
import argparse

def create_directory(path):
    if not os.path.exists(path):
        os.makedirs(path)

def calculate_mean_std(values):
    values = np.array(values)
    mean = np.mean(values)
    std = np.std(values, ddof=0)
    return mean, std

def determine_scenario_from_del_feature(del_feature, important_features, all_features):
    """
    ì‚­ì œëœ ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³´ê³  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì—­ì¶”ì í•˜ëŠ” í•¨ìˆ˜
    """
    if not del_feature:  # ì‚­ì œëœ ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´
        return 0, "Baseline (No feature removal)"
    
    del_set = set(del_feature)
    important_set = set(important_features) if important_features else set()
    unimportant_features = [f for f in all_features if f not in important_set] if all_features else []
    unimportant_set = set(unimportant_features)
    
    # ì‚­ì œëœ ë³€ìˆ˜ ì¤‘ ì¤‘ìš”í•œ ë³€ìˆ˜ì™€ ë¹„ì¤‘ìš”í•œ ë³€ìˆ˜ì˜ ê°œìˆ˜ ê³„ì‚°
    deleted_important = del_set & important_set
    deleted_unimportant = del_set & unimportant_set
    
    # ì‹œë‚˜ë¦¬ì˜¤ 1: ì¤‘ìš”í•œ 3ê°œ ì œê±°
    if len(deleted_important) == len(important_features) and len(deleted_unimportant) == 0:
        return 1, "Remove top-3 important features"
    
    # ì‹œë‚˜ë¦¬ì˜¤ 2: ì¤‘ìš”í•œ 3ê°œ + ë¹„ì¤‘ìš”í•œ 1ê°œ ì œê±°
    elif len(deleted_important) == len(important_features) and len(deleted_unimportant) == 1:
        return 2, "Remove top-3 important + 1 unimportant features"
    
    # ì‹œë‚˜ë¦¬ì˜¤ 3: ë¹„ì¤‘ìš”í•œ 3ê°œ ì œê±°
    elif len(deleted_important) == 0 and len(deleted_unimportant) <= 3:
        return 3, "Remove bottom-3 unimportant features"
    
    # ì‹œë‚˜ë¦¬ì˜¤ 4: ì¤‘ìš”í•œ 3ê°œë§Œ ìœ ì§€ (ë‚˜ë¨¸ì§€ ëª¨ë‘ ì œê±°)
    elif len(deleted_important) == 0 and len(deleted_unimportant) == len(unimportant_features):
        return 4, "Keep only top-3 important features"
    
    # ì•Œ ìˆ˜ ì—†ëŠ” íŒ¨í„´
    else:
        return -1, f"Unknown pattern (del: {len(del_feature)})"

def extract_important_features_from_json(data):
    """
    JSON íŒŒì¼ì—ì„œ important_featuresì™€ all_features ì •ë³´ ì¶”ì¶œ
    """
    # JSONì—ì„œ important_features ì •ë³´ ì°¾ê¸°
    important_features = data.get('important_features', [])
    all_features = data.get('all_features', [])
    
    # ë§Œì•½ ìœ„ í‚¤ê°€ ì—†ë‹¤ë©´ ë‹¤ë¥¸ ê²½ë¡œì—ì„œ ì°¾ê¸° ì‹œë„
    if not important_features and 'ablation_info' in data:
        important_features = data['ablation_info'].get('important_features', [])
        all_features = data['ablation_info'].get('all_features', [])
    
    return important_features, all_features

def create_combined_summary_by_model(results_by_scenario, dataset, base_dir):
    """ê° model_configë³„ë¡œ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ì˜ ëª¨ë“  ë©”íŠ¸ë¦­ì„ í•˜ë‚˜ì˜ í‘œë¡œ ìƒì„±"""
    
    # model_configë³„ë¡œ ê·¸ë£¹í™”
    results_by_model = defaultdict(dict)
    
    for key, few_shot_data in results_by_scenario.items():
        model_config, scenario_id, scenario_desc = key
        results_by_model[model_config][(scenario_id, scenario_desc)] = few_shot_data
    
    # ê° model_configë³„ë¡œ TSV íŒŒì¼ ìƒì„±
    for model_config, scenario_data in results_by_model.items():
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ì •ë ¬
        sorted_scenarios = sorted(scenario_data.items(), key=lambda x: x[0][0])  # scenario_idë¡œ ì •ë ¬
        
        summary_dir = os.path.join(base_dir, f"{dataset}_summary_scenarios", model_config)
        create_directory(summary_dir)
        
        # ê° model_configë³„ ì „ì²´ ê²°ê³¼ë¥¼ ë‹´ì„ TSV íŒŒì¼
        combined_file = os.path.join(summary_dir, 'all_scenarios_combined.tsv')
        
        with open(combined_file, 'w', newline='') as f:
            writer = csv.writer(f, delimiter='\t')
            
            # ê° ì‹œë‚˜ë¦¬ì˜¤ì˜ ê° ë©”íŠ¸ë¦­ë³„ë¡œ í–‰ ìƒì„±
            for (scenario_id, scenario_desc), few_shot_data in sorted_scenarios:
                few_shot_keys = sorted([k for k in few_shot_data.keys() if isinstance(k, int)])
                
                # 5ê°œ ë©”íŠ¸ë¦­ ê°ê°ì— ëŒ€í•´ í–‰ ìƒì„±
                for metric in ['auc', 'acc', 'precision', 'recall', 'f1']:
                    row = []
                    
                    # Few-shot ê²°ê³¼ë“¤ (4, 8, 16, 32, 64)
                    for few_shot in few_shot_keys:
                        values = few_shot_data[few_shot]['few_shot'][metric]
                        if values:
                            mean, std = calculate_mean_std(values)
                            row.append(f"{mean:.4f}({std:.4f})")
                        else:
                            row.append("")
                    
                    # Full dataset ê²°ê³¼
                    if 'full' in few_shot_data:
                        values = few_shot_data['full']['full'][metric]
                        if values:
                            mean, std = calculate_mean_std(values)
                            row.append(f"{mean:.4f}({std:.4f})")
                        else:
                            row.append("")
                    else:
                        row.append("")
                    
                    writer.writerow(row)

def process_json_files(directory_path, selected_datasets=None, selected_seeds=None):
    if selected_datasets:
        print(f"ì²˜ë¦¬í•  ë°ì´í„°ì…‹: {selected_datasets}")
    
    if selected_seeds:
        selected_seeds = [str(seed) for seed in selected_seeds]  # ë¬¸ìì—´ë¡œ ë³€í™˜
    
    datasets_to_process = selected_datasets if selected_datasets else ['heart']
    
    for dataset in datasets_to_process:
        # ğŸ”¥ êµ¬ì¡° ë³€ê²½: ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ê·¸ë£¹í™”
        results_by_scenario = defaultdict(lambda: defaultdict(lambda: {
            'few_shot': {'auc': [], 'acc': [], 'precision': [], 'recall': [], 'f1': []},
            'full': {'auc': [], 'acc': [], 'precision': [], 'recall': [], 'f1': []}
        }))
        
        dataset_path = os.path.join(directory_path, dataset)
        
        if not os.path.exists(dataset_path):
            print(f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {dataset_path}")
            continue
        
        json_files = []
        
        # ì„ íƒëœ ì‹œë“œê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì‹œë“œ ê²½ë¡œë§Œ ì²˜ë¦¬
        if selected_seeds:
            for seed in selected_seeds:
                seed_path = os.path.join(dataset_path, f"args_seed:{seed}")
                if os.path.exists(seed_path):
                    seed_json_pattern = os.path.join(seed_path, "TabularFLM/Embed:*_Edge:*_A:*/*scenario*.json")
                    seed_json_files = glob.glob(seed_json_pattern, recursive=True)
                    if seed_json_files:
                        json_files.extend(seed_json_files)
                else:
                    print(f"ì‹œë“œ {seed}ì˜ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {seed_path}")
        else:
            # ëª¨ë“  ì‹œë“œ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            json_pattern = os.path.join(dataset_path, "args_seed:*/TabularFLM/Embed:*_Edge:*_A:*/*scenario*.json")
            json_files = glob.glob(json_pattern, recursive=True)
        
        if not json_files:
            print(f"ë°ì´í„°ì…‹ {dataset}ì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            continue
        
        for json_file in json_files:
            try:
                with open(json_file, 'r') as f:
                    data = json.load(f)
                    
                path_parts = json_file.split('/')
                model_config = path_parts[-2]  # Embed:carte_Edge:mlp_A:gat_v1
                
                # ğŸ”¥ JSONì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ ì§ì ‘ ì¶”ì¶œ
                scenario_info = data.get('scenario_info', {})
                scenario_id = scenario_info.get('scenario_id', 'unknown')
                scenario_desc = scenario_info.get('scenario_description', 'Unknown scenario')
                
                few_shot = data['hyperparameters']['few_shot']
                
                # ğŸ”¥ ì‹œë‚˜ë¦¬ì˜¤ì™€ model_config ì¡°í•©ìœ¼ë¡œ í‚¤ ìƒì„±
                key = (model_config, scenario_id, scenario_desc)
                
                # Few-shot ê²°ê³¼ ì €ì¥
                if 'Ours_few' in data['results']:
                    results = data['results']['Ours_few']
                    metrics = {
                        'auc': results['Ours_best_few_auc'],
                        'acc': results['Ours_best_few_acc'],
                        'precision': results['Ours_best_few_precision'],
                        'recall': results['Ours_best_few_recall'],
                        'f1': results['Ours_best_few_f1']
                    }
                    for metric_name, value in metrics.items():
                        results_by_scenario[key][few_shot]['few_shot'][metric_name].append(value)
                
                # Full dataset ê²°ê³¼ ì €ì¥ (few_shot=4ì¼ ë•Œë§Œ)
                if few_shot == 4 and isinstance(data['results']['Ours'], dict):
                    results = data['results']['Ours']
                    metrics = {
                        'auc': results['Ours_best_full_auc'],
                        'acc': results['Ours_best_full_acc'],
                        'precision': results['Ours_best_full_precision'],
                        'recall': results['Ours_best_full_recall'],
                        'f1': results['Ours_best_full_f1']
                    }
                    for metric_name, value in metrics.items():
                        results_by_scenario[key]['full']['full'][metric_name].append(value)
                            
            except Exception as e:
                print(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {json_file}: {str(e)}")
        
        # ğŸ”¥ ê²°ê³¼ ì €ì¥: ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ê° few-shotë§ˆë‹¤ ë³„ë„ CSV íŒŒì¼
        for key, few_shot_data in results_by_scenario.items():
            model_config, scenario_id, scenario_desc = key
            
            summary_dir = os.path.join(directory_path, f"{dataset}_summary_scenarios")
            model_dir = os.path.join(summary_dir, model_config, f"scenario_{scenario_id}")
            create_directory(model_dir)
            
            # ì‹œë‚˜ë¦¬ì˜¤ ì„¤ëª…ì„ íŒŒì¼ì— ì €ì¥
            desc_file = os.path.join(model_dir, 'scenario_description.txt')
            with open(desc_file, 'w') as f:
                f.write(f"Scenario {scenario_id}: {scenario_desc}\n")
            
            # ğŸ”¥ ê° few-shotë³„ë¡œ ë³„ë„ íŒŒì¼ ìƒì„±
            for few_shot in few_shot_data.keys():
                if few_shot != 'full':  # 'full' í‚¤ ì œì™¸
                    output_file = os.path.join(model_dir, f'f{few_shot}_b32.csv')
                    
                    with open(output_file, 'w', newline='') as f:
                        writer = csv.writer(f)
                        
                        # Few-shot ê²°ê³¼ (ì›ë˜ í˜•íƒœ: ì„¸ë¡œë¡œ ë‚˜ì—´)
                        writer.writerow(['Few-shot Results:'])
                        for metric in ['auc', 'acc', 'precision', 'recall', 'f1']:
                            values = few_shot_data[few_shot]['few_shot'][metric]
                            if values:
                                mean, std = calculate_mean_std(values)
                                writer.writerow([f"{mean:.4f}({std:.4f})"])
                        
                        # Full dataset ê²°ê³¼ (few_shot=4ì¼ ë•Œë§Œ)
                        if few_shot == 4:
                            writer.writerow([''])
                            writer.writerow(['Full Dataset Results:'])
                            if 'full' in few_shot_data:
                                for metric in ['auc', 'acc', 'precision', 'recall', 'f1']:
                                    values = few_shot_data['full']['full'][metric]
                                    if values:
                                        mean, std = calculate_mean_std(values)
                                        writer.writerow([f"{mean:.4f}({std:.4f})"])
            
            # ğŸ”¥ ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ë³„ ìš”ì•½ í…Œì´ë¸” ìƒì„± (TSV, ìˆ«ìë§Œ)
            summary_output_file = os.path.join(model_dir, 'summary_all_fewshots.tsv')
            with open(summary_output_file, 'w', newline='') as f:
                writer = csv.writer(f, delimiter='\t')
                
                few_shot_keys = sorted([k for k in few_shot_data.keys() if isinstance(k, int)])
                
                # ğŸ”¥ ìˆ«ìë§Œ ë‚˜ì˜¤ëŠ” í‘œ ìƒì„± (í—¤ë”ì™€ ë©”íŠ¸ë¦­ ì´ë¦„ ëª¨ë‘ ì œê±°)
                for metric in ['auc', 'acc', 'precision', 'recall', 'f1']:
                    row = []  # ë©”íŠ¸ë¦­ ì´ë¦„ ì œê±°
                    
                    # Few-shot ê²°ê³¼ë“¤
                    for few_shot in few_shot_keys:
                        values = few_shot_data[few_shot]['few_shot'][metric]
                        if values:
                            mean, std = calculate_mean_std(values)
                            row.append(f"{mean:.4f}({std:.4f})")
                        else:
                            row.append("")
                    
                    # Full dataset ê²°ê³¼
                    if 'full' in few_shot_data:
                        values = few_shot_data['full']['full'][metric]
                        if values:
                            mean, std = calculate_mean_std(values)
                            row.append(f"{mean:.4f}({std:.4f})")
                        else:
                            row.append("")
                    else:
                        row.append("")
                    
                    writer.writerow(row)
        
        # ğŸ”¥ ê° model_configë³„ë¡œ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹œ TSV íŒŒì¼ ìƒì„±
        create_combined_summary_by_model(results_by_scenario, dataset, directory_path)
        
        print(f"ë°ì´í„°ì…‹ {dataset} ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ì´ {len(results_by_scenario)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ì¡°í•© ìƒì„±ë¨")

def main():
    parser = argparse.ArgumentParser(description='Summarize Ablation Study results by scenarios')
    parser.add_argument('--base_dir', type=str, 
                       default="/home/eungyeop/LLM/tabular/ProtoLLM/experiments/source_to_source_tabular_embedding_new_bio-clinical-bert",
                       help='Base directory containing the results')
    parser.add_argument('--datasets', nargs='+', type=str,
                       help='Specific datasets to process (e.g., --datasets adult diabetes)')
    parser.add_argument('--seeds', nargs='+', type=str,
                       help='Specific seeds to include in analysis (e.g., --seeds 42 123 456)')
    parser.add_argument('--best_seed', nargs='+', type=str,
                       help='Specific best seeds to include in analysis (alias for --seeds)')
    
    args = parser.parse_args()
    
    # --best_seedë¥¼ --seeds ë§¤ê°œë³€ìˆ˜ë¡œ ì²˜ë¦¬
    selected_seeds = args.seeds if args.seeds else args.best_seed
    
    process_json_files(args.base_dir, args.datasets, selected_seeds)

if __name__ == "__main__":
    main()