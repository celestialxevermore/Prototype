import json
import csv
import glob
import os
import numpy as np
from collections import defaultdict
import argparse

def create_directory(path):
    if not os.path.exists(path):
        os.makedirs(path)

def calculate_mean_std(values):
    values = np.array(values)
    mean = np.mean(values)
    std = np.std(values, ddof=0)
    return mean, std

def create_combined_summary_by_model(results_by_config, full_results, dataset, base_dir):
    """ê° model_configë³„ë¡œ ëª¨ë“  ì„¤ì •ì˜ ëª¨ë“  ë©”íŠ¸ë¦­ì„ í•˜ë‚˜ì˜ TSV í‘œë¡œ ìƒì„±"""
    
    # model_configë³„ë¡œ ê·¸ë£¹í™”
    results_by_model = defaultdict(dict)
    full_results_by_model = defaultdict(dict)
    
    for key, data in results_by_config.items():
        model_config, few_shot, batch_size = key
        results_by_model[model_config][(few_shot, batch_size)] = data
    
    for key, data in full_results.items():
        model_config, batch_size = key
        full_results_by_model[model_config][batch_size] = data
    
    # ê° model_configë³„ë¡œ TSV íŒŒì¼ ìƒì„±
    for model_config in results_by_model.keys():
        config_data = results_by_model[model_config]
        full_data = full_results_by_model.get(model_config, {})
        
        # few_shotê³¼ batch_sizeë³„ë¡œ ì •ë ¬
        sorted_configs = sorted(config_data.items(), key=lambda x: (x[0][0], x[0][1]))  # few_shot, batch_size ìˆœìœ¼ë¡œ ì •ë ¬
        
        summary_dir = os.path.join(base_dir, f"{dataset}_summary", model_config)
        create_directory(summary_dir)
        
        # ì „ì²´ ê²°ê³¼ë¥¼ ë‹´ì„ TSV íŒŒì¼
        combined_file = os.path.join(summary_dir, 'all_configs_combined.tsv')
        
        with open(combined_file, 'w', newline='') as f:
            writer = csv.writer(f, delimiter='\t')
            
            # í—¤ë” ì‘ì„± (ì„¤ì •ë³„ ì»¬ëŸ¼ëª…)
            header = []
            for (few_shot, batch_size), _ in sorted_configs:
                header.append(f"f{few_shot}_b{batch_size}")
            
            # Full dataset ì»¬ëŸ¼ ì¶”ê°€ (batch_sizeë³„ë¡œ)
            unique_batch_sizes = sorted(set([bs for (fs, bs), _ in sorted_configs]))
            for batch_size in unique_batch_sizes:
                header.append(f"full_b{batch_size}")
            
            writer.writerow(header)
            
            # ê° ë©”íŠ¸ë¦­ë³„ë¡œ í–‰ ìƒì„± (5ê°œ ë©”íŠ¸ë¦­)
            for metric in ['auc', 'acc', 'precision', 'recall', 'f1']:
                row = []
                
                # Few-shot ê²°ê³¼ë“¤
                for (few_shot, batch_size), data in sorted_configs:
                    values = data['few_shot'][metric]
                    if values:
                        mean, std = calculate_mean_std(values)
                        row.append(f"{mean:.4f}({std:.4f})")
                    else:
                        row.append("")
                
                # Full dataset ê²°ê³¼ë“¤
                for batch_size in unique_batch_sizes:
                    if batch_size in full_data and metric in full_data[batch_size]:
                        values = full_data[batch_size][metric]
                        if values:
                            mean, std = calculate_mean_std(values)
                            row.append(f"{mean:.4f}({std:.4f})")
                        else:
                            row.append("")
                    else:
                        row.append("")
                
                writer.writerow(row)

def process_json_files(directory_path, selected_datasets=None, selected_seeds=None):
    if selected_datasets:
        print(f"ì²˜ë¦¬í•  ë°ì´í„°ì…‹: {selected_datasets}")
    
    if selected_seeds:
        selected_seeds = [str(seed) for seed in selected_seeds]  # ë¬¸ìì—´ë¡œ ë³€í™˜
    
    datasets_to_process = selected_datasets if selected_datasets else ['heart']
    
    for dataset in datasets_to_process:
        results_by_config = {}
        dataset_path = os.path.join(directory_path, dataset)
        
        if not os.path.exists(dataset_path):
            print(f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {dataset_path}")
            continue
        
        json_files = []
        processed_seeds = []
        
        # ì„ íƒëœ ì‹œë“œê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì‹œë“œ ê²½ë¡œë§Œ ì²˜ë¦¬
        if selected_seeds:
            for seed in selected_seeds:
                seed_path = os.path.join(dataset_path, f"args_seed:{seed}")
                if os.path.exists(seed_path):
                    # íŒ¨í„´ì„ ì‹¤ì œ í´ë” êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                    seed_json_pattern = os.path.join(seed_path, "TabularFLM/*/f*.json")
                    seed_json_files = glob.glob(seed_json_pattern, recursive=True)
                    if seed_json_files:
                        json_files.extend(seed_json_files)
                        processed_seeds.append(seed)
                else:
                    print(f"ì‹œë“œ {seed}ì˜ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {seed_path}")
        else:
            # ëª¨ë“  ì‹œë“œ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹) - íŒ¨í„´ ìˆ˜ì •
            json_pattern = os.path.join(dataset_path, "args_seed:*/TabularFLM/*/f*.json")
            json_files = glob.glob(json_pattern, recursive=True)
        
        if not json_files:
            print(f"ë°ì´í„°ì…‹ {dataset}ì—ì„œ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            continue
        
        # Full dataset ê²°ê³¼ë¥¼ ì €ì¥í•  ë³„ë„ì˜ ë”•ì…”ë„ˆë¦¬
        full_results = defaultdict(lambda: {'auc': [], 'acc': [], 'precision': [], 'recall': [], 'f1': []})
        
        for json_file in json_files:
            try:
                with open(json_file, 'r') as f:
                    data = json.load(f)
                    
                path_parts = json_file.split('/')
                model_config = path_parts[-2]
                
                config_key = (
                    model_config,
                    data['hyperparameters']['few_shot'],
                    data['hyperparameters']['batch_size']
                )
                
                if config_key not in results_by_config:
                    results_by_config[config_key] = {
                        'few_shot': {'auc': [], 'acc': [], 'precision': [], 'recall': [], 'f1': []}
                    }
                
                if 'Ours_few' in data['results']:
                    results = data['results']['Ours_few']
                    metrics = {
                        'auc': results['Ours_best_few_auc'],
                        'acc': results['Ours_best_few_acc'],
                        'precision': results['Ours_best_few_precision'],
                        'recall': results['Ours_best_few_recall'],
                        'f1': results['Ours_best_few_f1']
                    }
                    for metric_name, value in metrics.items():
                        results_by_config[config_key]['few_shot'][metric_name].append(value)
                
                if data['hyperparameters']['few_shot'] == 4 and isinstance(data['results']['Ours'], dict):
                    results = data['results']['Ours']
                    full_key = (model_config, data['hyperparameters']['batch_size'])
                    metrics = {
                        'auc': results['Ours_best_full_auc'],
                        'acc': results['Ours_best_full_acc'],
                        'precision': results['Ours_best_full_precision'],
                        'recall': results['Ours_best_full_recall'],
                        'f1': results['Ours_best_full_f1']
                    }
                    for metric_name, value in metrics.items():
                        full_results[full_key][metric_name].append(value)
                            
            except Exception as e:
                print(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {json_file}: {str(e)}")
        
        # ê¸°ì¡´ ê°œë³„ CSV íŒŒì¼ ì €ì¥
        for config_key in results_by_config.keys():
            model_config, few_shot, batch_size = config_key
            
            # ë³€ê²½: ë°ì´í„°ì…‹ ì´ë¦„_summary í´ë” ìƒì„±
            summary_dir = os.path.join(directory_path, f"{dataset}_summary")
            model_dir = os.path.join(summary_dir, model_config)
            create_directory(model_dir)
            
            # ì‹œë“œ ì •ë³´ ì—†ì´ ê°„ë‹¨í•œ íŒŒì¼ëª… ì‚¬ìš©
            output_file = os.path.join(model_dir, f'f{few_shot}_b{batch_size}.csv')
            
            with open(output_file, 'w', newline='') as f:
                writer = csv.writer(f)
                
                # Few-shot ê²°ê³¼
                writer.writerow(['Few-shot Results:'])
                for metric in ['auc', 'acc', 'precision', 'recall', 'f1']:
                    values = results_by_config[config_key]['few_shot'][metric]
                    if values:
                        mean, std = calculate_mean_std(values)
                        writer.writerow([f"{mean:.4f}({std:.4f})"])
                
                # Full dataset ê²°ê³¼
                writer.writerow([''])
                writer.writerow(['Full Dataset Results:'])
                full_key = (model_config, batch_size)
                if full_key in full_results:
                    for metric in ['auc', 'acc', 'precision', 'recall', 'f1']:
                        values = full_results[full_key][metric]
                        if values:
                            mean, std = calculate_mean_std(values)
                            writer.writerow([f"{mean:.4f}({std:.4f})"])
        
        # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€: ê° model_configë³„ë¡œ ëª¨ë“  ì„¤ì •ì„ í•˜ë‚˜ë¡œ í•©ì¹œ TSV íŒŒì¼ ìƒì„±
        create_combined_summary_by_model(results_by_config, full_results, dataset, directory_path)
        
        print(f"ë°ì´í„°ì…‹ {dataset} ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ì´ {len(results_by_config)}ê°œ ì„¤ì • ì¡°í•© ì²˜ë¦¬ë¨")

def main():
    parser = argparse.ArgumentParser(description='Summarize DL results')
    parser.add_argument('--base_dir', type=str, #/home/eungyeop/LLM/tabular/ProtoLLM/experiments/source_to_source_tabular_embedding_new_bio-clinical-bert
                       default="/home/eungyeop/LLM/tabular/ProtoLLM/experiments/source_to_source_tabular_embedding_new_bio-clinical-bert",
                       help='Base directory containing the results')
    parser.add_argument('--datasets', nargs='+', type=str,
                       help='Specific datasets to process (e.g., --datasets adult diabetes)')
    parser.add_argument('--seeds', nargs='+', type=str,
                       help='Specific seeds to include in analysis (e.g., --seeds 42 123 456)')
    parser.add_argument('--best_seed', nargs='+', type=str,
                       help='Specific best seeds to include in analysis (alias for --seeds)')
    
    args = parser.parse_args()
    
    # --best_seedë¥¼ --seeds ë§¤ê°œë³€ìˆ˜ë¡œ ì²˜ë¦¬
    selected_seeds = args.seeds if args.seeds else args.best_seed
    
    process_json_files(args.base_dir, args.datasets, selected_seeds)

if __name__ == "__main__":
    main()